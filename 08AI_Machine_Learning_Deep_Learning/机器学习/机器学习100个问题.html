<!DOCTYPE html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta property="og:url" content="https://tobeprozy.github.io/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html"><meta property="og:site_name" content="南叔先生-开源笔记"><meta property="og:title" content="机器学习100个问题"><meta property="og:description" content="机器学习100个问题 基础知识 1、什么是数据科学?列出监督学习和非监督学习的区别。 数据科学是各种工具、算法和机器学习方法的混合，其目标是从原始数据中发现隐藏的模式。这与统计学家多年来一直在做的事情相似但是有什么不同?下图解释了数据分析和数据科学的区别： http://images.overfit.cn/upload/20220626/436dc6374098494cafbae43fe0c4c56a.png"><meta property="og:type" content="article"><meta property="og:image" content="https://tobeprozy.github.io/"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2023-04-26T10:25:41.000Z"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="机器学习100个问题"><meta property="article:modified_time" content="2023-04-26T10:25:41.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"机器学习100个问题","image":["https://tobeprozy.github.io/"],"dateModified":"2023-04-26T10:25:41.000Z","author":[]}</script><title>机器学习100个问题 | 南叔先生-开源笔记</title><meta name="description" content="机器学习100个问题 基础知识 1、什么是数据科学?列出监督学习和非监督学习的区别。 数据科学是各种工具、算法和机器学习方法的混合，其目标是从原始数据中发现隐藏的模式。这与统计学家多年来一直在做的事情相似但是有什么不同?下图解释了数据分析和数据科学的区别： http://images.overfit.cn/upload/20220626/436dc6374098494cafbae43fe0c4c56a.png">
    <meta name="keywords" content="自我提升,效率提升,开源工具,学习笔记" />
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #252232;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-ec8d68d6.css" as="style"><link rel="stylesheet" href="/assets/style-ec8d68d6.css">
    <link rel="modulepreload" href="/assets/app-1ea92eb0.js"><link rel="modulepreload" href="/assets/framework-d651fda7.js"><link rel="modulepreload" href="/assets/机器学习100个问题.html-bd3d3ea9.js"><link rel="modulepreload" href="/assets/机器学习100个问题.html-9ab05f34.js">

    <!-- 看板娘区块 -->
    <link href="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-1-y/font-awesome/6.0.0/css/all.min.css" type="text/css" rel="stylesheet" />
    <script src="/live2d-widget/autoload.js"></script>
    <!-- End 看板娘区块 -->

    <!-- Matomo 此区块为统计代码，请删除-->
    <script>
      var _paq = window._paq = window._paq || [];
      /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
      _paq.push(['trackPageView']);
      _paq.push(['enableLinkTracking']);
      (function() {
        var u="https://piwik.seoipo.com/";
        _paq.push(['setTrackerUrl', u+'matomo.php']);
        _paq.push(['setSiteId', '7']);
        var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
        g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
      })();
    </script>
    <!-- End Matomo Code 此区块为统计代码，请删除-->

  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header class="navbar" id="navbar"><div class="navbar-start"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><a href="/" class="brand"><img class="logo" src="/logo.svg" alt="南叔先生-开源笔记"><!----><span class="site-name hide-in-pad">南叔先生-开源笔记</span></a><!--[--><!----><!--]--></div><div class="navbar-center"><!--[--><!----><!--]--><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/blog" class="nav-link" aria-label="博客"><span class="font-icon icon iconfont icon-blog" style=""></span>博客<!----></a></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button class="dropdown-title" type="button" aria-label="代码"><span class="title"><span class="font-icon icon iconfont icon-code" style=""></span>代码</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a href="/26code/Markdown.html" class="nav-link" aria-label="Markdown"><span class="font-icon icon iconfont icon-markdown" style=""></span>Markdown<!----></a></li><li class="dropdown-item"><a href="/26code/AutoHotkey.html" class="nav-link" aria-label="AutoHotkey"><span class="font-icon icon iconfont icon-linter" style=""></span>AutoHotkey<!----></a></li><li class="dropdown-item"><a href="/26code/Electron.html" class="nav-link" aria-label="Electron"><span class="font-icon icon iconfont icon-layout" style=""></span>Electron<!----></a></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>页面开发</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/20web/VuePress" class="nav-link" aria-label="/20web/VuePress"><!---->/20web/VuePress<!----></a></li><li class="dropdown-subitem"><a href="/20web/docsify" class="nav-link" aria-label="/20web/docsify"><!---->/20web/docsify<!----></a></li><li class="dropdown-subitem"><a href="/27deploy/VPS.html" class="nav-link" aria-label="服务器 VPS"><span class="font-icon icon iconfont icon-IO" style=""></span>服务器 VPS<!----></a></li></ul></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button class="dropdown-title" type="button" aria-label="应用"><span class="title"><span class="font-icon icon iconfont icon-app" style=""></span>应用</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a href="/25apps/Applist.html" class="nav-link" aria-label="必备应用"><span class="font-icon icon iconfont icon-list" style=""></span>必备应用<!----></a></li><li class="dropdown-item"><a href="https://ai.newzone.top/" rel="noopener noreferrer" target="_blank" aria-label="ChatGPT SC" class="nav-link"><span class="font-icon icon iconfont icon-creative" style=""></span>ChatGPT SC<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></li><li class="dropdown-item"><a href="/25apps/livestreaming/1_obs_basic.html" class="nav-link" aria-label="直播手册"><span class="font-icon icon iconfont icon-quote" style=""></span>直播手册<!----></a></li><li class="dropdown-item"><h4 class="dropdown-subtitle"><span>服务/系统</span></h4><ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/15Linux/NAS.html" class="nav-link" aria-label="NAS"><span class="font-icon icon iconfont icon-process" style=""></span>NAS<!----></a></li><li class="dropdown-subitem"><a href="/24Windows/faq.html" class="nav-link" aria-label="Windows"><span class="font-icon icon iconfont icon-windows" style=""></span>Windows<!----></a></li></ul></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button class="dropdown-title" type="button" aria-label="生活"><span class="title"><span class="font-icon icon iconfont icon-emmet" style=""></span>生活</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a href="/28LifeNotes/Diet.html" class="nav-link" aria-label="健康饮食"><span class="font-icon icon iconfont icon-enum" style=""></span>健康饮食<!----></a></li><li class="dropdown-item"><a href="/28LifeNotes/Shoppinglist.html" class="nav-link" aria-label="购物清单"><span class="font-icon icon iconfont icon-info" style=""></span>购物清单<!----></a></li><li class="dropdown-item"><a href="/28LifeNotes/Coupon.html" class="nav-link" aria-label="网购攻略"><span class="font-icon icon iconfont icon-free" style=""></span>网购攻略<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><a href="https://nav.newzone.top/" rel="noopener noreferrer" target="_blank" aria-label="工具收藏" class="nav-link"><span class="font-icon icon iconfont icon-tool" style=""></span>工具收藏<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div></nav><!--[--><!----><!--]--></div><div class="navbar-end"><!--[--><!----><!--]--><div class="nav-item"><a class="repo-link" href="https://github.com/tobeprozy/LearnData" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button class="outlook-button" tabindex="-1" ariahidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="outlook-dropdown"><!----></div></button></div><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--><!--[--><!----><!--]--><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside class="sidebar" id="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"><li><section class="sidebar-group"><button class="sidebar-heading clickable"><span class="font-icon icon iconfont icon-daily" style=""></span><span class="title">📅每日一问</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><span class="font-icon icon iconfont icon-sensor" style=""></span><span class="title">🎥传感器</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🌅图像处理</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">👓计算机视觉</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">☁️点云处理</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🔍优化算法</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🚀SLAM</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🚗数学基础</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable active"><!----><span class="title">🧠AI-机器学习-深度学习</span><span class="arrow down"></span></button><ul class="sidebar-links"><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">C N N</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">Machine Learning</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">人工智能知识图谱</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable active"><!----><span class="title">机器学习</span><span class="arrow down"></span></button><ul class="sidebar-links"><li><!--[--><a href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95.html" class="nav-link sidebar-link sidebar-page" aria-label="一、回归算法"><!---->一、回归算法<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86.html" class="nav-link sidebar-link sidebar-page" aria-label="一、概述"><!---->一、概述<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html" class="router-link-active router-link-exact-active nav-link active sidebar-link sidebar-page active" aria-label="机器学习100个问题"><!---->机器学习100个问题<!----></a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#基础知识" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="基础知识"><!---->基础知识<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#数据分析" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="数据分析"><!---->数据分析<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#机器学习" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="机器学习"><!---->机器学习<!----></a><ul class="sidebar-sub-headers"></ul></li><li class="sidebar-sub-header"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#深度学习" class="router-link-active router-link-exact-active nav-link sidebar-link heading" aria-label="深度学习"><!---->深度学习<!----></a><ul class="sidebar-sub-headers"></ul></li></ul><!--]--></li><li><!--[--><a href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8D%E7%82%B9%E7%9F%A5%E8%AF%86.html" class="nav-link sidebar-link sidebar-page" aria-label="机器学习重点知识"><!---->机器学习重点知识<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">机器学习知识图谱</span><span class="arrow end"></span></button><!----></section></li><li><!--[--><a href="/08AI_Machine_Learning_Deep_Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6.html" class="nav-link sidebar-link sidebar-page" aria-label="深度学习常用框架"><span class="font-icon icon iconfont icon-🧑" style=""></span>深度学习常用框架<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">📟嵌入式开发</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🤖机器人</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🚘自动驾驶篇</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">💻C++</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🐍Python</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🐧Matlab</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🐋 Linux 服务</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🤖ROS</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">💻计算机基础</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">📊数据结构</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🌐网络编程</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🔧机械工程</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">⚡电路电子</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🔊信号处理</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🧰 应用手册</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🌐 页面开发</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🏗️ 网站部署</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🚀 代码学习</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><span class="font-icon icon iconfont icon-windows" style=""></span><span class="title">系统问题</span><span class="arrow end"></span></button><!----></section></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><!----><span class="title">🛖 生活记录</span><span class="arrow end"></span></button><!----></section></li><li><!--[--><a href="/DailyRoutine.html" class="nav-link sidebar-link sidebar-page" aria-label="每日仪式"><span class="font-icon icon iconfont icon-check" style=""></span>每日仪式<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a href="/Fitness.html" class="nav-link sidebar-link sidebar-page" aria-label="健身计划"><span class="font-icon icon iconfont icon-strong" style=""></span>健身计划<!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li><li><section class="sidebar-group"><button class="sidebar-heading clickable"><span class="font-icon icon iconfont icon-blog" style=""></span><a href="/blog.html" class="title">博客文章</a><span class="arrow end"></span></button><!----></section></li><li><!--[--><a href="https://newzone.top/reading/" rel="noopener noreferrer" target="_blank" aria-label="读书笔记" class="nav-link sidebar-link sidebar-page"><span class="font-icon icon iconfont icon-read" style=""></span>读书笔记<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a><ul class="sidebar-sub-headers"></ul><!--]--></li></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->机器学习100个问题</h1><div class="page-info"><!----><!----><span class="page-word-info" aria-label="字数🔠" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon word-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="word icon"><path d="M518.217 432.64V73.143A73.143 73.143 0 01603.43 1.097a512 512 0 01419.474 419.474 73.143 73.143 0 01-72.046 85.212H591.36a73.143 73.143 0 01-73.143-73.143z"></path><path d="M493.714 566.857h340.297a73.143 73.143 0 0173.143 85.577A457.143 457.143 0 11371.566 117.76a73.143 73.143 0 0185.577 73.143v339.383a36.571 36.571 0 0036.571 36.571z"></path></svg><span>约 14855 字</span><meta property="wordCount" content="14855"></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 50 分钟</span><meta property="timeRequired" content="PT50M"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span class="waline-pageview-count" id="ArtalkPV" data-path="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html">...</span></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<!----></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#基础知识" class="router-link-active router-link-exact-active toc-link level2">基础知识</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#数据分析" class="router-link-active router-link-exact-active toc-link level2">数据分析</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#机器学习" class="router-link-active router-link-exact-active toc-link level2">机器学习</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E4%B8%AA%E9%97%AE%E9%A2%98.html#深度学习" class="router-link-active router-link-exact-active toc-link level2">深度学习</a></li><!----><!--]--></ul></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="机器学习100个问题" tabindex="-1"><a class="header-anchor" href="#机器学习100个问题" aria-hidden="true">#</a> 机器学习100个问题</h1><h2 id="基础知识" tabindex="-1"><a class="header-anchor" href="#基础知识" aria-hidden="true">#</a> <strong>基础知识</strong></h2><p><strong>1、什么是数据科学?列出监督学习和非监督学习的区别。</strong></p><p>数据科学是各种工具、算法和机器学习方法的混合，其目标是从原始数据中发现隐藏的模式。这与统计学家多年来一直在做的事情相似但是有什么不同?下图解释了数据分析和数据科学的区别：</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!436dc6374098494cafbae43fe0c4c56a-c8bbf273.png" alt="http://images.overfit.cn/upload/20220626/436dc6374098494cafbae43fe0c4c56a.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/436dc6374098494cafbae43fe0c4c56a.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/436dc6374098494cafbae43fe0c4c56a.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>监督学习和无监督学习的区别如下:</p><p>有监督学习：输入数据是有标记的，主要用于预测。例如分类和回归等</p><p>无监督学习：输入数据是没有标记的，多用于分析。密度估计和降维等</p><p><strong>2、什么是选择偏差?</strong></p><p>选择性偏差溯源英文为Selection Bias，指的是在研究过程中因样本选择的非随机性而导致得到的结论存在偏差,也称选择性偏差为选择性效应（Selection Effect）。它是由于采集样本的方法造成的统计分析的失真。如果没有考虑到选择偏差，那么研究的一些结论可能不准确。</p><p>选择性偏差是指这样一种认知倾向：人们喜欢把事物分为典型的几个类别，然后在对事件进行概率估计时，过分强调这种典型类别的重要性，而不顾有关其他潜在可能性的证据。选择性偏差的后果势必使人们倾向于在实际上是随机的数据序列中“洞察”到某种模式，从而造成系统性的预测偏差。</p><p><strong>3、什么是偏差-方差权衡?</strong></p><p>偏差:偏差是由于机器学习算法过于简化而在模型中引入的错误。它会导致不适应。当你在那个时候训练你的模型时，模型会简化假设，使目标函数更容易理解。</p><p>低偏差机器学习算法有：决策树，k-NN和SVM，高偏差机器学习算法有：线性回归，逻辑回归</p><p>方差:方差是由于复杂的机器学习算法在模型中引入的误差，模型会从训练数据集学习噪声，在测试数据集上表现很差。它会导致高灵敏度和过拟合。</p><p>通常，当增加模型的复杂性时，会看到由于模型中较低的偏差而导致的误差的减少。然而，这种情况只会在特定的点发生。当模型变得更复杂时，最终会过度拟合型，因此你的模型将开始变为i高方差。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!b349d2796fe74be79d9341f1db459069-4dea8113.png" alt="http://images.overfit.cn/upload/20220626/b349d2796fe74be79d9341f1db459069.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/b349d2796fe74be79d9341f1db459069.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/b349d2796fe74be79d9341f1db459069.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>任何监督机器学习算法的目标都是具有低偏差和低方差，才能达到良好的预测性能。在机器学习中，偏见和方差之间的关系不可避免。增加偏差会减少方差。增加方差会减少偏差。</p><p><strong>4、任意语言，编写一个程序输出从1到50的数字</strong></p><p>打印1到50之间的数字的python代码如下-</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>for i in range(1,51):print(i)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p><strong>5、什么是混淆矩阵?</strong></p><p>混淆矩阵是一个2X2表，包含由二分类器提供的4个输出。错误率、准确率、精确度、查全（召回）率等指标都由它来衡量。混淆矩阵</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!ecae8dbf6652446983406301a445c7d3-5834c7f2.png" alt="http://images.overfit.cn/upload/20220626/ecae8dbf6652446983406301a445c7d3.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/ecae8dbf6652446983406301a445c7d3.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/ecae8dbf6652446983406301a445c7d3.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>用于性能评估的数据集称为测试数据集。它应该包含正确的标签和预测的标签。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!400fdaf87b63464eaf51173230d0b3b6-48cbb4ae.png" alt="http://images.overfit.cn/upload/20220626/400fdaf87b63464eaf51173230d0b3b6.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/400fdaf87b63464eaf51173230d0b3b6.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/400fdaf87b63464eaf51173230d0b3b6.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>如果分类器的性能是完美的，预测的标签将完全相同。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!31f0ffc61fcd480fa1c32683d45968a5-fa2dab25.png" alt="http://images.overfit.cn/upload/20220626/31f0ffc61fcd480fa1c32683d45968a5.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/31f0ffc61fcd480fa1c32683d45968a5.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/31f0ffc61fcd480fa1c32683d45968a5.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>但实际上模型预测的标签通常与现实场景中部分观察到的标签相匹配。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!af5723cb201b421b8c405076cd9be3b1-2e580d70.png" alt="http://images.overfit.cn/upload/20220626/af5723cb201b421b8c405076cd9be3b1.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/af5723cb201b421b8c405076cd9be3b1.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/af5723cb201b421b8c405076cd9be3b1.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>分类器预测测试数据集的所有数据实例为正或负。这产生了四种结果</p><p>真阳性(TP) -正确的阳性预测</p><p>假阳性(FP) -不正确的阳性预测</p><p>真负(TN) -正确的负预测</p><p>假阴性(FN) -错误的阴性预测</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!2cf147c3b6db4c6ea671f5235a7ca884-38e5f5e3.png" alt="http://images.overfit.cn/upload/20220626/2cf147c3b6db4c6ea671f5235a7ca884.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/2cf147c3b6db4c6ea671f5235a7ca884.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/2cf147c3b6db4c6ea671f5235a7ca884.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>由混淆矩阵推导出的基本度量有以下概念</p><p>错误率= (FP+FN)/(P+N)</p><p>精度= (TP + TN) / (P + N)</p><p>敏感度（<strong>Sensitivity</strong>真阳性率)= TP/P</p><p>特异度(<strong>Specificity</strong>真阴性率)= TN/N</p><p>精度(阳性预测值)= TP/(TP+FP)</p><p>F-Score(精度和查全率的调和平均值)= (1+b)(PREC.REC)/(b²PREC+REC)其中b通常为0.5,1,2。</p><p><strong>6、怎么理解真阳性率和假阳性率?</strong></p><p>真阳性率(TPR)是真阳性与真阳性和假阴性的比率。它是实际阳性结果被测试为阳性的概率。</p><p>TPR = TP / （TP + FN）</p><p>假阳性率(FPR)是假阳性与所有阳性(真阳性和假阳性)的比率。它是虚惊一场的概率，也就是说，当它实际上是负的时候，会给出一个正的结果。</p><p>FPR= FP / （TP + FP）</p><p><strong>7、简述马尔可夫链</strong></p><p>马尔可夫链是一种随机过程。在马尔可夫链中，任何状态的未来概率只取决于当前状态。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!c10d57f752b6496da55c9b62be5d6f84-76284669.png" alt="http://images.overfit.cn/upload/20220626/c10d57f752b6496da55c9b62be5d6f84.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/c10d57f752b6496da55c9b62be5d6f84.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/c10d57f752b6496da55c9b62be5d6f84.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>上图表示了一个马尔可夫链模型，其中每个步骤都有一个输出，只依赖于当前状态。</p><p>例如文字推荐。当我们输入一个段落时，这个模型会提示下一个单词，它只依赖于前一个单词，而不依赖于它之前的任何单词。马尔科夫链模型之前在一个类似的段落上进行训练，其中给定单词的下一个单词存储在训练数据中的所有单词上。根据这个训练数据输出，建议接下来的单词。</p><p><strong>8、ROC曲线是什么?</strong></p><p>ROC曲线是假阳性率(x轴)和真阳性率(y轴)之间的曲线。真阳性率是指真阳性率与阳性样本总数之比。假阳性率是假阳性与阴性样本总数之比。在几个阈值上绘制FPR和TPR，构建ROC曲线。ROC曲线下的面积范围为0 ~ 1。完全随机模型的ROC为0.5，用直线表示。ROC曲线偏离这条直线越多，模型越好。ROC曲线用于二元分类。下图展示了ROC曲线的一个例子。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!1dcd31925da54849a5fb4436ceab4bd1-5ee30436.png" alt="http://images.overfit.cn/upload/20220626/1dcd31925da54849a5fb4436ceab4bd1.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/1dcd31925da54849a5fb4436ceab4bd1.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/1dcd31925da54849a5fb4436ceab4bd1.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>9、什么是降维及其好处?</strong></p><p>减少给定数据集的特征数量被称为降维。有许多技术用于降低维度，如-</p><ul><li>特征选择</li><li>矩阵分解</li><li>Manifold 学习</li><li>Autoencoder方法</li><li>线性判别分析(LDA)</li><li>主成分分析(PCA)</li></ul><p>降维的主要原因之一是“降维魔咒”。当特征的数量增加时，模型变得更加复杂。但如果数据点较少，模型将开始学习过拟合数据。模型不会泛化。这就是众所周知的“维度诅咒”。</p><p>降低维度的其他好处包括-</p><ul><li>减少了时间和存储空间。</li><li>用2D或3D可视化和可视化表示数据变得更容易了。</li><li>空间复杂度降低。</li></ul><p><strong>10、如何在线性回归模型中找到RMSE和MSE ?</strong></p><p>采用均方根误差(RMSE)来检验线性回归模型的性能。它评估在最佳拟合线上分布了多少数据。MSE的公式是</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!4bb67f6a395d41b295811594a2c4fed4-4c35a07d.png" alt="http://images.overfit.cn/upload/20220626/4bb67f6a395d41b295811594a2c4fed4.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/4bb67f6a395d41b295811594a2c4fed4.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/4bb67f6a395d41b295811594a2c4fed4.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>f_i是预测值</p><p>Y_i是输出变量的实际值。</p><p>N是数据点的个数</p><p>均方误差(MSE)表示直线与实际数据的接近程度。取直线与数据点的差值并平方。对于一个好的模型，MSE值应该很低。这意味着实际输出值和预测输出值之间的误差应该很低。</p><p><strong>11、如何处理不平衡的二元分类?</strong></p><p>在进行二分类时，如果数据集不平衡，仅使用R2评分无法正确预测模型的精度。例如，如果属于其中一个类的数据在数量上比属于另一个类的数据少得多，那么传统的精度将在较小的类中占很小的百分比。如果只有5%的示例属于较小的类，而模型将属于其他类的所有输出分类，精度仍然在95%左右。但这是错误的。为了解决这个问题，我们可以这样做</p><ul><li>使用其他方法来计算模型性能，如精度/召回率，F1评分等。</li><li>使用以下技术对数据重新采样(减少较大类的样本大小)、过采样(使用重复、SMOTE和其他此类技术增加较小类的样本大小)。</li><li>使用K-fold交叉验证</li><li>使用集成学习，使每棵决策树考虑小类的整个样本，而只考虑大类的一个子集。</li></ul><p><strong>12、箱线图和直方图的区别是什么</strong></p><p>直方图和箱线图都用于直观地表示某一特征值的频率。下图显示了一个直方图。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!76d6d187911e4231a3aeb8fa02df2f02-05969002.png" alt="http://images.overfit.cn/upload/20220626/76d6d187911e4231a3aeb8fa02df2f02.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/76d6d187911e4231a3aeb8fa02df2f02.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/76d6d187911e4231a3aeb8fa02df2f02.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>下图为箱线图现实的相同数据</p><figure><img src="data:image/png;base64,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" alt="http://images.overfit.cn/upload/20220626/cdb88fb10e8a419e8df232d15520bd02.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/cdb88fb10e8a419e8df232d15520bd02.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/cdb88fb10e8a419e8df232d15520bd02.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>直方图用于了解数据的潜在概率分布。箱线图更多地用于比较多个数据集。箱线图比直方图有更少的细节和占用更少的空间。</p><p><strong>13、NLP都有什么主要的工作?</strong></p><p>NLP代表自然语言处理。它是对计算机编程来学习大量文本数据的研究。NLP的例子包括标记化、停止词去除、词根提取、情感分析等。</p><p><strong>14、概率论的基本原理</strong></p><p>在所有可能的结果中，某一事件发生的可能性被称为其概率。事件发生的概率总是在(包括)0和1之间。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!687b8155783542a598d7ca56fda96422-cccfc256.png" alt="http://images.overfit.cn/upload/20220626/687b8155783542a598d7ca56fda96422.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/687b8155783542a598d7ca56fda96422.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/687b8155783542a598d7ca56fda96422.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>加法规则：P(A，B)= P(A) + P(B) - P(A and B)</p><p>条件概率：它是事件B发生的概率，假设事件A已经发生。</p><p>P(A and B)= P(A)P (B|A)</p><p>中心极限定理：当我们从一个大总体中抽取随机样本，然后取这些样本的均值，它们形成一个正态分布。</p><p><strong>15、描述不同的正则化方法，如L1和L2正则化</strong></p><p>有3种重要的正则化方法如下：</p><p>L2正则化-(Ridge回归)-在L2正则化中，我们将所有权重的平方和，乘以一个值lambda，加到损失函数。Ridge回归公式为-</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!b9d437fbfe734e3f9ad627d621415ec0-50d57a57.png" alt="http://images.overfit.cn/upload/20220626/b9d437fbfe734e3f9ad627d621415ec0.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/b9d437fbfe734e3f9ad627d621415ec0.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/b9d437fbfe734e3f9ad627d621415ec0.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>可以看到，如果某一特定数据点和特征的权值乘以数据值变得非常大，那么原始的损失就会变得很小。但是增加值乘以权重平方和也会变大。同样如果原来的损失价值变得很大，那么增加的价值就会变小。因此它将控制最终值不变得太大或太小。</p><p>L1正则化-(Lasso回归)-在L1正则化中，我们将所有权重的绝对值加和，乘以一个值lambda，到损失函数。Lasso回归公式为-</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!4fd2db2ea58240fba7550f633dcfa2ca-4f9b019e.png" alt="http://images.overfit.cn/upload/20220626/4fd2db2ea58240fba7550f633dcfa2ca.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/4fd2db2ea58240fba7550f633dcfa2ca.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/4fd2db2ea58240fba7550f633dcfa2ca.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>损失函数和优化算法使参数趋近于零而不是实际为零，而lasso则消除了不太重要的特征，并将各自的权重值设置为零。</p><p>Dropout用于神经网络的正则化。全连接层更容易过拟合。Dropout丢掉了神经网络中一些概率为1-p的神经元。Dropout减少了过拟合，提高了训练速度，使模型更健壮。</p><p><strong>16、应该如何维护已部署的模型?</strong></p><p>在一个模型被部署之后，它输入的数据可能会随时间而改变。例如，在预测房价的模型中，房价可能会随着时间的推移而上涨，也可能会因为其他一些因素而波动。所以模型在新数据上的准确性可以被记录下来。一些确保准确性的常用方法包括-</p><ul><li>应经常通过输入阴性试验数据对模型进行检查。如果模型给出的精度较低且测试数据为负，则说明需要更新。</li><li>建立自动编码器，利用异常检测技术，AE模型计算重构误差值。如果重构错误值很高，这意味着新数据没有遵循模型学习到的旧模式。</li><li>如果模型对新数据显示出较好的预测精度，则说明新数据遵循模型对旧数据学习到的模式或泛化。因此，可以根据新数据对模型进行重新训练。如果新数据的准确性不是很好，那么可以使用对数据特征和旧数据进行特征工程的方法对新数据重新训练模型。</li><li>如果准确性不好，模型可能需要从头开始训练。</li></ul><p><strong>17、写出公式，计算准确率和召回率。</strong></p><p>Precision = True Positives / (True Positives + False Positives)</p><p>Recall = True Positives / (True Positives + False Negatives)</p><p><strong>18、如何在NumPy中测量两个数组之间的欧氏距离?</strong></p><p>2个阵列A[1,2,3，]和b[8,9,10]之间的欧氏距离可以通过分别取每个点的欧氏距离来计算。使用numpy. linalgy .norm()-</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!5e1c6837c4a74b869b768dee63871948-0db21ed3.png" alt="http://images.overfit.cn/upload/20220626/5e1c6837c4a74b869b768dee63871948.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/5e1c6837c4a74b869b768dee63871948.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/5e1c6837c4a74b869b768dee63871948.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>19、误差和剩余误差的区别是什么?</strong></p><p>误差是指预测值与实际值之间的差值。数据科学中最常用的误差计算方法是平均绝对误差(MAE)、均方误差(MSE)和均方根误差(RMSE)。而剩余误差是一组观测值与其算术平均值之间的差。误差通常是不可观察的，而剩余误差可以在图上显示出来。误差表示观测数据与实际总体的差异。而剩余误差则表示观察数据与样本总体数据的差异。</p><p><strong>20、归一化和标准化的区别?</strong></p><p>归一化，也称为最小-最大缩放，是一种将所有数据值转换为介于0和1之间的技术。</p><p>归一化公式是-</p><figure><img src="data:image/png;base64,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" alt="http://images.overfit.cn/upload/20220626/a0f1006987d347fd8b5330bf92d8887a.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/a0f1006987d347fd8b5330bf92d8887a.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/a0f1006987d347fd8b5330bf92d8887a.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>X_max是该特性的最大值</p><p>X_min是该特征的最小值</p><p>标准化是指将我们的数据进行转换，使其具有均值为0，标准差为1的正态分布。</p><p>标准化的公式是-</p><figure><img src="data:image/png;base64,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" alt="http://images.overfit.cn/upload/20220626/f1bd156468364a12856d326702a7c3b3.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/f1bd156468364a12856d326702a7c3b3.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/f1bd156468364a12856d326702a7c3b3.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>因此，虽然标准化只将数据缩放到0到1的范围，但标准化确保数据遵循标准的正态分布。</p><p><strong>21、你对正态分布有什么理解?</strong></p><p>数据通常以不同的方式分布，有向左或向右的偏差，也可能全部混杂在一起。</p><p>然而，也有可能数据分布在中心值周围，没有任何向左或向右的偏差，并以钟形曲线的形式达到正态分布。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!528056e23dd549dca84e6509aa6ae07a-e7d7fdff.png" alt="http://images.overfit.cn/upload/20220626/528056e23dd549dca84e6509aa6ae07a.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/528056e23dd549dca84e6509aa6ae07a.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/528056e23dd549dca84e6509aa6ae07a.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>正态分布的性质如下;</p><p>单峰，左右镜像对称，钟形-最大高度(模式)在平均值，均值、众数和中位数都位于中心</p><p><strong>22、什么是统计学上的相关性和协方差?</strong></p><p>协方差和相关是两个数学概念;这两种方法在统计学中被广泛使用。相关和协方差都建立了关系，也衡量两个随机变量之间的依赖性。虽然从数学的角度来说，这两者的工作是相似的，但他们彼此不同。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!35ba8a0d164047bb9561d41b08e51fce-201d4557.png" alt="http://images.overfit.cn/upload/20220626/35ba8a0d164047bb9561d41b08e51fce.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/35ba8a0d164047bb9561d41b08e51fce.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/35ba8a0d164047bb9561d41b08e51fce.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>相关性:相关被认为或描述为测量和估计两个变量之间的定量关系的最佳技术。相关性衡量的是两个变量的相关性有多强。</p><p>协方差:在协方差中，两个项目一起变化，它是一个衡量两个随机变量在周期变化的程度。这是一个统计术语;它解释了一对随机变量之间的系统关系，其中一个变量的变化与另一个变量的相应变化互为倒数。</p><p><strong>23、点估计和置信区间的区别是什么?</strong></p><p>点估计给我们一个特定的值作为总体参数的估计。采用矩量法和极大似然估计法导出了总体参数的点估计。</p><p>置信区间为我们提供了一个可能包含总体参数的值范围。通常首选置信区间，因为它告诉我们这个区间包含总体参数的可能性有多大。这种可能性或概率称为置信水平或置信系数，用1 - alpha表示，其中alpha是显著性水平。</p><p><strong>24、A/B测试的目标是什么?</strong></p><p>这是对一个有两个变量a和B的随机实验的假设检验。</p><p>A/B测试的目标是识别变化，以最大限度地找到改变实验动作后对结果产生的影响。</p><p><strong>25、P值是什么?</strong></p><p>当在统计学中进行假设检验时，p值可以帮助您确定结果的强度。p值是0到1之间的一个数字。它将根据值表示结果的强度。这个正在试验的主张被称为零假设。</p><p>较低的p值(≤0.05)意味着我们可以拒绝原假设。高p值(≥0.05)这意味着可以接受零假设，p值为0.05表明假设可以双向。</p><p><strong>26，概率计算：在任何15分钟的间隔内，有20%的概率你会看到至少一颗流星。你在一小时内看到至少一颗流星的概率是多少?</strong></p><p>十五分钟内看不到流星的概率是</p><p>= 1 - P(看到一颗流星)= 1-0.2 = 0.8</p><p>在一小时内看不到任何流星的概率</p><p>= (0.8) ^ 4 = 0.4096</p><p>在一小时内看到至少一颗流星的概率</p><p>= 1 - P(看不到任何流星)= 1-0.4096 = 0.5904</p><p><strong>27、如何用一个骰子产生1-7之间的随机数?</strong></p><p>任何骰子有从1到6有6个面。一次掷骰子不可能得到7个相同的结果。如果我们掷骰子两次，考虑两次的事件，我们现在有36种不同的结果。为了得到7个相等的结果我们要把36化简成能被7整除的数。因此可以只考虑35种结果，并排除其中的一种。</p><p>一个简单的场景便是排除组合(6,6)，即如果6出现两次便再次掷骰子。从(1,1)到(6,5)的所有剩余组合可以分为7个部分，每个部分5。这样七组结果都是等可能的。</p><p><strong>28、一对夫妇告诉你他们有两个孩子，其中至少有一个是女孩。他们有两个女孩的概率是多少?</strong></p><p>在两个孩子的情况下，有4种等可能的事件</p><p>BB、BG、GB、GG;</p><p>其中B =男孩，G =女孩，第一个字母表示第一个孩子。</p><p>从问题中，我们可以排除第一种BB。因此，从BG, GB, BB剩下的3种可能性中，我们必须找出两个女孩的情况的概率。</p><p>因此，P(有两个女孩给一个女孩)= 1 / 3</p><p><strong>29、一个罐子有1000枚硬币，其中999枚是正常的，1枚的两面都是正面。随机选择一枚硬币，投掷10次。假设你看到10个正面，那枚硬币下一次扔出来也是正面的概率是多少?</strong></p><p>选择硬币有两种方法。一种是选出一枚正常的硬币，另一种是选出两个正面的硬币。</p><p>选择正常硬币的概率= 999/1000 = 0.999</p><p>选择非正常硬币的概率= 1/1000 = 0.001</p><p>连续选择10个正面=选择正常硬币*得到10个正常+选择一枚非正常硬币</p><p>P (A) = 0.999 *(1/2)⁵= 0.999 * (1/1024)= 0.000976</p><p>P (b) = 0.001 * 1 = 0.001</p><p>P(a / a + b) = 0.000976 / (0.000976 + 0.001) = 0.4939</p><p>P(b / a + b) = 0.001 / 0.001976 = 0.5061</p><p>选择另一个正面的概率= P(A/A+B) * 0.5 + P(B/A+B) * 1 = 0.4939 * 0.5 + 0.5061 = 0.7531</p><p><strong>30、你对敏感度（Sensitivity）的统计能力有什么理解?你如何计算它?</strong></p><p>敏感度通常被用来验证分类器的准确性(Logistic, SVM, Random Forest等)。</p><p>敏感度是“预测的真实事件/总事件”。真实事件是指真实的事件模型也预测了它们是真实的。</p><p>计算非常简单。敏感度（<strong>Sensitivity</strong>真阳性率)= TP/P</p><p><strong>31、为什么要重采样?</strong></p><ul><li>通过使用可访问数据的子集或从一组数据点中随机抽取替换数据来估计样本统计数据的准确性</li><li>执行显著性检验时，在数据点上替换标签</li><li>通过使用随机子集(bootstrapping, cross-validation)来验证模型</li></ul><p><strong>32、过拟合和欠拟合有什么区别?</strong></p><p>在统计学和机器学习中，最常见的任务之一就是将模型拟合到一组训练数据中，从而能够对一般的未经训练的数据做出可靠的预测。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!c6044c6f0ad3499e9afd22c36e8a222b-a6a3aa6e.png" alt="http://images.overfit.cn/upload/20220626/c6044c6f0ad3499e9afd22c36e8a222b.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/c6044c6f0ad3499e9afd22c36e8a222b.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/c6044c6f0ad3499e9afd22c36e8a222b.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>在过拟合中，统计模型描述的是随机误差或噪声，而不是潜在的关系。过拟合发生在一个模型过于复杂的时候，比如相对于观测数据有太多的参数。过拟合的模型预测性能较差，因为它对训练数据的微小波动反应过度。</p><p>当统计模型或机器学习算法无法捕捉数据的潜在趋势时，就会发生欠拟合。例如，当用线性模型拟合非线性数据时，就会出现欠拟合。这种模型的预测性能也很差。</p><p><strong>33、避免对抗过拟合和过拟合?</strong></p><p>为了避免过拟合和欠拟合，可以重采样数据来估计模型的准确性(k倍交叉验证)，并通过验证数据集来评估模型。</p><p><strong>34、什么是正则化?为什么它有用?</strong></p><p>正则化可以防止过拟合。一般情况下是通过在现有的权重向量上加上一个常数倍数来实现的。这个常数通常是L1(Lasso)或L2(ridge)。然后模型预测应该最小化在正则化训练集上计算的损失函数。</p><p><strong>35、什么是大数定律?</strong></p><p>这个定理，描述了进行大量相同实验的结果。这个定理构成了频率式思维的基础：样本均值，样本方差和样本标准差收敛于他们试图估计的值。</p><p><strong>36、什么是混淆的变量？</strong></p><p>在统计数据中，混淆因素是一个影响因变量和独立变量的变量。混淆变量(Confounding Variable)是指与自变量和因变量均相关的变量，该变量使自变量和因变量间产生虚假的关系(Meinert, 1986)。</p><p>例如，如果您正在研究缺乏运动会导致体重增加，</p><p>缺乏运动=自变量</p><p>体重增加=分支变量。</p><p>这里的混淆变量将是影响这两个变量的任何变量例如受试者的年龄。</p><p><strong>37、抽样过程中可能发生的偏差都有哪些类型？</strong></p><ul><li>Selection bias</li><li>Under coverage bias</li><li>Survivorship bias</li></ul><p><strong>38、什么是生存偏差（Survivorship bias）?</strong></p><p>这是一个逻辑上的错误，即专注于支持幸存某些过程的方面，而忽略那些因为它们不突出而不起作用的方面。这可能会以各种不同的方式得出错误的结论。</p><p><strong>39、什么是选择偏差（Selection bias）?</strong></p><p>当获得的样本不能代表要分析的总体时，就会出现选择偏差。</p><p><strong>40、解释ROC曲线是如何工作的?</strong></p><p>ROC曲线是各种阈值下真实阳性率和假阳性率对比的图形表示。它经常被用作敏感性(真阳性率)和假阳性率之间权衡的标准。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!93b00b52a23b45749fc02cc6eb4e93bd-2d00bc32.png" alt="http://images.overfit.cn/upload/20220626/93b00b52a23b45749fc02cc6eb4e93bd.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/93b00b52a23b45749fc02cc6eb4e93bd.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/93b00b52a23b45749fc02cc6eb4e93bd.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>41、什么是TF/IDF?</strong></p><p>TF-IDF是 term frequency-inverse document frequency,的缩写，是反映一个词对集合或语料库中的文档的重要性的统计数字。在信息检索和文本挖掘中，它经常被用作加权因子。</p><p>TF-IDF值与单词在文档中出现的次数成比例增加，但会被单词在语料库中的出现频率所抵消，这有助于调整某些单词在一般情况下出现的频率更高的事实。</p><p><strong>42、为什么我们一般使用Softmax非线性函数作为网络最后一个操作?</strong></p><p>这是因为它采用了实数的向量并返回概率分布。它的定义如下。令X为实数的向量（正，负，无论如何，没有约束）。</p><p>则Softmax(x)的第i个分量为-</p><figure><img src="/assets/http!images.overfit.cn!upload!20220626!c27e687902e44709b4fdaee6f8ea5439-599915c7.png" alt="http://images.overfit.cn/upload/20220626/c27e687902e44709b4fdaee6f8ea5439.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220626/c27e687902e44709b4fdaee6f8ea5439.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220626/c27e687902e44709b4fdaee6f8ea5439.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>输出是概率分布：每个元素都是非负分布，所有分量的总和为1。</p><h2 id="数据分析" tabindex="-1"><a class="header-anchor" href="#数据分析" aria-hidden="true">#</a> <strong>数据分析</strong></h2><p><strong>43、数据清理如何在分析中发挥重要作用?</strong></p><p>数据清理可以帮助分析，因为:</p><ul><li>清理来自多个源的数据有助于将其转换为数据分析师或数据科学家可以使用的格式。</li><li>在机器学习中，数据清洗有助于提高模型的准确性。</li><li>这是一个繁琐的过程，因为随着数据源数量的增加，由于数据源的数量和这些数据源生成的数据量，清理数据所需的时间呈指数增长。</li><li>清理数据可能要花费多达80%的时间，这使得它成为分析任务的关键部分。</li></ul><p><strong>44、单变量、双变量和多变量分析。</strong></p><p>单变量分析是一种描述性统计分析技术，可以根据在给定的时间点所涉及的变量的数量进行区分。例如，基于地域的销售饼图只涉及一个变量，分析可以称为单变量分析。</p><p>双变量分析试图在散点图中理解两个变量在同一时间的差异。例如，分析销售和支出的数量可以被认为是双变量分析的一个例子。</p><p>多变量分析涉及两个以上变量的研究，以了解变量对反应的影响。</p><p><strong>45、解释星型模型</strong></p><p>它是具有中心表的传统数据库模式。附属表将ID映射到物理名称或描述，可以使用ID字段连接到中心事实表;这些表被称为查找表，主要用于实时应用程序，因为它们可以节省大量内存。有时星型模式涉及多个汇总层以更快地获取相应的信息。</p><p><strong>46、什么是整群抽样（Cluster sampling）?</strong></p><p>整群抽样是指整群地抽选样本单位，对被抽选的各群进行全面调查的一种抽样组织方式。例如，检验某种零件的质量时，不是逐个抽取零件，而是随机抽若干盒 (每盒装有若干个零件)，对所抽各盒零件进行全面检验。如果全及总体划分为单位数目相等的R个群，用不重复抽样方法，从R群中抽取r群进行调查。</p><p><strong>47、什么是系统抽样（Systematic Sampling）?</strong></p><p>先将总体的全部单元按照一定顺序排列，采用简单随机抽样抽取第一个样本单元(或称为随机起点)，再顺序抽取其余的样本单元，这类抽样方法被称为等距抽样(Systematic Sampling)。等距抽样又称为机械抽样、系统抽样。等距抽样往往不能给出估计量的估计方差。</p><p><strong>48、什么是特征向量（Eigenvectors）和特征值（Eigenvalues）?</strong></p><p>特征向量用于理解线性变换。在数据分析中，通常计算相关或协方差矩阵的特征向量。特征向量是特定线性变换通过翻转、压缩或拉伸作用的方向。</p><p>特征值可以被认为是在特征向量方向上的变换强度或压缩发生的因子。</p><p><strong>49、你能举出一些假阳性比假阴性重要的例子吗?</strong></p><p>假阳性是指错误地将非事件分类为事件，也就是第一类错误。假阴性是指错误地将事件归类为非事件的情况，也就是第二类错误。</p><p>在医疗领域，例如癌症检查他的癌症检测呈阳性，但他实际上没有癌症。这是一个假阳性的案例。在这个病人没有癌症的情况下对他进行化疗是非常危险的。在没有癌细胞的情况下，化疗会对他正常健康的细胞造成一定的损害，可能导致严重的疾病，甚至癌症。</p><p><strong>50、你能举出一些假阴性比假阳性重要的例子吗?</strong></p><p>假设有一个机场安检如果一个真正有威胁的客户被机场模型标记为无威胁，陪审团或法官决定释放犯罪的罪犯都是这种情况</p><p><strong>51、你能举出一些假阳性和假阴性同样重要的例子吗?</strong></p><p>在银行业，贷款是赚钱的主要来源，如果你的还款率不好，银行向你贷款面临巨大的损失风险。银行不想失去好客户，也不想获得差客户。在这种情况下假阳性和假阴性都变得非常重要。</p><p><strong>52、您能解释一下验证集和测试集之间的区别吗?</strong></p><p>验证集可以被认为是训练集的一部分，因为它用于参数选择和避免模型的过拟合。测试集用于测试或评估训练好的机器学习模型的性能。</p><p>简单地说，区别可以概括为;训练集是拟合参数，验证集是测试训练集的效果;测试集是评估模型的性能</p><p><strong>53、解释交叉验证</strong></p><p>交叉验证是一种模型验证技术，用于评估统计分析结果如何推广到独立数据集。主要用于预测目标和估计模型在实践中实现的准确性的背景。</p><p>交叉验证的目标是定义一个数据集来在训练阶段测试模型(即验证数据集)，以限制过拟合等问题，并深入了解模型将如何推广到一个独立的数据集。</p><h2 id="机器学习" tabindex="-1"><a class="header-anchor" href="#机器学习" aria-hidden="true">#</a> <strong>机器学习</strong></h2><p><strong>54、什么是机器学习?</strong></p><p>机器学习是一门多学科交叉专业，涵盖概率论知识，统计学知识，近似理论知识和复杂算法知识，使用计算机作为工具并致力于真实实时的模拟人类学习方式，并将现有内容进行知识结构划分来有效提高学习效率。</p><p>机器学习有下面几种定义：</p><p>（1）机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。</p><p>（2）机器学习是对能通过经验自动改进的计算机算法的研究。</p><p>（3）机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。</p><p><strong>55、什么是无监督学习?</strong></p><p>无监督学习是一种机器学习算法，用于从由输入数据组成的数据集中推断，并且学习时不需要对数据进行标记。</p><p>主要包括：聚类，降维，异常检测等</p><p><strong>56、有哪些不同的分类算法?</strong></p><p>下图列出了最重要的分类算法。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!f4fb8eacd20141cdaba30f6cc85f7c7f-042f97f1.png" alt="http://images.overfit.cn/upload/20220627/f4fb8eacd20141cdaba30f6cc85f7c7f.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/f4fb8eacd20141cdaba30f6cc85f7c7f.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/f4fb8eacd20141cdaba30f6cc85f7c7f.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>57、朴素贝叶斯中的“朴素”是什么?</strong></p><p>朴素贝叶斯算法是基于贝叶斯定理的。贝叶斯定理描述了一个事件发生的概率，基于可能与该事件相关的条件的先验知识。</p><p>这个算法很“幼稚”，因为它所做的假设可能是正确的，也可能不是。</p><p><strong>58、如何建立随机森林模型?</strong></p><p>随机森林模型结合了许多决策树模型。所选择的决策树具有高偏差和低方差。每个决策树都取样本的子集，并进行预测。每棵决策树的结果都被记录下来，并以大多数作为答案，在分类问题中是众数，在回归问题中是均值和中位数。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!1904b70a8ffc4e7d99fe6cf39146029c-801e64e9.png" alt="http://images.overfit.cn/upload/20220627/1904b70a8ffc4e7d99fe6cf39146029c.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/1904b70a8ffc4e7d99fe6cf39146029c.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/1904b70a8ffc4e7d99fe6cf39146029c.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>59、详细解释SVM算法</strong></p><p>SVM是支持向量机的缩写，它是一种监督机器学习算法，可以用于回归和分类。如果你的训练数据集中有n个特征，SVM尝试在n维空间中绘制它，每个特征的值是特定坐标的值。SVM基于所提供的核函数，利用超平面分离出不同的类。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!e1ec593021c14024a0986e5671257118-e73beda9.png" alt="http://images.overfit.cn/upload/20220627/e1ec593021c14024a0986e5671257118.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/e1ec593021c14024a0986e5671257118.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/e1ec593021c14024a0986e5671257118.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>60、支持向量机中的支持向量是什么?</strong></p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!96defe17e8294401bc546253400bac11.jpeg-d829d3a4.jpg" alt="http://images.overfit.cn/upload/20220627/96defe17e8294401bc546253400bac11.jpeg" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/96defe17e8294401bc546253400bac11.jpeg" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/96defe17e8294401bc546253400bac11.jpeg<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>在图中，我们看到细线标记了从分类器到最近的数据点(称为支持向量)的距离(黑色的数据点)。两条细线之间的距离叫做边距。</p><p><strong>61、支持向量机的核函数有哪些?</strong></p><p>支持向量机中一般使用四种核函数。</p><p>线性核、多项式的核、径向基核、Sigmoid 核</p><p><strong>62、详细解释决策树算法</strong></p><p>决策树是一种监督机器学习算法，主要用于回归和分类。它将数据集分解为越来越小的子集，同时逐步开发相关的决策树。最终的结果是一个具有决策节点和叶子节点的树。决策树可以同时处理分类数据和数值数据。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!827ada792aff43b08d4653296ea5f4e9-b7757656.png" alt="http://images.overfit.cn/upload/20220627/827ada792aff43b08d4653296ea5f4e9.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/827ada792aff43b08d4653296ea5f4e9.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/827ada792aff43b08d4653296ea5f4e9.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>63、决策树算法中的熵和信息增益是什么?</strong></p><p>构建决策树的核心算法有·ID3、C45等。ID3使用熵和信息增益来构造决策树。</p><p>熵：决策树是从根节点自上而下构建的，涉及到将数据划分为同构子集。ID3使用熵来检验样本的同质性。如果样本是完全均匀的，那么熵就是0如果样本是等分的，那么熵就是1。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!d67840c0fdf14fd893569e627c6b47b3-c83ea711.png" alt="http://images.overfit.cn/upload/20220627/d67840c0fdf14fd893569e627c6b47b3.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/d67840c0fdf14fd893569e627c6b47b3.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/d67840c0fdf14fd893569e627c6b47b3.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>信息增益是基于数据集在属性上分割后熵的减小。构建决策树是关于寻找返回最高信息收益的属性。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!830392beedaf49f6ba122ad1b582ca06-a0f0e4a7.png" alt="http://images.overfit.cn/upload/20220627/830392beedaf49f6ba122ad1b582ca06.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/830392beedaf49f6ba122ad1b582ca06.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/830392beedaf49f6ba122ad1b582ca06.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><figure><img src="/assets/http!images.overfit.cn!upload!20220627!555670c65ade43d5a2a56e9ebe1a35d2-070fac25.png" alt="http://images.overfit.cn/upload/20220627/555670c65ade43d5a2a56e9ebe1a35d2.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/555670c65ade43d5a2a56e9ebe1a35d2.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/555670c65ade43d5a2a56e9ebe1a35d2.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>64、什么是决策树中的剪枝?</strong></p><p>剪枝是机器学习和搜索算法中的一种技术，它通过移除决策树中对实例分类作用不大的部分来减少决策树的大小。当我们删除一个决策节点的子节点时，这个过程被称为剪枝或反向分裂过程。</p><p><strong>65、什么是逻辑回归?举一个你最近使用逻辑回归的例子。</strong></p><p>逻辑回归通常被称为logit模型，是一种通过预测变量的线性组合来预测二元分类的技术。</p><p>垃圾邮件检测、医疗的病症判断、金融的贷款评估等都是二元分类。</p><p><strong>66、什么是线性回归?</strong></p><p>线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w&#39;x+e，e为误差服从均值为0的正态分布。</p><p>x被称为自变量、y被称为因变量</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!f49be2f8ad874b73b9da23333e0d5de6-7871685a.gif" alt="http://images.overfit.cn/upload/20220627/f49be2f8ad874b73b9da23333e0d5de6.gif" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/f49be2f8ad874b73b9da23333e0d5de6.gif" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/f49be2f8ad874b73b9da23333e0d5de6.gif<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>67、线性模型的缺点是什么?</strong></p><ul><li>误差线性的假设</li><li>它不能用于计数结果或二元结果</li><li>它不能解决过拟合的问题</li></ul><p><strong>68、回归和分类的ML技术有什么不同?</strong></p><p>回归和分类机器学习技术都属于监督机器学习算法。在有监督的机器学习算法中，我们必须使用带标签的数据集来训练模型，而训练时我们必须明确地提供正确的标签，算法试图学习从输入到输出的模式。如果我们的标签是离散值，那么它将是一个分类问题，如a,B等，但如果我们的标签是连续值，那么它将是一个回归问题，如1.23,1.333等。</p><p><strong>69、什么是推荐系统?</strong></p><p>推荐系统是信息过滤系统的一个子类，它旨在预测用户对产品的偏好或评级。推荐系统广泛应用于电影、新闻、研究文章、产品、社会标签、音乐等领域。</p><p>例如IMDB、Netflix和BookMyShow的电影推荐，亚马逊、eBay和Flipkart等电子商务网站的产品推荐，YouTube视频推荐和Xbox游戏推荐。</p><p><strong>70、什么是协同过滤?</strong></p><p>大多数推荐系统都是用协同过滤的算法进行推荐</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!c55e568fc02941269836f82437901b44-4034e39a.png" alt="http://images.overfit.cn/upload/20220627/c55e568fc02941269836f82437901b44.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/c55e568fc02941269836f82437901b44.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/c55e568fc02941269836f82437901b44.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>协同过滤的一个例子是，可以根据特定用户对其他电影的评分和其他人对所有电影的评分来预测该用户的评分。这一概念广泛应用于IMDB、Netflix &amp; BookMyShow的电影推荐，亚马逊、eBay &amp; Flipkart等电子商务网站的产品推荐，Xbox的YouTube视频推荐和游戏推荐。</p><p><strong>71、如何处理异常值?</strong></p><p>异常值可以通过使用单变量或任何其他图形分析方法来识别。如果离群值的数量很少，那么可以单独评估它们，但如果离群值数量很大，则可以用第99个百分位数或第1个百分位数替换这些值。</p><p>但需要注意的是，并非所有的极端值都是异常值。</p><p><strong>72、机器学习项目一般步骤是什么?</strong></p><ul><li>理解业务问题</li><li>探索数据并熟悉它。</li><li>通过检测异常值、处理缺失值、转换变量等为建模准备数据。</li><li>准备好数据后，开始运行模型，分析结果并调整方法。这是一个迭代的步骤，直到获得最好的可能结果。</li><li>使用新数据集验证模型。</li><li>开始实现模型并跟踪结果，以分析模型在一段时间内的性能。</li></ul><p><strong>73、如何处理缺失的值?</strong></p><p>在识别具有丢失值的变量后，需要识别丢失值的范围。如果有任何模式，这可能导致利益和有意义的业务见解。</p><p>如果没有确定的模式，则缺失值可以用平均值或中位数（插补）代替，否则可以简单地忽略它们。如果是一个分类变量，则可以分配默认值。如果有数据的分布，则可以为正态分布给出平均值进行填充。如果丢缺失值的很多，例如超过了80％，则可以直接删除变量而不是处理缺失值。</p><p><strong>74、您将如何定义聚类算法中的群集数？</strong></p><p>尽管不是所有的聚类算法都需要确定集群数，但此问题主要是指k均值聚类。聚类的目的是以一个类似的属性进行分组，即组中的彼此相似，但组间相互不同。</p><p>例如，下图显示了三个不同的簇。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!ec932bbaa2f749e4881aaa0e8c8da063-6fb5e017.png" alt="http://images.overfit.cn/upload/20220627/ec932bbaa2f749e4881aaa0e8c8da063.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/ec932bbaa2f749e4881aaa0e8c8da063.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/ec932bbaa2f749e4881aaa0e8c8da063.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>如果您为一系列群集绘制WSS，则将获得下面显示的绘图。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!f3dd1f984d96483db6b8397ce9f25ced-b6d3cdfa.png" alt="http://images.overfit.cn/upload/20220627/f3dd1f984d96483db6b8397ce9f25ced.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/f3dd1f984d96483db6b8397ce9f25ced.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/f3dd1f984d96483db6b8397ce9f25ced.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>该图通常称为肘部曲线。红色在图上图中圈出一个点，即 群集的数量= 6。这一点被称为弯曲点，在k-含义中被称为k。</p><p><strong>75、什么是集成学习?</strong></p><p>集成学习基本上是将一组不同的学习者(个体模型)组合在一起可以保证模型的稳定性和预测能力。</p><p><strong>76、简要描述常见的集成学习?</strong></p><p>集成学习有多种类型，下面将介绍两种更受欢迎的集成学习技术。</p><p>Bagging尝试在小样本总体上实现相似的学习者，然后取所有预测的平均值。在可以在不同的子集中使用不同的学习方法，这有助于我们减少方差误差。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!744e6382a24144e1b9d3314c383cc8cb-dfee27ce.png" alt="http://images.overfit.cn/upload/20220627/744e6382a24144e1b9d3314c383cc8cb.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/744e6382a24144e1b9d3314c383cc8cb.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/744e6382a24144e1b9d3314c383cc8cb.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>Boosting是一种迭代技术，它根据最后的分类调整一次观测的权重。如果一个观测数据被错误地分类，它会试图增加这个观测数据的权重，反之亦然。Boosting降低了偏差，并建立了强大的预测模型。但是它们可能会对训练数据过度拟合。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!5f3bc76d30d44222b52d1452e8e89ad0-0f265594.png" alt="http://images.overfit.cn/upload/20220627/5f3bc76d30d44222b52d1452e8e89ad0.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/5f3bc76d30d44222b52d1452e8e89ad0.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/5f3bc76d30d44222b52d1452e8e89ad0.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>77、什么是随机森林?它是如何工作的?</strong></p><p>随机森林是一种Bagging的集成学习方法，能够执行回归和分类任务。它也用于降维，处理缺失值，异常值等。它将一组弱模型组合起来形成一个强大的模型。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!1d71fa68723a43ef96b5d0eb3ffe2f96-205e1e34.png" alt="http://images.overfit.cn/upload/20220627/1d71fa68723a43ef96b5d0eb3ffe2f96.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/1d71fa68723a43ef96b5d0eb3ffe2f96.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/1d71fa68723a43ef96b5d0eb3ffe2f96.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>在随机森林中，我们会生成多棵树，而不是一棵树。要根据属性对新数据进行分类，每个树给出一个分类。森林选择得票最多的分类(总体森林中的树)，在回归的情况下，它取不同树输出的平均值。</p><p><strong>78、如何创造随机森林?</strong></p><p>几个弱学习者结合起来就能成为一个强学习者。所涉及的步骤如下</p><ul><li>使用自举法在训练样本数据上构建若干决策树</li><li>在每棵树上，每次考虑拆分时，都会从所有预测器中选择一个预测器的随机样本作为拆分候选</li><li>预测:按多数决定原则</li></ul><p><strong>79、你时间序列数据集使用什么交叉验证技术?</strong></p><p>时间序列不是随机分布的数据—它本质上是按时间顺序排列的。</p><p>对于时间序列数据，应该基于过去的数据建模，然后查看向前的数据。</p><p>折叠1:训练[1]，测试[2]</p><p>折叠2训练[1 2]，测试[3]</p><p>折叠3:训练[1 2 3]，测试[4]</p><p>折叠4:训练[1 2 3 4]，测试[5]</p><p><strong>80、什么是Box-Cox变换?</strong></p><p>Box-Cox变换是Box和Cox在1964年提出的一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。Box-Cox变换之后，可以一定程度上减小不可观测的误差和预测变量的相关性。Box-Cox变换的主要特点是引入一个参数，通过数据本身估计该参数进而确定应采取的数据变换形式，Box-Cox变换可以明显地改善数据的正态性、对称性和方差相等性，对许多实际数据都是行之有效的 。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!2b2d7d60e11d46b3a8e98a99870a9cc3.jpeg-095cc4b6.jpg" alt="http://images.overfit.cn/upload/20220627/2b2d7d60e11d46b3a8e98a99870a9cc3.jpeg" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/2b2d7d60e11d46b3a8e98a99870a9cc3.jpeg" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/2b2d7d60e11d46b3a8e98a99870a9cc3.jpeg<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>81、如果你的机器有4GB内存，你想在10GB数据集上训练你的模型。你将如何着手解决这个问题?到目前为止，在你的机器学习/数据科学经验中，你是否遇到过这种问题?</strong></p><p>首先，你要问你想训练哪个ML模型。</p><p>对于神经网络:批量大小是可调的，所以调整到适合4GB的批大小就可以。</p><p>对于SVM可以使用部分拟合，将一个大数据集划分为小数据集，使用SVM的部分拟合方法。</p><h2 id="深度学习" tabindex="-1"><a class="header-anchor" href="#深度学习" aria-hidden="true">#</a> <strong>深度学习</strong></h2><p><strong>82、你理解的深度学习是什么意思?</strong></p><p>深度学习只不过是机器学习的一个范例，它在最近几年显示出了令人难以置信的前景。这是因为深度学习与人脑的功能有很大的相似之处。</p><p><strong>83、机器学习和深度学习的区别是什么?</strong></p><p>机器学习是计算机科学的一个领域，它赋予计算机学习的能力，而无需明确编程。机器学习可以分为以下三类。</p><ul><li>有监督机器学习,</li><li>无监督机器学习,</li><li>强化学习</li></ul><figure><img src="/assets/http!images.overfit.cn!upload!20220627!38d69fb6b80b4c34bcaa693df10ee532-64426440.png" alt="http://images.overfit.cn/upload/20220627/38d69fb6b80b4c34bcaa693df10ee532.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/38d69fb6b80b4c34bcaa693df10ee532.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/38d69fb6b80b4c34bcaa693df10ee532.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>深度学习是机器学习的一个子领域，其算法的灵感来自于被称为人工神经网络的大脑结构和功能。</p><p><strong>84、最近深度学习流行的原因是什么?</strong></p><p>尽管深度学习已经存在多年，但这些技术的重大突破是在最近几年才出现的。这主要有两个原因:</p><ul><li>通过各种来源产生的数据量的增加</li><li>运行这些模型所需的硬件资源的增长</li></ul><p>gpu的速度是以前的好几倍，在相对较短的时间内构建更大、更深入的深度学习模型。</p><p><strong>85、解释神经网络基础知识</strong></p><p>数据科学中的神经网络旨在模仿人类大脑神经元，不同的神经元结合在一起执行任务。它从数据中学习概括或模式，并使用这些知识来预测新数据的输出，而无需任何人工干预。</p><p>最简单的神经网络是感知器。它包含一个神经元，执行两个操作，所有输入的线性计算和一个激活函数。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!5d1cf6b6d0994964b4c3ff17792e1d7c-6b48faa1.png" alt="http://images.overfit.cn/upload/20220627/5d1cf6b6d0994964b4c3ff17792e1d7c.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/5d1cf6b6d0994964b4c3ff17792e1d7c.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/5d1cf6b6d0994964b4c3ff17792e1d7c.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>更复杂的神经网络由以下3层组成-</p><p>输入层——它接收输入</p><p>隐藏层——这是输入层和输出层之间的层。最初的隐藏层通常有助于检测低级模式，而进一步的层结合以前层的输出来发现更多的模式。</p><p>输出层——输出层是输出预测的最后一层。</p><p>下图显示了一个神经网络-</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!9d2124bb349641d1b5fc2383b81fea3d.jpeg-367101f4.jpg" alt="http://images.overfit.cn/upload/20220627/9d2124bb349641d1b5fc2383b81fea3d.jpeg" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/9d2124bb349641d1b5fc2383b81fea3d.jpeg" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/9d2124bb349641d1b5fc2383b81fea3d.jpeg<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>86、什么是强化学习?</strong></p><p>强化学习是学习该做什么以及如何将情况映射到行动上的一种模型。学习者没有被告知采取哪种行动，而是必须发现哪种行动会产生最大的回报。强化学习是受人类学习的启发，建立在奖惩机制的基础上。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!2d4b11252fd94a179e397dc2f3ab51e7-26ea3776.png" alt="http://images.overfit.cn/upload/20220627/2d4b11252fd94a179e397dc2f3ab51e7.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/2d4b11252fd94a179e397dc2f3ab51e7.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/2d4b11252fd94a179e397dc2f3ab51e7.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>87、什么是人工神经网络?</strong></p><p>人工神经网络是一套特定的算法，它彻底改变了机器学习。它们的灵感来自生物神经网络。神经网络能够适应输入的改变，从而在不需要重新设计输出标准的情况下产生最佳的可能结果。</p><p><strong>88、描述人工神经网络的结构?</strong></p><p>人工神经网络的工作原理与生物神经网络相同。它由输入组成，在激活函数的帮助下，用加权和和偏差进行处理。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!bb216a28e2bb42888c20a7958ec38d50-2a6f55e1.png" alt="http://images.overfit.cn/upload/20220627/bb216a28e2bb42888c20a7958ec38d50.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/bb216a28e2bb42888c20a7958ec38d50.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/bb216a28e2bb42888c20a7958ec38d50.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>89、如何初始化网络中权重?</strong></p><p>这里有两种方法:我们可以将权重初始化为0，或者随机赋值。</p><p>将所有权值初始化为0:这将使您的模型类似于线性模型。所有的神经元和每一层都执行相同的操作，产生相同的输出，使深网变得无用。除了rnn/lstm的隐藏状态其他都不应该初始化为0，但是rnn/lstm在特殊情况下也可以不初始化为0</p><p>随机初始化所有权值:权值是通过非常接近0的初始化随机分配的。由于每个神经元的计算量不同，因此模型的精度更高。这是最常用的方法。</p><p><strong>90、什么是成本函数?</strong></p><p>也被称为“损失”或“错误”，成本函数是评估您的模型性能有多好的度量。用于计算反向传播过程中输出层的误差。我们通过神经网络向后传播这个错误并在不同的训练函数中使用它。</p><p><strong>91、什么是超参数?</strong></p><p>在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数。而其他参数的值通过训练得出。也就是说，超参数会影响我们参数的训练，所以被称之为超参数。</p><p>超参数：</p><ul><li>定义关于模型的更高层次的概念，如复杂性或学习能力。</li><li>不能直接从标准模型培训过程中的数据中学习，需要预先定义。</li><li>可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定</li></ul><p>超参数的一些示例：</p><ul><li>树的数量或树的深度</li><li>矩阵分解中潜在因素的数量</li><li>学习率（多种模式）</li><li>深层神经网络隐藏层数</li><li>k均值聚类中的簇数</li></ul><p><strong>92、学习率设置不准确(过高或过低)会发生什么?</strong></p><p>当学习率太低时，模型的训练将进展得非常缓慢，因为我们只对权重进行最小的更新。在到达最小值点之前需要进行多次更新。</p><p>如果学习率设置得太高，由于权重的剧烈更新，会导致损失函数产生不希望看到的发散行为。它可能无法收敛(模型可以给出很好的输出)，甚至发散(数据太混乱，网络无法训练)。</p><p><strong>92、深度学习中Epoch、Batch和Iteration的区别是什么?</strong></p><p>Epoch——表示整个数据集的一次迭代(所有放入训练模型的内容)。</p><p>Batch -指我们不能一次将整个数据集传递到神经网络，所以我们将数据集分成几个Batch。</p><p>Iteration——如果我们有10,000张图像作为数据，而批处理大小为200。然后一个Epoch应该运行50次Iteration(10,000除以50)。</p><p><strong>93、CNN有哪些常见的层?</strong></p><ul><li>卷积层——执行卷积操作的层，创建几个较小的图片窗口来查看数据。</li><li>激活层-它为网络带来非线性，一般使用relu</li><li>池化层—池化是一种降采样操作，可以降低特征映射的维度。</li><li>全连接层-该层识别并分类图像中的对象。</li></ul><figure><img src="/assets/http!images.overfit.cn!upload!20220627!0f6b9140dd4e4485b260ed6662356134-1f229a3e.png" alt="http://images.overfit.cn/upload/20220627/0f6b9140dd4e4485b260ed6662356134.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/0f6b9140dd4e4485b260ed6662356134.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/0f6b9140dd4e4485b260ed6662356134.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>94、池化层在CNN中是如何运作的?</strong></p><p>使用池化的方法来降低CNN的空间维度。它执行下采样操作来降低维数，并通过在输入矩阵上滑动一个滤波矩阵来创建一个汇集的特征映射。</p><p><strong>95、什么是循环神经网络(RNNs)?</strong></p><p>rnn是一种人工神经网络，旨在从时间序列、股票市场和政府机构等数据序列中识别模式。要理解循环，首先必须了解前馈神经网络的基础知识。</p><p>这两种网络都是以它们通过在网络节点上执行的一系列数学运算来传递信息的方式命名的。一种直接提供信息(从不接触相同的节点两次)，而另一种通过循环提供信息，所以被称为循环。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!b520456579584e56b9611241601e6686-f8424f86.png" alt="http://images.overfit.cn/upload/20220627/b520456579584e56b9611241601e6686.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/b520456579584e56b9611241601e6686.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/b520456579584e56b9611241601e6686.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>循环网络的输入不仅是他们看到的当前输入例子，还包括他们之前感知到的。</p><p>循环神经网络在t-1时刻做出的决定会影响到它在t时刻之后做出的决定。因此网络有两个输入源，现在和最近的过去，模型将他们结合起来对新数据做出反应，就像我们在生活中做的那样。</p><p><strong>96、LSTM网络是如何工作的?</strong></p><p>长-短期记忆(long-term Memory, LSTM)是一种特殊的循环神经网络，具有学习长期依赖的能力，其默认行为是长时间记忆信息。在LSTM网络中有三个步骤:</p><ul><li>网络决定什么该忘记，什么该记住。</li><li>它有选择地更新单元格状态值。</li><li>网络决定输出当前状态的哪一部分。</li></ul><p><strong>97、什么是多层感知机(MLP)?</strong></p><p>mlp有一个输入层、一个隐藏层和一个输出层。它与具有一个或多个隐藏层的单层感知器具有相同的结构。单层感知器只能对输出为二元(0,1)的线性可分离类进行分类，而多层感知器可以对非线性类进行分类。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!5db5cec17daf4bec91e2683fef86f92a-f09ea0fc.png" alt="http://images.overfit.cn/upload/20220627/5db5cec17daf4bec91e2683fef86f92a.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/5db5cec17daf4bec91e2683fef86f92a.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/5db5cec17daf4bec91e2683fef86f92a.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>除了输入层，其他层中的每个节点都使用非线性激活函数。这意味着输入层、传入的数据和激活函数基于所有节点和权重相加，从而产生输出。MLP使用了一种名为“反向传播”的监督学习方法。在反向传播中，神经网络利用代价函数计算误差。它从它的来源向后传播这个错误(调整权重以更准确地训练模型)。</p><p><strong>98、简单解释梯度下降</strong></p><p>要了解梯度下降，让我们先了解什么是梯度。</p><p>梯度测量的是如果输入改变一点点函数输出的变化。它只是测量所有权重的变化和误差的变化。你也可以把梯度看成是一个函数的斜率。</p><p>可以认为梯度下降是爬到山谷的底部，而不是爬上山丘。这是因为它是一种最小化给定函数（激活函数）的最小化算法。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!53aea07414454c1f849df4100d57d59b-4303c83c.png" alt="http://images.overfit.cn/upload/20220627/53aea07414454c1f849df4100d57d59b.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/53aea07414454c1f849df4100d57d59b.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/53aea07414454c1f849df4100d57d59b.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>99、什么是梯度爆炸?</strong></p><p>在训练时，如果你看到指数级增长(非常大)的误差梯度，积累并导致在训练过程中对神经网络模型权重进行非常大的更新，它们被称为梯度爆炸。在极端情况下，权值可能变得非常大，以至于溢出并导致NaN值。</p><p>这会导致模型不稳定，无法从训练数据中学习。</p><p><strong>100、什么是梯度消失?</strong></p><p>当训练时，你的梯度可以变得太小;这使得训练变得困难。当梯度太小时，这个问题被称为消失梯度。这会导致长时间的训练，糟糕的表现和低准确性。</p><p><strong>101、什么是反向传播并解释它的工作原理。</strong></p><p>反向传播算法是一种用于多层神经网络的训练算法。在这种方法中，将误差从网络的一端移动到网络内的所有权值，从而可以高效地计算梯度。</p><p>它有以下步骤:</p><ul><li>训练数据前向传播</li><li>利用输出和目标计算导数</li><li>反向传播用于计算wrt输出激活的误差导数</li><li>使用先前计算的导数来计算输出</li><li>更新权重</li></ul><p><strong>102、反向传播有哪些变体?</strong></p><p>随机梯度下降:我们只使用一个单一的训练例子来计算梯度和更新参数。</p><p>批量梯度下降:我们计算整个数据集的梯度，并在每次迭代时执行更新。</p><p>小批量梯度下降:这是最流行的优化算法之一。它是随机梯度下降法的一种变体，这里使用的不是单一的训练例子，而是小批量的样本。</p><p><strong>103、有哪些不同的深度学习框架?</strong></p><ul><li>Pytorch</li><li>TensorFlow</li><li>Keras</li><li>Caffe</li></ul><p><strong>104、激活函数的作用是什么?</strong></p><p>利用激活函数将非线性引入神经网络，帮助其学习更复杂的函数。没有它神经网络将只能学习线性关系，即输入数据的线性组合。</p><p><strong>105、什么是自动编码器?</strong></p><p>自动编码器是一种简单的学习网络，旨在以最小的误差将输入转换为输出。这意味着我们希望输出尽可能接近输入。我们在输入和输出之间添加了几层，这些层的大小比输入层小。自编码器接收未标记的输入，然后对其进行编码以重构输入。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!aa5614a0e6b54f6aa40c4ee526e6ca56-9d99f83a.png" alt="http://images.overfit.cn/upload/20220627/aa5614a0e6b54f6aa40c4ee526e6ca56.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/aa5614a0e6b54f6aa40c4ee526e6ca56.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/aa5614a0e6b54f6aa40c4ee526e6ca56.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>106、什么是玻尔兹曼机?</strong></p><p>玻尔兹曼机器是一个简单的学习算法，可以让它们发现在训练数据中代表复杂规律的有趣特征。玻尔兹曼机主要用于优化给定问题的权重和数量。在具有多层特征检测器的网络中，学习算法的速度很慢。“受限玻尔兹曼机器”算法只有一层特征检测器，这使得它比其他算法更快。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!a3233242ebae4cbbb719bb8a1ee45206-fea1ffd3.png" alt="http://images.overfit.cn/upload/20220627/a3233242ebae4cbbb719bb8a1ee45206.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/a3233242ebae4cbbb719bb8a1ee45206.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/a3233242ebae4cbbb719bb8a1ee45206.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p><strong>107、什么是Dropout和BN?</strong></p><p>Dropout是一种随机删除网络中隐藏和可见单元的技术，以防止数据过拟合(通常会删除20%的节点)。它使网络收敛所需的迭代次数翻倍。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!5916f2debb374c0c8e67f2bc4c60c2af-b36771ac.png" alt="http://images.overfit.cn/upload/20220627/5916f2debb374c0c8e67f2bc4c60c2af.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/5916f2debb374c0c8e67f2bc4c60c2af.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/5916f2debb374c0c8e67f2bc4c60c2af.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>BN是通过对每一层的输入进行归一化，使其平均输出为0，标准差为1，从而提高神经网络的性能和稳定性的技术。</p><p><strong>108、什么是计算图?</strong></p><p>张量流中的一切都是基于创建一个计算图。它有一个节点网络，每个节点都在其中工作，节点代表数学运算，边代表张量。在计算图中，节点是输入值或用于组合值的函数。当数据流过图形时，边会收到它们的权重。输入节点的出站边用该输入值加权；来自函数节点的出站节点通过使用指定函数组合入站边的权重来加权。</p><figure><img src="/assets/http!images.overfit.cn!upload!20220627!afcff2caa9c84968bd0a0491977729f7-772c10d8.png" alt="http://images.overfit.cn/upload/20220627/afcff2caa9c84968bd0a0491977729f7.png" tabindex="0" loading="lazy"><figcaption><a href="http://images.overfit.cn/upload/20220627/afcff2caa9c84968bd0a0491977729f7.png" target="_blank" rel="noopener noreferrer">http://images.overfit.cn/upload/20220627/afcff2caa9c84968bd0a0491977729f7.png<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></figcaption></figure><p>所有深度学习框架都依赖于创建计算图来计算梯度下降优化所需的梯度值。通常，你必须构建前向传播图，而框架将为你处理反向微分。</p><p>静态图的优点之一是它允许对图进行强大的离线优化/调度。这意味着这些通常会比动态图更快(在每个用例中差异可能并不显著，这取决于我们的图)。缺点是处理结构化或者可变大小的数据比较复杂。</p><p>动态图是调试友好的。查找代码中的问题要容易得多，因为它允许逐行执行代码，并且你可以访问所有变量。如果你想将深度学习应用于行业的任何实际目的，这绝对是一个非常重要的特性。</p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/tobeprozy/LearnData/edit/main/docs/08AI_Machine_Learning_Deep_Learning/机器学习/机器学习100个问题.md" rel="noopener noreferrer" target="_blank" aria-label="编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><!----></div></footer><nav class="page-nav"><a href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86.html" class="nav-link prev" aria-label="一、概述"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->一、概述</div></a><a href="/08AI_Machine_Learning_Deep_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8D%E7%82%B9%E7%9F%A5%E8%AF%86.html" class="nav-link next" aria-label="机器学习重点知识"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">机器学习重点知识<!----></div></a></nav><div class="waline-wrapper" id="comment" darkmode="false" style="display:block;"><div data-waline provider="Waline"><div class="wl-reaction"><div class="wl-reaction-title">已到达文章底部，欢迎留言、表情互动~</div><ul class="wl-reaction-list"><!--[--><li class="wl-reaction-item"><div class="wl-reaction-img"><img src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-y/twemoji/13.1.0/72x72/1f44d.png" alt="赞一个"><div class="wl-reaction-votes">0</div></div><div class="wl-reaction-text">赞一个</div></li><li class="wl-reaction-item"><div class="wl-reaction-img"><img src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-y/twemoji/13.1.0/72x72/1f44f.png" alt="支持下"><div class="wl-reaction-votes">0</div></div><div class="wl-reaction-text">支持下</div></li><li class="wl-reaction-item"><div class="wl-reaction-img"><img src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-y/twemoji/13.1.0/72x72/1f60e.png" alt="有点酷"><div class="wl-reaction-votes">0</div></div><div class="wl-reaction-text">有点酷</div></li><li class="wl-reaction-item"><div class="wl-reaction-img"><img src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-y/twemoji/13.1.0/72x72/1f602.png" alt="啥玩意"><div class="wl-reaction-votes">0</div></div><div class="wl-reaction-text">啥玩意</div></li><li class="wl-reaction-item"><div class="wl-reaction-img"><img src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-y/twemoji/13.1.0/72x72/1f635-200d-1f4ab.png" alt="看不懂"><div class="wl-reaction-votes">0</div></div><div class="wl-reaction-text">看不懂</div></li><!--]--></ul></div><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">昵称</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">邮箱</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">网址</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="请留言。(填写邮箱可在被回复时收到邮件提醒)"></textarea><div class="wl-preview" style="display:none;"><hr><h4>预览:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="表情" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="表情包"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="上传图片"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="预览"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-captcha-container"></div><div class="wl-text-number">0 <!--v-if-->  字</div><button type="button" class="wl-btn">登录</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->提交<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="搜索表情包"><!--v-if--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> 评论</div><ul class="wl-sort"><!--[--><li class="active">按正序</li><li class="">按倒序</li><li class="">按热度</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><!--[--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><!--]--><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v2.15.2</div></div></div><!----><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/assets/app-1ea92eb0.js" defer></script>
    <script src="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-y/instant.page/5.1.0/instantpage.min.js" type="application/javascript"></script>
  </body>
</html>
