<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="https://tobeprozy.github.io/rss.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://tobeprozy.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>南叔先生-开源笔记</title>
    <link>https://tobeprozy.github.io/</link>
    <description>关于我认知里的工科世界：自动驾驶和机器人是工科世界里的百科全书，记录并输出一切能让自己提升的知识。</description>
    <language>zh-CN</language>
    <pubDate>Wed, 26 Apr 2023 15:18:36 GMT</pubDate>
    <lastBuildDate>Wed, 26 Apr 2023 15:18:36 GMT</lastBuildDate>
    <generator>vuepress-plugin-feed2</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <copyright>Copyright by 南叔先生</copyright>
    <image>
      <title>南叔先生-开源笔记</title>
      <url>https://tobeprozy.github.io/logo.svg</url>
      <link>https://tobeprozy.github.io/</link>
    </image>
    <category>头脑驿站</category>
    <category>传感器</category>
    <category>图像处理</category>
    <category>计算机视觉</category>
    <category>SLAM</category>
    <category>人工智能</category>
    <category>自动驾驶</category>
    <item>
      <title>每日一问！目录</title>
      <link>https://tobeprozy.github.io/00Daily_Question/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%97%AE.html</link>
      <guid>https://tobeprozy.github.io/00Daily_Question/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%97%AE.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">每日一问！目录</source>
      <description>提示 作为一个电脑打交道的打工人，经常会需要用到各种工具，而且这次用了，下次又忘了，所以就想着把这些问题记录下来，方便自己以后查阅，也方便大家一起交流。 📅每日一问系列 01如何在指定目录下的文件名前面加上“0”+序号 02如何实现PDF合并?</description>
      <category>头脑驿站</category>
      <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container tip">
<p class="hint-container-title">提示</p>
<p>作为一个电脑打交道的打工人，经常会需要用到各种工具，而且这次用了，下次又忘了，所以就想着把这些问题记录下来，方便自己以后查阅，也方便大家一起交流。</p>
</div>
<h1> 📅每日一问系列</h1>
<h3> <a href="/00Daily_Question/QandA/01.html" target="blank">01如何在指定目录下的文件名前面加上“0”+序号</a></h3>
<h3> <a href="/00Daily_Question/QandA/02.html" target="blank">02如何实现PDF合并?</a></h3>
<h3> <a href="/00Daily_Question/QandA/03.html" target="blank">03如何拆分指定页码pdf</a></h3>
<h3> <a href="/00Daily_Question/QandA/04.html" target="blank">04如何利用华为云和Frp进行内网穿透?</a></h3>
<h3> <a href="/00Daily_Question/QandA/05.html" target="blank">05将网络图的链接保存为本地链接</a></h3>
<h3> <a href="/00Daily_Question/QandA/06.html" target="blank">06如何将docx转化为markdown</a></h3>
]]></content:encoded>
    </item>
    <item>
      <title>机器人、自动驾驶传感器概述</title>
      <link>https://tobeprozy.github.io/01Sensor/%E4%BC%A0%E6%84%9F%E5%99%A8%E7%AF%87.html</link>
      <guid>https://tobeprozy.github.io/01Sensor/%E4%BC%A0%E6%84%9F%E5%99%A8%E7%AF%87.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">机器人、自动驾驶传感器概述</source>
      <description>提示 传感器是机器人和无人车中最重要的感知不部件，这篇文章讲解相机。 摄像机 硬件知识 1、相机主要参数： 分辨率、像素尺寸、帧率、像素深度、数字接口 2、相机种类： （1）面阵相机和线阵相机 （2）CMOS和CCD （3）黑白和彩色相机 3、面阵相机的选型</description>
      <category>传感器</category>
      <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container tip">
<p class="hint-container-title">提示</p>
<p>传感器是机器人和无人车中最重要的感知不部件，这篇文章讲解相机。</p>
</div>
<h1> 摄像机</h1>
<h3> 硬件知识</h3>
<p><strong>1、相机主要参数：</strong></p>
<p>分辨率、像素尺寸、帧率、像素深度、数字接口</p>
<p><strong>2、相机种类：</strong></p>
<p>（1）面阵相机和线阵相机</p>
<p>（2）CMOS和CCD</p>
<p>（3）黑白和彩色相机</p>
<p><strong>3、面阵相机的选型</strong></p>
<p>帧率、分辨率、接口、靶面尺寸、黑白/彩色、感光类型、像元尺寸</p>
<p><strong>4、线阵相机的选型</strong></p>
<p>幅宽、精度要求、运动速度</p>
<p>行频、分辨率、像素尺寸、数据接口、黑白/彩色、感光类型、镜头接口</p>
<p><strong>5、镜头选型</strong></p>
<p>接口、最靶面尺寸、物距焦距、、光圈、分辨率和成像质量、镜头倍率和视场范围</p>
<ul>
<li><strong>选型步骤</strong></li>
</ul>
<p>（1）确定相机连接的镜头接口类型，如C口或者F口，这个接口决定了镜头的接口。</p>
<p>（2）确定镜头的最大靶面尺寸和相机匹配。</p>
<p>（3）确定焦距。首先测量工作距离和目标物体大小，得到图像的宽或者高度；然后确定相机的安装位置，从相机的拍摄角度推测视角，最后根据几何关系计算相机焦距。</p>
<p>（4）根据现场拍摄要求，考虑光圈、价格等其它因素。</p>
<p><strong>6、光源选择</strong></p>
<p><strong>类型：</strong> LED光源、红外光源、激光光源、卤素灯</p>
<p>环形光源、背光源、电光源</p>
<p>突出物体结构特性，可以使用正面或者侧面光源</p>
<p>突出物体轮廓，使用背面光源</p>
<h3> 单目视觉空间定位原理</h3>
<p>三维空间测量和定位可以采用单目和双目视觉，与单目视觉相比，双目视觉的定位精度更高，但是定位算法更加复杂，同时也会带来更大的尺寸和重量，不仅会导致测量速度更慢，也不符合直升机上对测量装置小尺寸、轻质量的基本要求。单目视觉可以基于单帧图像对特征点进行空间定位，这种方式需要与标志物配合使用，采用PnP方法建立相机的像素坐标系与标志物的世界坐标系之间的位置关系。特别地，为了保证弱光环境下还能够拍摄清楚，可以通过补光灯或者设计荧光标记增大标记与环境的对比度。</p>
<p>图11是世界坐标系中一点的成像过程，依次经历相机坐标系、图像坐标系，最终形成像素坐标系下的点。</p>
<figure><figcaption></figcaption></figure>
<p>图11 相机成像过程</p>
<p>世界坐标系的一点投射到像素坐标系的投影变换可以概括为以下表达式：</p>
<p>(11)</p>
<p>其中和表示单个像素的尺寸大小，表示图像坐标系在像素坐标系下的中心坐标，表示相机的焦距，表示成像点相对于相机原点在光轴方向的距离，和表示世界坐标系与相机坐标系之间的位置关系。令，表示内参矩阵。令，表示外参矩阵。实际上，由透镜形状的非理想会导致成像发生径向畸变，由机械安装的非理想会导致成像产生切向畸变。那么图像坐标系坐标的理想点与实际的畸变点存在如下关系：</p>
<p>(12)</p>
<p>其中，径向畸变主要由参数，切向畸变主要由参数，参数满足。</p>
<p>由畸变后的点通过内参矩阵可得到像素平面上的实际坐标：</p>
<p>(13)</p>
<p>采用张正友标定法可以实现内参和畸变系数的标定，使用相机拍摄10~20张不同姿态的清晰的标定图像。实验中，采用格数，边长为30 mm的高精度标定板，完成标定，图12是相机标定时标定板呈现不同姿态的情况。</p>
<figure><figcaption></figcaption></figure>
<p>图12 相机标定实验</p>
<p>将采集好的图像，通过OpenCV工具识别角点，计算坐标变换关系，获得相机的畸变参数和内参系数。采用张正友法标定的基本流程如图13。</p>
<p>图13 张正友相机标定基本流程</p>
<h3> 基于ArUco标记视觉定位原理</h3>
<p>ArUco标记是一种特殊的编码，每个标记都有一个用于自身检测的黑色方形边框，通过明显的边框角点和内部的二进制编码，边框角点可以用来检测二维平面和三维世界之间的射影关系，从而实现空间点的定位。内部编码用于需要多个标记的场合，可以快速高效判断整副图像中各标记所属的ID，即准确、快速地找到标记点所在位置。ArUco标记采用二值图像编码信息，其中黑色格子用0表示，白色格子用1表示，如图14所示。</p>
<p> </p>
<p>图14 ArUco标记视图和编码</p>
<p>ArUco标记的检测和识别与棋盘格相机标定是类似的，但ArUco标记包含更加丰富的信息，可以实现更高从检测精度，并且被部分遮挡也可以完成定位。ArUco标记的检测和识别是一个图像处理过程。</p>
<p>主要分为以下几个步骤完成：</p>
<p>（1）阈值处理。实际采集的图像一般为灰度图，采用局部自适应阈值处理，对不同光照条件都有很好的鲁棒性。</p>
<p>（2）轮廓提取和过滤。使用Suzuki和Abe算法完成轮廓提取，再通过Douglas-Peucker算法进行多边形近似，由于标记肯定是在矩形框内，所以需要丢弃非四边形轮廓。最后，保留最外轮廓。</p>
<p>（3）标记提取。计算单应性矩阵，将图像中标记转变成正视图。再使用OTSU算法进行阈值处理，利用二值化将图像划分成规则网格，网格中黑色像素为0，白色像素为1，如图15所示。</p>
<p>（4）标记识别。除去外边框像素，将像素值从原点按行编码，图示二进制编码为110111001111010011110100，将其与ArUco字典进行对比和编码纠错，获得标记编号。</p>
<figure><figcaption></figcaption></figure>
<p>图15ArUco标记的检测和识别</p>
<p>经过以上步骤可以获得标记的四个角点像素坐标以及标记的编号。采用PnP算法可以求解，ArUco标记成像示意图如图16所示。</p>
<figure><figcaption></figcaption></figure>
<p>图16 PnP算法求解示意图</p>
<p>假设ArUco的边长为，以点作为原点，建立世界坐标系，那么各点坐标分别为。经过前面的标记识别和检测可以获得四个角点的像素坐标，假设，以B点为例，根据成像公式(14)可得点坐标：</p>
<p>(14)</p>
<p>令，其中外参矩阵也被称为齐次变换矩阵，那么式(14)可以改写成：</p>
<p>(15)</p>
<p>消除，可以得到两个等式：</p>
<p>(16)</p>
<p>由于旋转矩阵是一个单位的正交矩阵，其本身具有六个约束关系：</p>
<p>(17)</p>
<p>因此，只要能够获取三个角点就能够求解出具有十二个未知变量的外参。本文采用的ArUco标记，能够提供四个精确角点，通过最小二乘求解具有12变量的14个方程的超定方程问题，即可求出外参数，即计算相机外参矩阵便可获取标记空间位置。那么，世界坐标系的原点点在相机坐标系下的坐标为：</p>
<p>(18)</p>
<h1> 激光雷达</h1>
<h1> 毫米波雷达</h1>
<h1> 超声波雷达</h1>
<h1> 编码器</h1>
<h1> IMU/陀螺仪</h1>
<h1> GPS/RTK</h1>
<h1> 传感器融合算法</h1>
<p>传感器融合是一种多传感器数据处理技术，它结合来自多个传感器的信息，以提供更准确、可靠和稳定的数据。传感器融合算法可以用于各种应用，如自动驾驶、机器人、物联网设备等。</p>
<p>常见的传感器融合技术包括以下几类：</p>
<p>**1. 数据融合：**基于统计理论和信号处理方法，用于处理多个传感器的噪声和不确定性，从而提高数据精度。通过对同类传感器数据进行加权平均、中值滤波等处理，利用卡尔曼滤波和粒子滤波等算法，将不同传感器的数据融合以增强精度和可靠性。应用场景包括无人驾驶定位和机器人导航。</p>
<p>**2. 特征融合：**其原理是通过特征变换和降维，提取出更具代表性的特征，有利于模型训练和分类性能的提高。对传感器数据提取特征后进行融合。可应用于不同类型的传感器，如摄像头、激光雷达等。特征融合可以提高数据处理速度和准确性。在特征层对来自不同传感器的特征进行融合，以提高特征的表达能力。常用的方法有主成分分析（PCA）、线性判别分析（LDA）等。应用场景包括目标识别、行为分析等。</p>
<p>**3. 决策融合：**其原理是通过整合多个传感器的决策结果，提高系统的鲁棒性和可靠性，从而提高系统的决策性能。常用的方法有投票法（Voting）、贝叶斯融合（Bayesian Fusion）等。应用场景包括多传感器目标跟踪、异常检测等。</p>
<p>**4. 深度融合：**其原理是利用深度学习方法自动学习数据的表征，能处理高维、非线性、多模态数据，具有较强的泛化能力。利用深度学习方法（如卷积神经网络，CNN）对多模态数据进行端到端的融合，提高系统性能。应用场景包括图像识别、语音识别等。</p>
<p>以下是一些常用的传感器融合算法：</p>
<p><strong>（1）加权平均法（Weighted Average）</strong></p>
<p>加权平均法是一种简单的传感器融合方法，它根据每个传感器的可靠性为其分配权重。融合后的数据是每个传感器数据乘以相应权重之和。这种方法适用于传感器输出具有相似性质的场景，如温度、湿度等。</p>
<p><strong>（2）卡尔曼滤波器（Kalman Filter）</strong></p>
<p>卡尔曼滤波器是一种线性最优滤波器，用于估计动态系统的状态变量。它通过线性系统动态模型和观测模型，结合系统噪声和观测噪声，来融合多个传感器的数据。卡尔曼滤波器广泛应用于导航、定位和追踪等领域。</p>
<p><strong>（3）扩展卡尔曼滤波器（Extended Kalman Filter, EKF）</strong></p>
<p>当系统模型和观测模型非线性时，可以使用扩展卡尔曼滤波器。EKF通过将非线性模型在当前状态附近进行线性化，然后使用卡尔曼滤波器的方法进行融合。EKF适用于具有非线性特征的系统，例如机器人定位、姿态估计等。</p>
<p><strong>（4）无迹卡尔曼滤波器（Unscented Kalman Filter, UKF）</strong></p>
<p>无迹卡尔曼滤波器是另一种处理非线性系统的方法。与EKF通过线性化近似不同，UKF通过选择一组代表性样本（称为Sigma点）来近似非线性函数的均值和协方差。UKF通常比EKF更精确，但计算复杂度较高。</p>
<p><strong>（5）粒子滤波器（Particle Filter）</strong></p>
<p>粒子滤波器是一种基于蒙特卡洛方法的非线性、非高斯滤波器。它使用一组随机抽样的粒子来表示系统状态的概率分布。粒子滤波器可以处理具有复杂非线性和非高斯特征的系统，但计算成本较高。</p>
<p><strong>（6）多传感器信息融合</strong></p>
<p>当涉及到多个传感器类型时，可以使用多层次的传感器融合架构。常见的方法包括：<strong>中央融合（Centralized Fusion）</strong>、<strong>分布式融合（Distributed Fusion）<strong>和</strong>协同融合（Cooperative Fusion）</strong>。这些方法可以根据传感器的特性、通信限制和计算资源来选择合适的融合策略。</p>
<p>多传感器信息融合是指利用来自不同来源的传感器数据以提高目标跟踪等任务性能的技术。通常，在将不同源头信息汇聚到一起时，需要考虑到诸如时间戳、噪声估计、坐标系转换和协方差矩阵等问题。在执行多传感器信息融合时，可以参考以下步骤：</p>
<p>1. 执行数据校准和缩放操作。</p>
<p>2. 定义参数化模型以捕捉观察误差及其相关统计特征。</p>
<p>3. 建立多变量联合概率密度函数表示两个观察量之间的关系。</p>
<p>4. 将带有相应权重因素加权后建立混合密度函数</p>
<p>5,使用贝叶斯框架更新当前时刻系统状态的估计。</p>
<p>这些传感器融合算法的选择取决于应用场景、传感器类型和数据特性。通常，线性系统可以使用卡尔曼滤波器，而非线性系统可以选择EKF、UKF或粒子滤波器。多传感器信息融合方法可以根据系统需求灵活选择。</p>
<p>第二种分类方法：</p>
<p>**1. 基于卡尔曼滤波的传感器融合：**卡尔曼滤波是一种递归的最优线性估计算法，用于整合不同传感器的数据。特点是计算简单、实时性好，但对系统模型和噪声假设有较高要求。</p>
<p>2. **基于粒子滤波的传感器融合：**粒子滤波是一种蒙特卡洛方法，通过采样和重要性重采样来估计系统状态。特点是适用于非线性、非高斯系统，但计算复杂度较高。</p>
<p>3. **基于卷积神经网络（CNN）的传感器融合：**CNN是一种深度学习方法，可以自动学习特征并整合多模态数据。特点是能处理复杂场景，但需要大量数据和计算资源。</p>
<p>4. **基于多传感器数据融合的定位与地图构建：**如SLAM（同时定位与地图构建）技术，通过整合激光雷达、视觉、惯性测量单元（IMU）等多种传感器数据，实现自动驾驶和机器人的定位与地图构建。特点是实时性好、精度高，但受环境变化影响较大。</p>
<p>5. **基于DSM（证据理论）的传感器融合：**DSM是一种处理不确定性信息的方法，通过计算不同传感器数据的置信度，实现数据融合。特点是适用于不确定性较高的场景，但计算复杂度较高。</p>
]]></content:encoded>
    </item>
    <item>
      <title>图像处理几块重点知识</title>
      <link>https://tobeprozy.github.io/02Image_Processing/%E5%87%A0%E5%9D%97%E9%87%8D%E7%82%B9%E7%9F%A5%E8%AF%86.html</link>
      <guid>https://tobeprozy.github.io/02Image_Processing/%E5%87%A0%E5%9D%97%E9%87%8D%E7%82%B9%E7%9F%A5%E8%AF%86.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">图像处理几块重点知识</source>
      <description>注意 图像处理的一般步骤：提取原始图像→灰度处理→滤波→边缘检测→图像分割→特征提取。 一、图像滤波 1、平均滤波 2、中值滤波：对于消除椒盐噪声很有效 3、高斯滤波 4、BM3D滤波 5、双边滤波 基本原理 1、中值滤波——非线性滤波 用像素点领域灰度值的中值来代替该像素点的灰度值，该方法在去除脉冲噪声，椒盐噪声的同时又能保留图像的边缘细节，中值滤波去除椒盐噪声和斑块噪声时，效果非常明显。</description>
      <category>图像处理</category>
      <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container warning">
<p class="hint-container-title">注意</p>
<p>图像处理的一般步骤：提取原始图像→灰度处理→滤波→边缘检测→图像分割→特征提取。</p>
</div>
<h2> 一、图像滤波</h2>
<p>1、平均滤波
2、中值滤波：对于消除椒盐噪声很有效
3、高斯滤波
4、BM3D滤波
5、双边滤波</p>
<h3> 基本原理</h3>
<p>1、<strong>中值滤波——非线性滤波</strong></p>
<p>用像素点领域灰度值的中值来代替该像素点的灰度值，<strong>该方法在去除脉冲噪声，椒盐噪声的同时又能保留图像的边缘细节</strong>，<strong>中值滤波去除椒盐噪声和斑块噪声时，效果非常明显。</strong></p>
<p>选用一个模板nxn，每次在图像中取出模板大小的矩阵，将所有元素进行排序，取中值代替原像素，而未被赋值的元素取原值。中值滤波通常采用一个含奇数个点的滑动窗口，用窗口的中的灰度值的中值来代替中心点的灰度值，其实就是对这个窗口中的灰度值进行排序，然后将其中值赋值给中心点即可。常用的中值滤波窗口形状有线状、方形、圆形以及十字形等</p>
<p><strong>算法描述：</strong></p>
<p>[1] 获得源图像的<strong>首地址</strong>及图像的<strong>宽和高</strong> [2] 开辟一块内存缓冲区，用以暂存结果图像，并初始化为0 [3] <strong>逐个扫描图像中的像素点</strong>，将其邻域各元素的像素值<strong>从小到大进行排序</strong>，将求得到的<strong>中间值</strong>赋值给目标图像中与当前点对应的像素点 [4] 循环步骤[3]，直到处理完源图像的全部像素点 [5] 将结果从内存缓冲区复制到源图像的数据区
I=medfilt2(I0,[n,n]);%matlab中自带的中值滤波函数。</p>
<p>2、<strong>均值滤波</strong></p>
<p>与中值滤波类似，用其像素点周围像素的平均值代替元像素值，在滤除噪声的同时也会滤掉图像的边缘信息。</p>
<p>3、<strong>高斯模糊（高斯滤波）</strong></p>
<p>所谓”模糊”，可以理解成每一个像素都取周边像素的平均值。“中间点”取”周围点”的平均值，在数值上，这是一种”平滑化”。在图形上，就相当于产生”模糊”效果，”中间点”失去细节。计算平均值时，取值范围越大，”模糊效果”越强烈。 与均值滤波不同的是，不是简单平均，而是依据距离来加权平均，距离越近的点权重越大，距离越远的点权重越小。采用二维的正态分布（高斯函数）的权重分配模式</p>
<p>4、<strong>双边滤波——非线性滤波</strong></p>
<p>双边滤波与高斯滤波器相比，对于<strong>图像的边缘信息能过更好</strong>的保存。其原理为一个与空间距离相关的高斯函数与一个灰度距离相关的高斯函数相乘。</p>
<p>对于高斯滤波，仅用空间距离的权值系数核与图像卷积后确定中心点的灰度值。即认为离中心点越近，其权值系数越大。
</p>
<p>双边滤波中加入了对<strong>灰度信息的权重</strong>，即在领域内，灰度值越接近<strong>中心点灰度值的点的权值更大</strong>，灰度值相差大的点权重越小。<strong>其权重大小则由值域高斯函数确定</strong>。 两者权重系数相乘，得到最终的卷积模板，由于双边滤波需要每个中心点领域的灰度信息来确定其系数，所以速度比一般的滤波慢得多，而且计算量增长速度为核的大小的平方。 σ越大，边缘越模糊；σ越小，边缘越清晰。</p>
<h2> 二、二值化</h2>
<p>二值化的算法分为<strong>固定阈值</strong>和<strong>自适应阈值,<strong>比较常用的二值化方法则有：<strong>双峰法</strong>、<strong>P参数法</strong>、<strong>迭代法</strong>和</strong>OTSU法</strong>等。</p>
<h2> 三、特征提取</h2>
<h3> 1、特征提取概述</h3>
<p>图像特征主要有图像的颜色特征、纹理特征、形状特征和空间关系特征。</p>
<p>颜色直方图特征匹配方法：直方图相交法、距离法、中心距法、参考颜色表法、累加颜色直方图法。</p>
<p>纹理特征的提取与匹配主要有：灰度共生矩阵、Tamura 纹理特征、自回归纹理模型、小波变换等。灰度共生矩阵特征提取与匹配主要依赖于能量、惯量、熵和相关性四个参数。Tamura 纹理特征基于人类对纹理的视觉感知心理学研究，提出6种属性，即：粗糙度、对比度、方向度、线像度、规整度和粗略度。自回归纹理模型（ SAR）是马尔可夫随机场（MRF）模型的一种应用实例。</p>
<p>形状特征有两类表示方法，一类是轮廓特征，另一类是区域特征。图像的轮廓特征主要针对物体的外边界，而图像的区域特征则关系到整个形状区域。</p>
<p>目前常用的图像检测和识别的图像特征有：Hu不变矩、Haar特征算子、Surf特征，他们都具有平移旋转缩放不变形，具有很好的鲁棒性。
</p>
<h3> 2、边缘检测</h3>
<p>图像<a href="https://so.csdn.net/so/search?q=%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">边缘检测</a>主要用于增强图像中的<strong>轮廓边缘、细节以及灰度跳变部分</strong>，形成完整的物体边界，达到将物体从图像中分离出来或将表示同一物体表面的区域检测出来的目的。目前为止最通用的方法是检测亮度值的不连续性，用一阶和二阶导数检测的。</p>
<ul>
<li>
<p>1、<strong><strong>微分法</strong></strong>
利用微分运算求信号的变化率，加强高频分量的作用，从而使轮廓清晰。 遵循如下两个基本准则之一： 找到亮度的一阶导数在幅度上比指定的阈值大的地方； 找到亮度的二阶导数有零交叉的地方。</p>
</li>
<li>
<p>2、<strong><strong>差分边缘检测方法</strong></strong>
利用像素灰度的<strong>一阶导数算子</strong>在<strong>灰度迅速变化处</strong>得到<strong>高值</strong>来进行奇异点的检测。<strong>它在某一点的值就代表该点的边缘强度</strong>，通过对这些值设置阈值来进一步得到边缘图像。差分边缘检测方法是最原始、最基本的方法。但要求差分方向与边缘方向垂直，这就需要对图像的不同方向（一般为垂直方向、水平方向和对角线方向）都进行差分运算，增加了实际运算的繁琐性，目前很少采用。</p>
</li>
<li>
<p>3、<strong><strong>Roberts 边缘检测算子</strong></strong><br>
Roberts边缘检测算子<strong>根据任意一对互相垂直方向上的差分</strong>可用来计算梯度的原理，采用<strong>对角线方向相邻两像素之差</strong>。 Roberts检测器较为简单，但具有一些功能上的限制，例如，它是非对称的，而且不能检测诸如45°倍数的边缘。然而，它还是经常用于硬件实现中，因为它既简单又快速。</p>
</li>
<li>
<p>4、<strong><strong>Sobel 边缘检测算子</strong></strong><br>
对数字图像的每个像素，考察它上下左右邻点灰度的加权差，与之接近的邻点的权大。 Sobel算子很容易在空间上实现，<strong>边缘检测效果较好，且受噪声的影响也较小</strong>。邻域增大抗噪性会更好，但计算量也会增大，得出的边缘也会相应变粗。 Sobel算子会检测出许多伪边缘，<strong>边缘定位精度不够高</strong>，在精度要求不高时是一种较常用的边缘检测方法。</p>
</li>
<li>
<p>5、<strong><strong>Prewitt 边缘检测算子</strong></strong></p>
</li>
</ul>
<figure><figcaption></figcaption></figure>

<ul>
<li>
<p>6、<strong><strong>拉普拉斯边缘检测算子</strong></strong></p>
<p>拉普拉斯边缘检测算子是<strong>一种二阶微分算子</strong>，与其它边缘检测方法的不同之处在于，该方法是一种<strong>各向同性</strong>的检测方法，即其边缘的增强程度与边缘的方向无关，从而可以满足不同走向的边缘锐化的要求。 拉普拉斯算子自身<strong>很少被直接用作边缘检测</strong>，因为二阶导数对噪声具有<strong>无法接受的敏感性</strong>，它的幅度会产生<strong>双边缘</strong>，而且<strong>它不能检测边缘的方向</strong>。然而，当与其它边缘检测技术组合使用时，拉普拉斯算子是一种有效的补充方法。例如，虽然它的双边缘使得<strong>它不适合直接用于边缘检测</strong>，但该性质可用于边缘定位。</p>
</li>
<li>
<p>7、<strong><strong>Laplacian of a Gaussian(LoG)检测器</strong></strong></p>
<p>高斯函数</p>
</li>
</ul>
<figure><figcaption></figcaption></figure>

<figure><figcaption></figcaption></figure>

<ul>
<li>
<p>8、<strong><strong>Canny算子</strong></strong></p>
<p>Canny算子是一个综合了滤波，增强，检测等多阶段的边缘检测算子, 其目标是找到一个最优的边缘轮廓。该算法对最优的边缘的定义是：</p>
<ol>
<li>好的检测——算法能够尽可能多地标示出图像中的实际边缘</li>
<li>好的定位——算法标示出的边缘要与实际的边缘尽可能地相同</li>
<li>最小响应——图像中的边缘只能被标示一次，并且可能存在的图像噪声点不应该被标示为边缘点</li>
</ol>
<p>步骤：高斯(Gaussian)滤波→计算梯度图像和角度图像→梯度图像的非极大值抑制处理→双阈值算法检测并连接图像边缘</p>
</li>
</ul>
<p>大致可分为两类：基于搜索和基于零交叉。基于搜索的边缘检测方法首先计算边缘强度， 通常用一阶导数表示， 例如<a href="https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">梯度</a> 模，然后，用计算估计边缘的局部方向， 通常采用梯度的方向，并利用此方向找到局部梯度模的最大值。基于零交叉的方法找到由图像得到的<a href="https://baike.baidu.com/item/%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">二阶导数</a>的零交叉点来定位边缘。 通常用<a href="https://baike.baidu.com/item/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%AE%97%E5%AD%90?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">拉普拉斯算子</a>或非线性微分方程的零交叉点。</p>
<p>一阶:：<strong>Roberts</strong> 算子，<strong>Prewitt算子</strong>，<strong>Sobel算子</strong>， Kirsch算子； 二阶： 在梯度方向的二阶导数过零点，<strong>Canny算子</strong>，<strong>Laplacian算子</strong>。</p>
<p><strong>Marr-Hildreth边缘检测器（LoG滤波器）算法步骤：</strong> 1.使用高斯滤波器平滑图像，并计算其拉普拉斯算子 2.寻找步骤2所得图像的零交叉</p>
<p><strong>坎尼边缘检测器（Canny）算法步骤：</strong></p>
<ol>
<li>用一个高斯滤波器平滑输入图像</li>
<li>计算梯度幅值图像和角度图像</li>
<li>对梯度幅值图像应用非最大限制</li>
</ol>
<p><strong>边缘检测流程</strong></p>
<p>①滤波：边缘检测<a href="https://baike.baidu.com/item/%E7%AE%97%E6%B3%95?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">算法</a>主要是基于图像强度的一阶和二阶导数，但导数的计算对噪声很敏感，因此必须使用滤波器来改善与噪声有关的边缘检测器的性能。需要指出，大多数滤波器在降低噪声的同时也导致了边缘强度的损失，因此，增强边缘和降低噪声之间需要折中。</p>
<p>②增强:增强边缘的基础是确定图像各点邻域强度的变化值。增强算法可以将<a href="https://baike.baidu.com/item/%E9%82%BB%E5%9F%9F?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">邻域</a>(或局部)强度值有显著变化的点突显出来。<a href="https://baike.baidu.com/item/%E8%BE%B9%E7%BC%98%E5%A2%9E%E5%BC%BA?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">边缘增强</a>一般是通过计算梯度<a href="https://baike.baidu.com/item/%E5%B9%85%E5%80%BC?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">幅值</a>来完成的。</p>
<p>③检测：在图像中有许多点的梯度<a href="https://baike.baidu.com/item/%E5%B9%85%E5%80%BC?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">幅值</a>比较大，而这些点在特定的应用领域中并不都是边缘，所以应该用某种方法来确定哪些点是边缘点。最简单的边缘检测判据是梯度<a href="https://baike.baidu.com/item/%E5%B9%85%E5%80%BC?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">幅值</a>阈值判据。</p>
<p>④定位：如果某一应用场合要求确定边缘位置，则边缘的位置可在子像素分辨率上来估计，边缘的方位也可以被估计出来。在边缘检测算法中，前三个步骤用得十分普遍。这是因为大多数场合下，仅仅需要边缘检测器指出边缘出现在图像某一像素点的附近，而没有必要指出边缘的精确位置或方向。</p>
<p>边缘检测的实质是采用某种<a href="https://baike.baidu.com/item/%E7%AE%97%E6%B3%95?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">算法</a>来提取出图像中对象与背景间的交界线。我们将边缘定义为图像中<a href="https://baike.baidu.com/item/%E7%81%B0%E5%BA%A6?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">灰度</a>发生急剧变化的区域边界。图像灰度的变化情况可以用图像灰度分布的梯度来反映，因此我们可以用局部图像<a href="https://baike.baidu.com/item/%E5%BE%AE%E5%88%86?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">微分</a>技术来获得边缘检测<a href="https://baike.baidu.com/item/%E7%AE%97%E5%AD%90?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">算子</a>。经典的边缘检测方法，是通过对原始图像中像素的某小<a href="https://baike.baidu.com/item/%E9%82%BB%E5%9F%9F?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">邻域</a>构造边缘检测<a href="https://baike.baidu.com/item/%E7%AE%97%E5%AD%90?fromModule=lemma_inlink" target="_blank" rel="noopener noreferrer">算子</a>来达到检测边缘这一目的的。</p>
<p>与Sobel、Prewitt等算子相比，Canny算法更为优异。Sobel、Prewitt等算子有如下缺点：</p>
<ul class="task-list-container">
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-0" disabled="disabled"><label class="task-list-item-label" for="task-item-0"> 1、没有充分利用边缘的梯度方向。</label></li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-1" disabled="disabled"><label class="task-list-item-label" for="task-item-1"> 2、最后得到的二值图，只是简单地利用单阈值进行处理。</label></li>
</ul>
<p>而Canny算法基于这两点做了改进，提出了：</p>
<ul class="task-list-container">
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-2" disabled="disabled"><label class="task-list-item-label" for="task-item-2"> 1、基于边缘梯度方向的非极大值抑制。</label></li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-3" disabled="disabled"><label class="task-list-item-label" for="task-item-3"> 2、双阈值的滞后阈值处理</label></li>
</ul>
<p>Robert算子定位比较精确，但由于不包括平滑，所以对于噪声比较敏感。Prewitt算子和Sobel算子都是一阶的微分算子，而前者是平均滤波，后者是加权平均滤波且检测的图像边缘可能大于2个像素。这两者对灰度渐变低噪声的图像有较好的检测效果，但是对于混合多复杂噪声的图像，处理效果就不理想了。LOG滤波器方法通过检测二阶导数过零点来判断边缘点。LOG滤波器中的a正比于低通滤波器的宽度，a越大，平滑作用越显著，去除噪声越好，但图像的细节也损失越大，边缘精度也就越低。所以在边缘定位精度和消除噪声级间存在着矛盾，应该根据具体问题对噪声水平和边缘点定位精度要求适当选取。</p>
<p>讨论和比较了几种常用的边缘检测算子。梯度算子计算简单,但精度不高,只能检测出图像大致的轮廓,而对于比较细的边缘可能会忽略。Prewitt 和Sobel 算子比Roberts 效果要好一些。LOG 滤波器和Canny 算子的检测效果优于梯度算子,能够检测出图像较细的边缘部分。不同的系统,针对不同的环境条件和要求,选择合适的算子来对图像进行边缘检测。</p>
<p><strong>点、线、边缘检测</strong></p>
<p>点：可以用二阶导为基础检测孤立点，在普拉斯算子的模板响应添加一个阈值T，就可以轻易的检测出孤立点</p>
<p>线：采用拉普拉斯模板，由于拉普拉斯是各向同性的（其响应与方向无关），则我们可以通过对模板做出调整来检测特定方向的线。</p>
<h3> 3、特征描述子</h3>
<ul>
<li>
<p><strong>1.SIFT</strong></p>
<p><strong>特点</strong>图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。</p>
<p><strong>可以解决的问题</strong>目标的自身状态、场景所处的环境和成像器材的成像特性等因素影响图像配准/目标识别跟踪的性能，SIFT算法在一定程度上可以解决：</p>
<ul>
<li>目标的旋转、缩放、平移</li>
<li>图像仿射/投影变换</li>
<li>光照影响</li>
<li>目标遮挡</li>
<li>杂物场景</li>
<li>噪声</li>
</ul>
</li>
<li>
<p>2.<strong>SURF</strong></p>
<p>在积分图像上使用了高斯<a href="https://so.csdn.net/so/search?q=%E6%BB%A4%E6%B3%A2%E5%99%A8&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">滤波器</a>对二阶微分模板进行了简化，从而构建了Hessian矩阵元素值，进而缩短了特征提取的时间，提高了效率。</p>
<p>主要特点是快速性，同时也具有尺度不变的特性，对光照变化和仿射、透视变化也具有较强的鲁棒性（系统在其特性或参数发生摄动时仍可使品质指标保持不变的性能）</p>
</li>
<li>
<p>3.<strong>ORB</strong></p>
<p>使用FAST进行特征点检测，然后用BREIF进行特征点的特征描述，但是BRIEF并没有特征点方向的概念，所以ORB在BRIEF基础上引入了方向的计算方法，并在点对的挑选上使用贪婪搜索算法，挑出了一些区分性强的点对用来描述二进制串。</p>
</li>
<li>
<p>4.对于SIFT SURF ORB</p>
<p>三种方法在对特征点的描述的细致程度上 SIFT高于SURF 、SURF高于ORB。但是从运行的速度快慢来看，整个顺序恰好相反。 在选择特征提取方法的时候要根据实际应用情况来选择，如进行离线的图像的拼接，3D建模等对时间要求不是很严格应用，可以选择SIFT。但是当应用场合是在线的实时检测，就要选择比较快速的 SURF或ORB</p>
</li>
<li>
<p>5.<strong>LoG</strong></p>
<p>因为拉普拉斯算子对噪声很敏感，所以首先利用高斯对图像进行降噪处理，再采用拉普拉斯算子进行边缘检测，就可以提高对噪声和离散点的鲁棒性</p>
<p>即利用二阶高斯导数（也叫拉普拉斯变换）与原始信号（图像）卷积，通过检测局部极值获得角点。</p>
<p><strong>特别适用于以突出图像中的孤立点、孤立线或线端点为目的的场合</strong></p>
<p>LoG的斑点检测（类似SIFT）：</p>
<p>1.预定义一组方差值（因为不知道待检信号的尺度），对每个方差生成一个二阶高斯模板</p>
<p>2.对每个方差，将对应的高斯模板与原始信号做卷积（DOH需要将三个模板分别与原始图做卷积，然后计算其加权行列式），得到一组不同尺度的图像集</p>
<p>3.对每个空间位置，比较其在图像集里26（3_3_3-1）个位置（图1-4）的值，如果为极值，则认为在该点有一个斑点（高斯金字塔极值检测）</p>
</li>
<li>
<p>6.**DoH&nbsp;(**Determinant of Hessian)</p>
<p>基本思路和LoG差不多，只不过使用了Hessian矩阵
</p>
<p>一张图计算xx，yy，xy三个方向的卷积，然后计算其加权行列式：</p>
<p>理论上，与LOG相比，DOH对细长结构的斑点有较好的抑制作用。</p>
<p>利用像素点Hessian矩阵（二阶微分）及其行列式值
</p>
<p>无论是LoG还是DoH，它们对图像中的斑点进行检测，其步骤都可以分为以下两步：</p>
<p>1）使用不同的σ生成(∂2g∂x2+∂2g∂y2)(∂2g∂x2+∂2g∂y2)或∂2g∂x2,∂2g∂y2,∂2g∂x∂y∂2g∂x2,∂2g∂y2,∂2g∂x∂y模板，并对图像进行卷积运算；</p>
<p>2）在图像的位置空间与尺度空间中搜索LoG与DoH响应的峰值。</p>
</li>
<li>
<p>7.<strong>DoG</strong></p>
<p>高斯差分法，SIFT的第一步。通过将目标图像与高斯函数进行卷积运算得到一幅目标图像的低通滤波结果</p>
<p>LoG比DoG明显需要更多的加法运算和乘法运算。虽然DoG需要在每层金字塔多做一次高斯操作，但通过减法取代LoG核的计算过程，显著减少了运算次数，大大节省了运算时间。</p>
<p>通过数学公式可以发现：可以用DoG 算子来近似 LoG算子</p>
</li>
<li>
<p>8.<strong>Harris</strong></p>
<p>基于图像灰度的一阶导数矩阵检测方法。检测器的主要思想是局部自相似性/自相关性，即在某个局部窗口内图像块与在各个方向微小移动后的窗口内图像块的相似性。</p>
<p>Harris 算子的优越性有：</p>
<p>计算简单:Harris 算子中只用到灰度的一阶差分以及滤波,操作简单。 提取的点特征均匀而且合理:Harris 算子对图像中的每个点都计算其兴趣值,然后在邻域中选择最优点。实验表明,在纹理信息丰富的区域,Harris 算子可以提取出大量有用的特征点,而在纹理信息少的区域,提取的特征点则较少。 稳定:Harris算子的计算公式中只涉及到一阶导数，因此对图像旋转、灰度变化、噪声影响和视点变换不敏感,它也是比较稳定的一种点特征提取算子。 Harris 算子的局限性有：</p>
<p>1.它对尺度很敏感，不具有尺度不变性。（不管原图尺度是多少，在包含了所有尺度的尺度空间下都能找到那些稳定的极值点，这样就做到了尺度不变） 2.提取的角点是像素级的。</p>
</li>
<li>
<p>9.<strong>FAST</strong></p>
<p>基于加速分割测试的FAST算法可以快速地提取出角点特征。</p>
<p>在图像中选取一个像素点，来判断它是否为关键点。表示像素点的灰度值。 选择适当的阈值。 在像素点的周围选择16个像素点进行测试。 如果在这16个像素点中存在个连续像素点的灰度值都高于，或者低于，那么像素点被认为是一个角点。 一种更加快的改进是首先检测像素点周围的四个点（1，5，9，12）中是否有三个点满足阈值要求。如果不满足，则直接跳过，如果满足，则继续使用前面的算法，全部判断16个点中是否有12个满足条件。 上述算法效率很高，但是缺点如下所示：</p>
<p>1、获得的候选点比较多。 2、检测出来的角点不是最优的，因为它的效果取决于要解决的问题和角点的分布情况。 3、对于角点分析的结果被丢弃了。 4、检测到的很多角点都是连在一起的。</p>
<p>前3个问题可以通过机器学习的方法解决（待完善），最后一个问题可以使用非最大值抑制的方法解决。</p>
<p>非极大值抑制，如下所示：</p>
<p>对所有检测到的角点构建一个打分函数。就是像素点与周围16个像素点差值的绝对值之和。 考虑两个相邻的角点，并比较它们的值。 值较低的角点将会被删除</p>
<p>总体来说，FAST算子比其它角点检测算法都快，但是当图像中的噪点较多时，它的健壮性并不好，而且算法的效果还依赖于阈值。并且FAST算子不产生多尺度特征而且FAST角点没有方向信息，这样就会失去旋转不变性。</p>
</li>
<li>
<p>10、<strong>BRIEF</strong></p>
<p>BRIEF是对已检测到的特征点进行描述，它是一种二进制编码的描述子，摈弃了利用区域灰度直方图描述特征点的传统方法，大大的加快了特征描述符建立的速度，同时也极大的降低了特征匹配的时间，是一种非常快速，很有潜力的算法。</p>
<p>BRIEF仅仅是特征描述子，所以事先要得到特征点的位置，可以利用FAST特征点检测算法或Harris角点检测算法或SIFT、SURF等算法检测特征点的位置。接下来在特征点邻域利用BRIEF算法建立特征描述符。</p>
<p>特征点周围邻域内选取若干个像素点对，通过对这些点对的灰度值比较，将比较的结果组合成一个二进制串字符串用来描述特征点。最后，使用汉明距离来计算在特征描述子是否匹配。</p>
</li>
<li>
<p>11、<strong>BRISK</strong></p>
<p>具有较好的旋转不变性、尺度不变性，较好的鲁棒性等。在图像配准应用中，速度比较：SIFT &lt; SURF &lt; BRISK &lt; FREAK &lt; ORB，在对有较大模糊的图像配准时，BRISK算法在其中表现最为出色。</p>
<p>BRISK算法主要利用FAST9-16进行特征点检测（为什么是主要？因为用到一次FAST5-8）。要解决尺度不变性，就必须在尺度空间进行特征点检测，于是BRISK算法中构造了图像金字塔进行多尺度表达。</p>
<p><strong>步骤：</strong></p>
<p>特征点检测:</p>
<p>建立尺度空间→特征点检测→非极大值抑制→亚像素插值 特征点描述:</p>
<p>高斯滤波→局部梯度计算→特征描述符</p>
</li>
<li>
<p>12、<strong>FREAK&nbsp; （Fast Retina Keypoint）</strong></p>
<p>可以看出来该算法的一个特点是快速，另外一个特点就是该算法是被人眼识别物体的原理上得到启发提出的。</p>
<p>BRIEF、ORB和BRISK都是特征点周围的邻域像素点对之间的比较形成的二进制串作为特征点的描述符，这种做法有着快速和占用内存低的优点，在如今的移动计算中有很大的优势，但是也遗留了一些问题。比如，如何确定特征点邻域中哪些像素点对进行比较，如何匹配它们呢？</p>
<p>特征点检测方法与BRISK中特征点检测方法相同</p>
</li>
</ul>
<h2> 三、图像分割</h2>
<ul>
<li><strong>基于阈值分割</strong></li>
</ul>
<p>通过设定不同的特征阈值，把像素点分为具有不同灰度等级的目标区域和背景区域。</p>
<ul>
<li><strong>关键在于确定阈值</strong></li>
</ul>
<p>利用灰度直方图的峰谷法、最小误差法、基于过渡区法、利用像素点空间位置信息的变化阈值法、结合连通信息的阈值法、最大相关性原则和最大熵原则的自动阈值法。</p>
<ul>
<li><strong>基于区域分割</strong></li>
</ul>
<p>1、基于区域生长，从单个像素出发，逐步合并所需要的分割区域</p>
<p>2、区域分裂合并，从全局出发，逐步分割所需要分割的区域</p>
<ul>
<li><strong>基于边缘检测</strong></li>
</ul>
<p>通过检测包含不同区域的边缘来解决分割方法，是最常用的方法，通常来说，不同区域之间的边缘像素灰度的变化比较剧烈。
<strong>基本方法</strong>：<strong>图像一阶导数极大值，二阶导数过零点信息作为基本判据。</strong></p>
<p>一阶算子：<strong>Robert算子</strong>，Prewitt算子，<strong>Sobel算子</strong> 二阶算子：<strong>拉普拉斯算子</strong> 优点:边缘定位准确、速度快 难点：单纯的边缘检测无法保证边缘的连续性和封闭性，二是边缘检测在高细节区存在大量的碎片边缘。</p>
<ul>
<li><strong>基于深度模型</strong></li>
</ul>
<figure><figcaption></figcaption></figure>
<h2> 四、形态学运算</h2>
<ul>
<li>开运算:先腐蚀后膨胀：dilate(erode(src,element))</li>
<li>闭运算：先膨胀后腐蚀：erode(dilate(src,element))</li>
<li>形态梯度：膨胀图与腐蚀图之差：dilate(src,element)-erode(src,element)</li>
<li>顶帽：原图与开运算之差：src-open(src,element)</li>
<li>黑帽：闭运算与原图之差：close(src,element)-src</li>
</ul>
<figure><figcaption></figcaption></figure>
<figure><figcaption></figcaption></figure>
<h2> 五、姿态估计</h2>
<ul>
<li>
<p>aruco标记检测</p>
<p>ArUco 标记是由宽黑色边框和确定其标识符（id）的内部二进制矩阵组成的正方形标记。</p>
<p>黑色边框有助于其在图像中的快速检测，内部二进制编码用于识别标记和提供错误检测和纠 正。</p>
<p>对 ArUco 标记进行检测时，需要返回检测结果的一系列信息，包括：  图像中四个角的位置（按原始顺序）。  标记的 ID。</p>
<p>在 aruco 模块中，detectMarkers()函数实现标志的检测。</p>
<p>标记检测过程的第一步是对输入图像进行自适应阈值化。</p>
<p>在候选标志检测完成后需要对每个候选标志进行数位分析，以确定它们是否是 ArUco 标记。</p>
<p>提取标记中二进制信息后，接下来检查提取的二进制代码是否属于标记字典。</p>
<p>检测并识别标记后，最后一步是对角点位置执行亚像素细化。这一步是可选的，只有在 标记角点位置必须为精确值的情况下才有意义，例如姿态估计、运动测量等问题中。角点位置细化通常是一个耗时的步骤，默认情况下是不使用的。</p>
<p>检测到标记后，我们需要从标记中获取相机姿态。要执行相机姿态估计，我们需要了解 相机的标定参数。这是相机内参矩阵和畸变系数。使用 OpenCV 基础库中标定函数。</p>
<p>相机相对于标记的姿态是从标记坐标系到相机坐标系的 3d 转换。它由旋转量和平移矢 量确定。aruco 模块提供了 cv::aruco::estimatePoseSingleMarkers()函数用于估计所有检测到的标记的姿态。</p>
</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>图像处理-冈萨雷斯</title>
      <link>https://tobeprozy.github.io/02Image_Processing/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0.html</link>
      <guid>https://tobeprozy.github.io/02Image_Processing/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">图像处理-冈萨雷斯</source>
      <description>注意 图像处理的一般步骤：提取原始图像→灰度处理→滤波→边缘检测→图像分割→特征提取。 〇、图像性质和表达 0.1 基本概念 🪁 灰度图像表示为两个变量的标量函数f(x,y),其中x,y是平面内的坐标 数字化图像的定义域是一个有限的离散山歌，器坐标是自然数。数字化图像的值域是一个有限的灰度值（亮度）的离散集合。像素是图像的基本单位。 0.2 图像数字化</description>
      <category>图像处理</category>
      <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container warning">
<p class="hint-container-title">注意</p>
<p>图像处理的一般步骤：提取原始图像→灰度处理→滤波→边缘检测→图像分割→特征提取。</p>
</div>
<h1> 〇、图像性质和表达</h1>
<h2> 0.1 基本概念</h2>
<aside> 🪁 灰度图像表示为两个变量的标量函数f(x,y),其中x,y是平面内的坐标 数字化图像的定义域是一个有限的离散山歌，器坐标是自然数。数字化图像的值域是一个有限的灰度值（亮度）的离散集合。像素是图像的基本单位。
</aside>
<h2> 0.2 图像数字化</h2>
<aside> 🪁 图像是数字化可以看作是采样函数与连续图像函数的乘积
<p>通常栅格由规则的多边形（方形或正六边形）组成。采样的第二个当面是设置采样点间的距离（采样距离越小图像的分辨率越高）。</p>
<p>灰阶的量化决定着明暗和伪轮廓。人最多可以识别大约60个灰度级别，只含有黑和白像素的图像称为二值图。</p>
</aside>
<h2> 0.3 数字图像性质</h2>
<aside> 🪁 为了能够表达离散几何，必须定义像素的邻接关系。 必须建立根据两个像素间距离的函数，把几种已在使用的定义方法。最常用的是”城市街区“、”棋盘“和日常生活中用到的欧氏距离。如果栅格上设置了邻接关系，就获得了光栅。
<p>给定光栅，就引入了拓扑性质。这些性质是基于”连通的“这以关系，导致区域、背景、孔和区域边界概念。区域的凸包是包含它的一个最小凸子集。</p>
<p>4-邻接和8-邻接会产生”交叉线条“悖论，使基本的离散几何算法复杂化。但是，对于二值和灰度图像都存在解决这些悖论的方法。</p>
<p>一副二值图像的距离变换（斜切）提供每个像素到最近的非零像素的距离。存在一个在计算上高效的计算距离变换的两遍算法，具有与像素数目成线性的复杂度。</p>
<p>亮度直方图是图像亮度全局描述，给出了像素具有某个亮度的概率密度估计。 人的视觉感知有很多错觉。感知组织作为人感知图像的一些性质对于计算机视觉有启发作用。</p>
<p>现场图像与任何测量或观测一样，总是带有噪声，定量地估计噪声的程度是可能的，比如用信噪比。</p>
<p>噪声的常见模型有：白噪声、高斯噪声、冲击噪声、椒盐噪声</p>
</aside>
<h2> 0.4 彩色图像</h2>
<aside> 🪁 人的色彩感知是在电磁辐射的波长这一客观的物理性质基础之上的主观心理物理层次。
<p>人类针对入射幅照光建立起了波长敏感的三种基本类型的传感器。人类视网膜上的颜色敏感的感受器是锥状体。视网膜上另一种光敏感受器是杆状体，专注于周围光照强度低的情况下的单色感知。</p>
<p>锥状体按照感知的波长范围分为三类：仅是对应于红、绿、蓝。</p>
</aside>
<h2> 0.5 摄像机</h2>
<aside> 🪁 多数相机使用CCD或CMOS光敏元件，两者都使用光伏原理。它们捕获单色图像的亮度。 摄像机配备了必要的电子组件以提供数字化的图像。彩色摄像机和单色摄像机类似，含有色彩滤波器。
</aside>
<h1> 一、图像预处理</h1>
<h2> <strong>1.1 图像预处理</strong></h2>
<p>输入输出都是亮度图像</p>
<p>**目的：**抑制不想要的变形或者增强某些对于后续处理重要的图像特征</p>
<ul>
<li><strong>方法：</strong>
<ol>
<li>像素亮度变换</li>
<li>几何变换</li>
<li>局部邻域预处理</li>
<li>图像复原</li>
</ol>
</li>
</ul>
<h2> 1.2 像素亮度变换</h2>
<h3> 1.2.1 两类：</h3>
<ol>
<li>亮度校正：在修改像素亮度的同时要考虑像素原来的亮度和其在图像中的位置</li>
<li>灰度级变换：在修改像素亮度时无需考虑其位置</li>
</ol>
<h3> 1.2.2 常见的亮度变换有</h3>
<ol>
<li>亮度阈值化</li>
<li><strong>直方图均衡化:目的是创建一幅在整个亮度范围内具有相同分布的亮度图像</strong></li>
<li>对数的灰度级变换</li>
<li>查找表变换</li>
<li>伪彩色变换</li>
</ol>
<h2> 1.3 几何变换</h2>
<h3> 1.3.1作用：可以消除图像获取时所出现的几何变形</h3>
<h3> 1.3.2步骤：</h3>
<ol>
<li>像素坐标变换</li>
<li>亮度插值</li>
</ol>
<p>像素坐标变换将输入图像映射到输出图像，常用的有仿射变换和双线性变换，经过变换后，输出点的坐标一般并不符合数字离散光栅；插值被用来确定输出像素的亮度。常使用的有最近邻插值、线性插值、双三次插值</p>
<h2> 1.4局部预处理</h2>
<h3> 1.4.1 方法</h3>
<p>使用输入图像中一个像素的小邻域来产生输出图像中新的亮度数值</p>
<h3> <strong>1.4.2 分类</strong></h3>
<ol>
<li><strong>图像平滑</strong></li>
<li><strong>边缘检测</strong></li>
</ol>
<h3> 1.4.3图像平滑</h3>
<p>目的：抑制噪声或其他小的波动，等同于在傅里叶频域抑制高频部分</p>
<p><strong>算法：</strong></p>
<ol>
<li>均值<strong>滤波</strong></li>
<li><strong>中值滤波：是一种减小边缘模糊的非线性平滑方法</strong></li>
<li>非线性均值滤波：</li>
</ol>
<p>基于直接平均的平滑方法会模糊边缘，改进的方法通过在一致性的局部区域内平均来减小模糊。</p>
<p>中值滤波是一种非线性操作，它用邻域中亮度的中值代替图像中当前的点来减小模糊</p>
<h3> 1.4.4 边缘检测</h3>
<p>**通过梯度算子确定边缘：**边缘是亮度函数发生急剧变化的位置，它们的效果类似于在傅里叶频域抑制低频部分。</p>
<ul>
<li>
<p><strong>常用算子：</strong></p>
<ol>
<li>Roborts算子：2x2，缺点是对噪声高度敏感</li>
<li>Laplace算子：3x3，缺点是对某些边缘产生双重响应</li>
<li>Prewitt算子：3x3，与Sobel、Kirsch算子类似，近似一阶导数,在8个可能的方向估计梯度</li>
<li>Sobel算子：3x3，通常用于检测水平或者垂直边缘</li>
<li>Kirsch算子：</li>
</ol>
<p>图像锐化：一般使用Laplacian算子</p>
</li>
</ul>
<aside> 🪁 边缘是赋给单个像素的性质，它既有幅值强度又有方向。多数的梯度算子可以用卷积掩膜来表达，比如Roberts、Laplace、Prewitt、Sobel、Kirsch算子。
</aside>
<blockquote>
<p>卷积边缘检测子的主要缺点是：依赖尺度且对噪声敏感。选择某个最好的局部邻域算子尺度并不是那么容易的。</p>
</blockquote>
<p>二阶导数过零点比小尺度的梯度检测更加的稳定，可以用Laplacian of Gaussians (LoG)或者difference of Gaussions（DoG）来计算。</p>
<p>在多光谱图像中，也可以检测边缘。</p>
<h3> 1.4.5边缘提取</h3>
<p>Canny边缘检测子对受白噪声影响的阶跃型边缘是最优的。最优性标准是基于如下要求的:检测重要边缘、小的定位误差、单边缘响应。该检测子与一个对称2D高斯做卷积，再沿梯度方向微分；接着的步骤包括非最大边缘抑制、滞后阈值化处理和特征综合。</p>
<ol>
<li>Canny边缘提取</li>
</ol>
<p>（1）将图像f与尺度为delta的高斯函数做卷积</p>
<p>（2）对图像中每个像素，估计局部边缘的法线向量</p>
<p>（3）用非极大值抑制方法找到边缘位置</p>
<p>（4）计算边缘强度</p>
<p>（5）对边缘图像做滞后阈值化处理</p>
<p>（6）对递增的标准差，重复1-5步骤</p>
<p>（7）用特征综合办法，收集来自多尺度</p>
<h3> 1.4.6 角点检测</h3>
<p>1、Moravec检测</p>
<p>2、Harris角点检测器</p>
<p>（1）首先，对图像进行高斯滤波</p>
<p>（2）对每个像素，估计其垂直两个方向的梯度，使用近视导数的做两次一维卷积即可</p>
<p>（3）对每个像素和给定的邻域窗口：计算局部结构矩阵A，计算响应函数R（A）</p>
<p>（4）选取响应函数R（A）的一个阈值，以选取最佳候选角点，并完成非最大化抑制。</p>
<p>特点：对二维平移、旋转、少量光照变化、视角变化都不敏感，而且计算量很小</p>
<ol>
<li>
<p>最大稳定极值区域检测（MSER）</p>
<p>不仅在旋转、平移后，即便经历相似和仿射变换也可以被重复检测出来</p>
<p><strong>遍历极值区域算法流程：</strong></p>
<p>（1）根据亮度值对图像像素进行排序</p>
<p>（2）从最小灰度开始向上迭代</p>
<p>（3）考虑当前灰度值为g的像素；不断加入像素块并更新连通域结构</p>
<p>（4）如果两个区域合并，可以看做一个小块的消失</p>
<p>（5）当所有的亮度值都已被处理，我们得到一个保存每一个连通域面积的数据结构，可以看做是一个阈值的函数。</p>
</li>
</ol>
<h3> 1.4.7其它局部预处理</h3>
<ol>
<li>线条寻找</li>
<li>线条细化</li>
<li>线条不缺</li>
<li>兴趣点检测</li>
</ol>
<p>一幅图像中，诸如角点和最大稳定极值区域等结构包含更丰富的信息，检测边缘更为稳定，他们常用语图像匹配。</p>
<h2> 1.5图像复原</h2>
<p>1.5.1 概念：利用有关退化性质的知识来抑制退化，多数图像复原方法是基于整幅图像上的全局性去卷积的方法。</p>
<p>1.5.2 三类典型的退化具有简单的函数形式：物体相对于摄像机作近似匀速运动、不当的镜头焦距、大气扰动。</p>
<p>1.5.3 逆滤波假设退化是由线性函数引起的</p>
<p>1.5.4 维纳滤波给出了对未被噪声污染的原始图像的一个最小均方差估计；一般而言，它是退化图像的非线性函数。</p>
<h1> 二、图像的分割</h1>
<h2> 2.1 图像分割概述</h2>
<p>**目的：**将图像划分为与其中含有的真实世界的物体区域有强相关性的组成部分</p>
<p>**分割的方法：**阈值化、基于边缘的、基于区域的</p>
<p>每个区域可以用其封闭的边界来表示，每个封闭的边界描述一个区域</p>
<p>**主要的分割问题：**图像数据的不明确性和信息噪声</p>
<p>分割过程中可得到的先验信息越多，所获得的的分割效果就越好，</p>
<h2> 2.2 阈值化</h2>
<p>1、阈值化是最简单的分割处理，计算代价小、速度快。一个常量阈值用来分割物体和背景。既可以在整幅图像上施加阈值(全局阈值)，也可以依赖于图像部分而改变的阈值（局部阈值）。单个阈值在整幅图像上成功的例子很少。</p>
<p>2、阈值化有许多修正：局部阈值化、带阈值化、多阈值化</p>
<p>3、阈值检测方法自动地确定阈值。如果事先知道分割后的图像的某种性质，就可以简化阈值选择，因为阈值可以按照该性质得以满足的条件来选择。阈值检测可以使用p率阈值化、直方图形状分析、最优阈值等等。</p>
<p>4、在二模态直方图中，阈值可以确定为两个最大的局部极大值和极小值位置。</p>
<p>5、最优阈值化确定阈值为离对应于两个或更多个正态分布最大值之间的最小概率处最近的灰度值。其结果是具有最小错误的分割。</p>
<p>6、多光谱阈值化适合彩色或多谱段图像。</p>
<h2> <strong>2.3 基于边缘的分割</strong></h2>
<p>1、基于边缘的分割依赖于边缘检测子；边缘标示了图像在灰度、彩色、纹理等方面连续的位置。</p>
<p>2、图像噪声或不适合的信息通常可以导致在没有边界的地方出现了边缘以及在实际存在边界的地方没有出现边缘。</p>
<p>3、边缘图像阈值化是基于边缘图构建的，由合适的阈值来实现。</p>
<p>4、边缘松弛法在相邻边缘的上下文中考虑边缘。如果存在边界出现的足够证据，就增加局部边缘强度，反之亦然。全局松弛法优化过程建立边界。</p>
<p>5、可以定义内边界、外边界和拓展的边界。内边界总是区域的一部分；外边界绝不是，那么利用内边界、外边界的定义，如果两个区域相邻，它们绝对不会有共同的边界。拓展边界定义了相邻区域的单一的共同边界，可以用标准的像素坐标来标识。</p>
<p>6、如果定义了最优性准则，可以使用**（启发式）图搜索<strong>或</strong>动态规划方法**确定全局最优边界。基于搜索的边界检测是一种极为有利的分割方法——边界检测过程被转化为在加权图中搜索最优路径的问题。节点与费用关联起来，该费用反映边界通过某个特定节点（像素）的可能性。连接两个指定节点即起点和终点的最优路径（最优边界，相对于某个目标函数来说）就得以确定。</p>
<p>7、费用定义（评价函数）是边界检测成功的关键。费用计算的复杂度的变化范围覆盖了从简单的边缘强度的逆到复杂的先验知识的表示，先验知识是有关待搜索的边界、分割任务、图像数据等的。</p>
<p>8、图搜索使用Nilsson的A-算法，可以确保最优性。启发式图搜索可以显著地加快搜索速度，尽管启发式必须满足附加的约束才能确保最优性。</p>
<p>9、动态规划是基于最优化原理的，给出了从多个起点和终点中同时搜索最优路径的一个有效的方法。</p>
<p>10、使用A-算法搜索图，并不需要构造整个图，因为只有需要时才计算扩展节点的费用。在动态规划中，必须建好完整的图。</p>
<p>如果局部费用函数的计算简单，动态规划 可能是在计算上话费不高的选择。然而，对于特定的问题而言，两种图搜索方法（A-算法、动态规划）中究竟哪种方法更有效，取决于评价函数和A-算法的启发式信息的性质。</p>
<p>11、Hough变换分割适用于在图像中检测已知形状的物体。Hough变换可以检测直线和已知解析公式的曲线（物体边界）。在识别有遮挡物的或受到噪声影响的物体方面是鲁棒性的。</p>
<p>如果待搜索形状的解析公式并不存在，可以用广义Hough变化，参数曲线（区域边界）描述是基于样本情形的，并在学习阶段确定下来。</p>
<p>12、尽管根据完全的边界形成区域是微不足道的，根据部分边界确定区域可能是一个非常复杂的任务。可以根据如下的概率来简历区域：像素是否位于由部分边界包围的区域内。这些方法并不总是能找到可以接受的区域，但是它们在很多实际情况中很有用</p>
<h2> 2.4 基于区域的分割</h2>
<p>1、区域增长分割应该满足：完全分割条件、最大区域一致性条件</p>
<p>2、有三种基本的区域增长方式：区域归并、区域分裂、分裂与归并区域增长</p>
<p>3、在分水岭分割中，集水盆地代表了分割后图像的区域。分水岭分割的最初的算法开始与寻找从图像的每个像素到图像表面搞成的局部极小的下游路径。定义集水盆地为满足以下条件的所有像素的集合：这些像素的下游路径终止于同一个高程极小点。在第二种方法中，每个极小值代表了一个集水盆地，策略是冲这个高程极小值开始填充集水盆地。</p>
<p>4、使用区域增长方法分割图像，时常由于参数设置的非最优性造成的结果，不是含有太多的区域（欠增长）就是含有过少的区域（过增长）。许多后处理器被提出来改进的分类结果。简单的后处理器减少分割后图像中的小区域的数目。更复杂的后处理方法可以从区域增长得到的分割信息与基于边缘的分割结合起来。</p>
<h2> 2.5 匹配</h2>
<p>1、模板匹配可以用于在图像中定位已知表观的物体，也可以用于搜索的模式等，最好的匹配是基于某种最优性准则的，该准则依赖于物体的性质和物体的关系。</p>
<p>2、匹配标准的定义可以有多种方式，特别地，模式与被搜索的图像数据间的相关性是一个普遍性的匹配标准。</p>
<p>3、斜切匹配可以用于定位一维特征，不然使用基于费用的最优方法可能会失效。</p>
<h2> 2.6 评测</h2>
<p>1、基于模板的匹配耗时，但是该过程可以通过引入合适的模板匹配控制策略来加速</p>
<p>2、分割的评测对于决定分割算法，给定算法的参数选择非常有用。</p>
<p>3、监督式的评测比较了分割算法的输出和真值。</p>
<p>4、监督式的方法通常比较相互重叠区域，或者分割边界间的距离——存在一些不同的做法。</p>
<p>5、真值尝尝难以定义或者获得的代价很大。非监督式的方法评价分割效果时不需要考虑真值。</p>
<p>6、有很多非监督式的存在，但它们通常受到图像区域假设的限制。</p>
<h1> 三、物体识别</h1>
<h2> <strong>3.1 物体识别和模式识别</strong></h2>
<p>1、模式识别用于区域及物体分类，是复杂机器视觉处理中的重要组成部分。</p>
<p>2、所有识别操作都要根据一定的知识，既需要关于待处理物体的知识，也需要关于物体类别的更高层次上的一般性知识。</p>
<h2> <strong>3.2 知识表示</strong></h2>
<p>1、描述与特征 2、语法与语言 3、谓词逻辑 4、 产生式规则 5、模糊逻辑 6、语义网络 7、 框架和脚本</p>
<h2> 3.3 统计模式识别</h2>
<p>1、物体识别判断物体的类别，完成这种判定的仪器成为分类器</p>
<p>2、通常类别的数目事先已知，一般可以根据具体的问题确定。</p>
<p>3、分类器将使用从物体中检测出的模式来进行决策。</p>
<p>4、最小近邻分类器易于理解且应用广泛。在高维或者大数据情况下，使用它非常耗时，但可以通过K-D树或者其他近似方法进行改进。</p>
<p>5、统计模式识别的一个特点是定量的物体秒速，并且采用基本的数值描述——特征。所有可能的模式构成了模式空间或特征空间。在特征空间中形成聚类，而这些聚类可以用分类超曲面分开。</p>
<p>6、统计分类器是一个具有n输入和1输出的装置。每个输入端接收关于n个特征中一个的信息，这n个特征由待分类物体测量得到。一个R-分类器输出R个符号中的一个wr，即标识符。</p>
<p>7、在分类器学习过程中，分类参数由一个样本训练集合确定。两种常用的学习策略为概率密度估计和直接损失最小化。</p>
<p>9、支持向量机的训练基于最大化两类的间隔。支持向量机的非线性分类得益于核技巧。结合多个两类问题的分类器可以得到多类问题的分类器。</p>
<p>10、聚类分析不需要学习训练集合。它根据待处理模式集合中各元素间相似度将整个集合划分为若干个集合（聚类）。</p>
<h2> 3.4 神经元网络</h2>
<p>1、大多神经元方法都基于对基本处理单元（神经元）的组合，每个处理器接收若干个输入，并生成一个输出。对每个输入都有一个对应的权值与其相对应，输入即是关于输入加权和的函数。</p>
<p>2、前馈网络在模式识别问题中经常用到，前馈网络采用反向传播算法学习一个训练集合而得到。</p>
<p>3、自组织网络不需要学习训练集合来达到给模式聚类的目的。</p>
<h2> 3.5 句法模式识别</h2>
<p>1、句法模式识别的特点是对物体的定性描述。句法描述的物体的基本性质成为基元。关系结构用来描述物体基元间的关系。</p>
<p>2、所有基元都成的集合成为字符集。由字符集中字符组成的 能够描述一类物体的所有词语的集合称为描述语言。语法是一个规则的集合，这些规则定义了特定语言中由字符集中的字符构造词语的可能方式。</p>
<p>3、构造语法通常需要很多人为干预，对于简单的情况，可以采用自动从样本构造语法的过程，这一过程成为语法推导。</p>
<p>4、关于待处理词语是否能由特定语法产生的识别判定在语法分析过程中完成。</p>
<h2> <strong>3.6 作为图匹配的识别</strong></h2>
<p>1、模型与物体图表示间的匹配可以用于识别。精确的图匹配成为图的同构。判定图的同构计算量非常大。</p>
<p>2、在现实世界中，物体图和模型图很难精准匹配。图同构不能估计不匹配的程度。为了识别由相似图表示的物体，需要决定图的相似度。</p>
<h2> 3.7 识别中的优化技术</h2>
<p>1、优化问题寻找目标函数的最小值和最大值。目标函数的设计是性能的关键。</p>
<p>2、大多数传统的优化方法采用基于微积分的爬山方法。这些方法很可能只能找到局部极大值，而不是全局极大值。</p>
<p>3、遗传算法利用适者生存的自然进化机制寻找目标函数的最大值。可能的解表示为一些字符串。遗传算法对可能解的一代进行搜索，而不是对单个解。复制、交叉和突变的序列作用于字符串的当前代，从而生成新的一代。具有最高适合度的串表示了最终解。</p>
<p>4、模拟退火将两个基本优化原理结合起来，分而治之和迭代改进（爬山算法）。这种结合避免了算法陷入局部极值点。</p>
<h2> 3.8 模糊系统</h2>
<p>1、模糊系统可以表示为多变的、不精确的、不确定的和不准确的知识和信息。与人类表达知识的方式类似，模糊系统采用修饰语。</p>
<p>2、模糊推理在模糊系统模型的环境下进行，后者由控制、解、操作数据变量、模糊集合、限制、模糊规则及一个控制机制构成。</p>
<p>3、模糊集合表示模糊空间中的性质。隶属函数体现了描述的模糊性，表示元素属于某个特定集合的确定程度。模糊隶属函数的形状可以通过模糊集限制进行调整。一个限制及其模糊结合构成了一个语义实体，称为语义变量。</p>
<p>4、模糊if then规则是存储知识的模糊联想存储器。</p>
<p>5、模糊推理将单独模糊集中蕴含的知识结合起来做出决策。决定相关模糊区域隶属度的函数关系被称为合成方法，并且由此决定了模糊解空间。为了做出决策，逆模糊过程被采用。合成和逆模糊过程构成了模糊推理的基本部分。</p>
<h2> 3.9 Boosting</h2>
<p>1、Boosting 是一个一般性的方法，它能够结合多个分类性能一般的分类器（也就是所谓的弱分类器）的输出，提高分类性能。</p>
<p>2、在Boosting里，一个复杂的分类规则被很多简单的分类规则代替了。其中，每个简单的分类规则可能只是比随机选择稍微好一点。因此，Boosting能够通过结合仅仅稍微精确度的分类器的输出得到非常精确的结构。</p>
<p>3、AdaBoosting 是一个广泛应用的Boosting算法，其中在训练集上依次训练弱分类器，每次下一个弱分类器是在训练样本的不同权重集合上训练的。权重是有每个样本的分类器的难度确定的。分类的难度是通过前面步骤中的分类器的输出估计的。</p>
<p>4、所有弱分类器的处处结合起来形成一个强分类器。这种结合是基于加权投票多数的。</p>
<p>5、对于弱分类器的选择，除了要求它们比随机分类的效果好以外，没有其他要求。</p>
<h2> <strong>3.10 随机森林</strong></h2>
<p>1、随机森林却别适合于那些包含很多类且有大量数据集可用于训练的问题。</p>
<p>2、随机森林主要用于两类决策任务：分类和回归</p>
<p>3、在分类问题中，决策输出是一个类标签。</p>
<p>4、在非线性回归问题中，输出是一个连续的数值。</p>
<p>5、森林中的每一棵树都可以被并行的训练，一旦训练完成，每个内部结点都关联一个预先定义的二值测试，而之前未曾见过的数据模式根据内部结点测试的结果从根结点被送到一个叶子结点。</p>
<p>6、一个随机森林包含了一个决策树集合，其中每一个可能从训练集的一个随机采样的子集训练得到。</p>
<p>7、与使用单个树进行决策相比，集成多个稍微不同的树能够得到明显高的精度和更好的噪声鲁棒性。</p>
<h1> 四、图像的理解</h1>
<h2> 4.1 图像的理解</h2>
<p>1、机器视觉是由较低和较高的处理层次构成，图像理解在这种分类方法中是最高层次的处理。</p>
<p>2、类似于生物系统，计算机视觉的目的是通过可能的技术和处理方法得到机器行为。</p>
<h2> 4.2 图像理解的控制策略</h2>
<p>1、并行和串行的处理控制</p>
<p>并行处理同时进行多个计算</p>
<p>串行处理操作是顺序的</p>
<p>几乎所有的低层次图像处理都可以并行处理。高层次的处理使用更高层抽象形式，在本质上，通常是串行处理。</p>
<p>2、分层控制</p>
<p>由图像数据控制（自底向上的控制策略）：处理过程从光栅图像开始分割图像，再到区域（物体）描述，最后是识别。</p>
<p>基于模型的控制（自顶向下的控制策略）：根据可利用的知识得到一系列假设和期望的性质。</p>
<p>混合的控制策略使用数据驱动和模型驱动这两种控制策略。</p>
<h2> 4.3 尺度不变特征变换：SIFT</h2>
<p>SIFT可以在视角变换的图像中检测已知的图像特征点。</p>
<p>SIFT可以从图像中提取稳定的点，并且用鲁棒特征对其进行描述，这些特征的一个小的具有几何一致性的子集就可以确定物体在其它图像的出现。</p>
<p>SIFT包括三个阶段：关键点检测、特征提取和匹配。</p>
<p>SIFT只需要三个匹配点对就可以定义一个可以使用的变换并且非常鲁棒。</p>
<h2> 4.4 RANSAC：通过随机抽样一致性来拟合</h2>
<p>经典的模型拟合方法通常基于最小二乘法、最小残差化的平方和。</p>
<p>如果数据集有瑕疵，异常值会对模型产生负面影响。</p>
<p>RANSAC从一个基于可用数据中的少数样本的简单模型开始，然后利用剩下的数据点来确定一致点和异常点，排除异常点后重新计算模型。</p>
<p>RANSAC代表模型拟合的范畴的改变：“从少数开始增长”是最小二乘法和其它相关方法的对立面，后者期望通过平均来消除偏差。</p>
<h2> 4.5 点分布模型PDM</h2>
<p>AAM同时对形状及其变化以及表观及其变化进行建模。</p>
<p>在训练集中建模时对形状及表观变化分别进行主分量分析。</p>
<p>对形状和亮度模型的参数的组合进行主分量分析，从而得到一系列同时刻画形状和纹理变化的分量。</p>
<p>AAM是PDM的推广，它增强了图像块纹理的亮度统计模型。</p>
<p>AAM的方法需要一个训练样本（图像块以及确定的物体边界）。根据这个数据集推导出对形状、亮度以及它们的组合的变化的统计描述。</p>
<h2> 4.6 图像理解中的模式识别</h2>
<p>监督和非监督的模式识别方法可以用于像素分类。在图像理解阶段，从局部多谱图像像素值中得到的特征向量送到分类器，分类器负责为图像的每个像素分配标记。图像理解可以通过像素标记得到。</p>
<p>被标注的结果图像可能会出现很多小的区域，它们可能是错分类的。基于上下文的后处理方法用于避免这种错误分类。</p>
<p>局部表观和形状可以通过特征方向图（HOG）来描述。可以用（线性）分类器对图像中的物体进行检测和定位。</p>
<h2> 4.7 Boosted层叠分类器</h2>
<p>Boosted层叠分类器使用了注意焦点样式。</p>
<p>自适应提升算法计算了大量的简单特征并选出了少部分最好的特征。</p>
<p>在下一级阶段，分类器被组织成为一个层叠的序列，以简单而快速的分类器为首，用于快速排除物体检测假设，然后仅在剩下的未被排除的假设上应用更加复杂强大而缓慢的分类器。</p>
<h2> 4.8 基于随机森林的图像理解</h2>
<p>随机森林把图像分成预先定义大小的图像块</p>
<p>训练集中的图像块来自物体，并带有标签，非物体的图像块是背景</p>
<p>通过联合分类和回归同时进行物体检测和定位。</p>
<p>识别阶段考虑图像尺度，所以图像块大小都相同。</p>
<p>微软Xbox是随机森林最成功的商业应用。</p>
<h2> 4.9 场景标注、约束传播</h2>
<p>离散标注仅仅允许在最终标注结果中，为每个物体分配一个标记。努力的方向是在整幅图像的范围内获得相容的标注。离散标注总是可以发现一个相容的标记或检测出无法为该场景分配相容的标记。</p>
<p>概率标注允许在物体中同时存在多个标记。标记以概率加权，为每一个物体的标记分配标记记置信度。概率标注通常可以给定一个解释结果以及该解释的置信度量。</p>
<p>约束传播的策略有助于整幅图像中通过局部相容性调整得到全局相容（全局相容）</p>
<p>物体标注依赖于物体性质和潜在物体标记与其他直接相互作用的物体的标记之间的相容性度量。</p>
<p>当搜索解释树，树结点被分配到所有可能的标记，使用基于回溯的深度优先搜索方法。解释树搜索并测试所有可能的标记。</p>
<h2> 4.10图像的语义分割和理解</h2>
<p>语义区域增长技术是使用邻接区域之间的先验知识将上下文结合到区域归并中，然后利用约束传播得到整幅图像全局最优的图像分割和解释。</p>
<p>遗传图像解释是基于假设和验证准则的。一个目标函数用于估计分割的优劣，使用遗传算法优化图像解释，该算法负责产生新的图像分割种群和用于检测的解释假设。</p>
<h2> 4.11 隐马尔可夫模型</h2>
<p>当试图进行图像理解时，我们常常可以将观察到的模式建模为跃迁系统。如果跃迁是事先知道的，且我们知道某个时刻系统的状态，它们就可以被用于帮助决定下一时刻的状态。马尔可夫模型是该思想最简单的例子。</p>
<p>隐马尔可夫模型要处理的三个问题：评价、解码和学习。</p>
<p>Viterbi算法可以用于从可能是不精确的观察中重建系统的演进。</p>
<p>简单的隐马尔可夫模型本身又有各种扩展：两个（或多个）概率上相互合作的隐马尔可夫模型，即耦合的马尔可夫模型非常成功。</p>
<h2> 4.12 贝叶斯信念网络</h2>
<p>由马尔可夫概率关系连接的隐藏和可见活动组成的网络。</p>
<p>如果这些网络无环，给定先验概率，有效的算法可以计算后验概率。</p>
<p>贝叶斯新年网络是一个通用的技术，在计算机领域广泛用于帮助推理。</p>
<h2> 4.13 马尔可夫随机场</h2>
<p>马尔可夫随机场是一种概率的网络，是局部影响的马尔可夫准则的推广。</p>
<p>理论上，它们的行为可以用团来刻画。如果网络是一个网格，这表明团是直接近邻：这在网格是像素的时候最为有用。</p>
<p>该理论可以映射到许多视觉问题中，其中先验假设可以解释图像。最可能的解释可以通过马尔可夫随机场产生。</p>
<p>先验和观测之间的强度可以控制。</p>
<p>最大化马尔可夫随机场似然可以通过高效的图分割方法求解。</p>
<p>该理论在视觉领域应用很广泛。</p>
<h2> 4.14 高斯混合模型和期望最大化</h2>
<p>高斯混合模型可以为真实场景中的很多方面提供易得的解析表示。</p>
<p>期望最大化算法可以确定高斯混合模型的参数（但可能不是最优的）</p>
<p>期望最大化算法是用于寻找某种描述性模型的未知参数的通用迭代过程。</p>
<p>利用Baum-Welch算法训练隐马尔可夫模型是期望最大化算法的另一个特例。</p>
<h1> 五、3D几何、对应、从亮度到3D</h1>
<h2> 5.1 概述</h2>
<p>3D视觉的目标在于从2D场景推断3D信息，是一个内含几何和辐射的困难任务。几何问题是单幅图像并不提供有关3D结构的充分信息，而辐射问题是创建亮度图像的物理过程的复杂性。这个过程是负责的，通常并非所有输入参数是精确地知道的。</p>
<h2> 5.2 3D视觉任务</h2>
<p>有几种不同的研究3D视觉的方法，可以分类为自底向上（或重构）或自上而下（基于模型的视觉）</p>
<p>Marr的理论，成形于20世纪70年代，是自底向上方法的一个例子。其目标是在有关场景中物体的非常弱的假设下，从一副或更多幅亮度图像重构出定性和定量的3D几何描述。</p>
<p>按自底向上的形式排出四个表达：（1）输入亮度图像（2）基元图，以观察者为中心的坐标系中表达图像中的显著边缘（3）2.5D图，表达到观察者的深度和表面的局部方向；（4）3D表达，在于物体自身相关的坐标系中表达物体的几何。</p>
<p>2.5D图是基元图通过各种称之为由X到形状的技术导出的。</p>
<p>3D表达非常难以获得，这个步骤还没有在一般情况下得到解决。</p>
<p>新的感知范畴，比如主动的、有目的的、定性的视觉，试图为解释视觉的“理解”方面提供计算模型。</p>
<p>还没有直接带来实际应用，但是很多部分技术（比如，从X到形状）被广泛应用到实践中。</p>
<h2> 5.3 3D视觉及其几何</h2>
<p>3D透视几何是3D视觉的基本数学工具，正如它在解释针孔相机中那样。</p>
<p>在3D世界中的平行线在2D图像中的投影并非是平行的。</p>
<p>在单透视相机的情况，可以做有关内外相机参数标定的仔细研究。</p>
<p>极限几何告诉我们对应点的搜索是内在的一维的。这可以用基本举证表达为代数形式。</p>
<p>这个工具有几个应用，包括图像矫正、从标定后相机测量进行子运动估计、从两个完全标定好的相机做3D欧式重构、从两个只做了内参标定的相机做3D相似重构、从两个未标定的相机做3D射影重构。</p>
<p>从三个相机的视图间存在三线性关系，这在代数上用三焦距张量表达。</p>
<p>三线性张量的应用是极线迁移；如果已知两个图像，还有三焦距张量，第三个透视图像可以计算出来。</p>
<p>对应问题是3D视觉的核心；存在各种被动的和主动的求解技术。</p>
<h2> 5.4 辐射学和3D视觉</h2>
<p>辐射学告诉我们图像形成的物理机制。</p>
<p>若已知光源的未知，类型、表面方向和观察者位置，就可以从一幅亮度图像得到某种有关深度和表面方向的信息。</p>
<p>这个任务被称为由阴影到形状。</p>
<p>该任务是不明确的和数值上不稳定的。由阴影到形状可以在lambertian表面的简单情况下理解。</p>
<p>有一个实际的方法，它使用一个相机和三个已知光源，选择性照明提供了三个亮度图像。</p>
<p>光度测量立体视觉可以测量表面方向。</p>
<h2> 5.5 3D视觉应用</h2>
<p>1、由X到形状</p>
<p>形状可以由运动、光流、纹理、聚焦/散聚、会聚、轮廓抽取出来。</p>
<p>这些技术中的每一个都可以用于到处Marr视觉理论中的2.5D图，它们自身具有实用价值。</p>
<p>2、完全的3D物体</p>
<p>对于重建具有平的面的物体而言，线条标注是一个过时但是容易接触到的技术。</p>
<p>转换为3D物体需要以物体为重心的坐标系。</p>
<p>3D物体可以机械地测量或通过X线断层摄影术量测。</p>
<p>体建模策略包括构造立体几何、超二次曲面和广义圆柱。</p>
<p>表面建模策略包括：边界表达、三角剖分表面和二次曲面面偏。</p>
<p>3、基于3D模型的视觉</p>
<p>为了从一组距离图像中创建完整的3D模型，必须首先标记测量得到的表面，即应该找到使一个表面与另一个相匹配的旋转和平移。</p>
<p>基于模型的视觉使用有关物体的先验知识来简化识别。</p>
<p>Goad算法是在单幅亮度图像中搜索多面体的方法。</p>
<p>存在从距离图像中确定曲面物体的技术。</p>
<p>4、基于2D视图的3D场景表达</p>
<p>基于2D视图和3D场景表达可以用多视图表达或geons获得。</p>
<p>选择存储一些参考图像再从它们绘制任意视图是肯恩给定。</p>
<p>视图内插并不足够，还需要视图外推。这需要知道几何信息，基于视图的方法与3D几何重建相差并不明显。</p>
<p>从2D无组织视图集合进行3D重建是可能的。该方法最近经常被使用，比如，谷歌街景。</p>
<p>5、重建场景集合</p>
<p>大尺度场景特征比如平面参数可以从直线和近似尺寸等已知物体的特性得到。</p>
<p>众所周知的集合结果可以得到小食店和地面方向。</p>
<p>尽管大尺度的线索不能得到，相似方法也能很好地工作。</p>
<h2> 5.6 重要概念</h2>
<p>单应性：也认为是共线或投影变换。</p>
<aside> 🪁 极限几何学——基本矩阵 反映【**空间一点P的像素点】**在【**不同视角摄像机】**下【**图像坐标系**】中的表示之间的关系。 设$u$， $^CP_a$是场景点X在a,b相机的坐标系，那么存在$u'^{T} F u=0$,其中$F$被称为基本矩阵。
</aside>
<aside> 🪁 摄像机的相对运动——本质矩阵 反映【**空间一点P的像点】**在【**不同视角摄像机】**下【**摄像机坐标系】**中的表示之间的关系。 设 $^CP_a$， $^CP_a$是场景点X在a,b相机的坐标系，那么存在 $^CP_a^{T} E ^CP_a=0$,其中$F$被称为基本矩阵。
</aside>
<h1> 六、形态学处理</h1>
<h2> 6.1 形态学目的</h2>
<p>图像预处理（去噪声、简化形状）</p>
<p>增强物体结构（抽取骨骼、细化、粗化、凸包）</p>
<p>从背景中分割图像</p>
<p>物体量化描述（面积，周长，阴影）</p>
<h2> 6.2 形态学四原则</h2>
<p>平移相容、尺度缩放相容、局部知识、上部半连通</p>
<h2> 6.3 数学形态学</h2>
<aside> 🪁 数学形态学强调形状在图像预处理、分割和物体描述中的作用。它产生了具有数学上合理的快速算法。
</aside>
<aside> 🪁 基本实体是点集。形态学中的变换由来自一个较简单的非线性代数中的运算符描述。数学形态学是传统意义上基于线性运算符的信号处理方法的一个对应。
</aside>
<aside> 🪁 数学形态学通常分为处理二值图像的二值数学形态学，和处理灰度图像的灰度级数学形态学。
</aside>
<h2> 6.4 形态学运算</h2>
<aside> 🪁 形态学运算是一个两个集合间的关系。其中一个集合是图像，另一个则是小探测器，称为结构元素，结构元素系统地扫描整幅图像；在图像的每个位置上的关系都被记录下来，作为输出图像。
</aside>
<aside> 🪁 形态学的基本运算是使物体扩张的膨胀和使物体缩小的腐蚀。腐蚀和膨胀不是可逆运算；它们的组合构成了新的运算，开运算和闭运算。
</aside>
<aside> 🪁 细长的物体通常简化为一个股价——一条位于“物体中轴”的线。
</aside>
<aside> 🪁 到背景的距离函数是许多快速形态学运算的基础。最终腐蚀经常用来标记斑点的中心。一种高效的重构算法从标记开始生长物体，知道其原始边界。
</aside>
<blockquote>
<p>测地变换允许在处理过程中变换元素，这就使算法更加灵活。</p>
</blockquote>
<aside> 🪁 测地方法为图像分割提供了快速高效的算法。分水岭变换是最好的分割方法之一。目标区域的边界是影响区域的区域极小值（即地形中的海或湖）。区域边界是这些海或湖的分水线，
</aside>
<blockquote>
<p>分割通常从交互选择的标记或者通过利用图像语义性质的某种自动化过程开始。</p>
</blockquote>
<blockquote>
<p>粒度测定法是一种分析图像中不同大小例子分布情况的量化工具（类似于筛分法）。得到的结果是一条离散的粒度测定取下（谱）</p>
</blockquote>
<h1> 七、运动分析</h1>
<h3> 7.1 运动分析概述</h3>
<aside> 🪁 运动分析主要处理三种类型的运动相关问题：运动检测、移动物体检测和定位、三维物体性质的推导。
</aside>
<aside> 🪁 我们通常称三维运动的二维表示为运动场，其中每个点被赋予一个速度向量，它对应于运动方向、速率以及离开在适当图像上的观察者的距离。
</aside>
<aside> 🪁 光流是运动场的一种构造方法，其中要确定的可能是在图像所有点上的运动方向和运动速度。
</aside>
<aside> 🪁 特征点的对对应关系是运动场构造的另一种方法。只确定对应特征点上的速度向量。
</aside>
<aside> 🪁 物体运动参数可以从计算出的运动场向量中得到。
</aside>
<blockquote>
<p>运动假设能够帮助定位移动物体，经常使用的假设包括：最大速度、小加速度、共同运动、相互对应关系</p>
</blockquote>
<h3> 7.2 差分运动分析</h3>
<aside> 🪁 假设在固定的相机位置和在恒定的光照条件下，将在不同时刻上获取的图像相减，就可以检测出运动。
</aside>
<aside> 🪁 存在许多于这个方法相关联的问题，相减的结果高度依赖于物体—背景的对比度。
</aside>
<aside> 🪁 累计差分图像提高了差分运动分析的性能。他提供了众多信息，包括有关于运动方向和其它与时间相关的运动特性以及缓慢运动和小的物体运动的信息。
</aside>
<aside> 🪁 检测移动边缘有助于进一克服差分运动分析方法的局限。通过结合空间和时间图像梯度，差分分析能够可靠的用于检测缓慢移运动和小的物体运动的信息。
</aside>
<h3> 7.3 光流</h3>
<aside> 🪁 光流反映了在时间间隔dt内由于运动造成的图像变化，其中时间间隔dt必须满足足够短以保证小的帧间运动变化。
</aside>
<aside> 🪁 光流场是一个速度场，它表示物体点的三维运动在二维图像上的表现。
</aside>
<aside> 🪁 光流计算基于两个假设：
</aside>
<blockquote>
<p>所观测到的任务物体点的亮度不随时间变化</p>
</blockquote>
<blockquote>
<p>图像平面中的近邻点以相类似的方式移动（速度平滑性约束）</p>
</blockquote>
<aside> 🪁 如果违背恒定亮度和速度平滑性约束假设，将会出现光流计算错误。在实际图像中，这种违背是很常见的。典型的，光流在高纹理区域、移动边界周围、深度不连续处等位置会发生剧烈的变化。所以导致的误差会在整个光流解上传播。
</aside>
<aside> 🪁 全局误差传播是全局光流计算方案的最大问题，局部光流估计帮助克服这些困难。
</aside>
<aside> 🪁 光流分析并不以运动轨迹为结果；代替的是检测出更一般性的运动特性，这样可以显著提高复杂运动分析的可靠性，检测出的参数包括：相互物体速度、延伸焦点（FOE）确定、距离（深度）确定、碰撞检测。
</aside>
<h3> 7.4 基于兴趣点对应关系的运动分析</h3>
<aside> 🪁 这个方法在所有的序列图像中寻找显著点（兴趣点，特征点）——这些点与它们的周围环境最不相似，代表了物体的角点、边界或图像中任何其他典型的特征，对它们可以随着时间进行跟踪。
</aside>
<aside> 🪁 Lucas-Kanade 跟踪器广泛使用：假设局部快移动类似，它通过求解一个线性系统来进行鲁棒性跟踪。
</aside>
<aside> 🪁 KLT跟踪器使用Lucas-Kanade方法来自动获得在图像【视频】中稳定的点。
</aside>
<aside> 🪁 点检测后面跟着匹配过程，它寻找这些点之间的对应关系。
</aside>
<aside> 🪁 这个处理的结果是产生一个稀疏的数据场
</aside>
<aside> 🪁 基于对应关系的运动检测甚至可以用于具有相对较长的帧间时间间隔的情况。
</aside>
<h3> 7.5 特定运动模式的检测</h3>
<aside> 🪁 特定运动信息可以从训练集合的实例中推导出。可以作到区分不同形式的运动和其他现象。
</aside>
<aside> 🪁 同时使用基于图像和基于短期运动的信息。
</aside>
<aside> 🪁 运动检测使用工作与任意尺度的简单矩形滤波器的小集合；通过检测在时序上对应图像块的差异来检测运动。滤波器的小集合是使用AdaBoost方法从一个大的滤波器集合中选择出来。
</aside>
<h3> 7.6 视频跟踪</h3>
<aside> 🪁 背景建模： 视频跟踪一般基于当前帧于背景模型之间的某种差分。 简单的方法容易受噪声和微弱背景变化的影响。 更为鲁棒性的方法有很多，其中最为主要的是为每个像素建立一个混合多高斯背景模型。通过一些启发式的近似来对参数进行实时更新。
</aside>
<aside> 🪁 基于核函数的跟踪： 基于梯度的物体定位和跟踪可以通过使用一个通用相关准则来完成。 基于核函数的跟踪方法非常高效，能够实时进行跟踪。 这种方法使用一个各向同性核函数在空间上给目标施加一个掩膜，然后在其上应用一个平滑相似度函数来将跟踪问题转化为目标前一时刻邻域范围内的最大相似度搜索。 相似度的优化过程使用的是均值译为算法。
</aside>
<aside> 🪁 目标轨迹分析： 如果跟踪几个独立的目标，解决的方法常常依赖于运动约束和最小路径一致性函数，这个一致性函数是对导出的目标轨迹和运动约束的一致性度量。
</aside>
<h3> 7.7 运动模型</h3>
<aside> 🪁 预测其—修正器机制可以用于在存在观测噪声情况下的物体运动估计，然后再对估计进行修正。
</aside>
<aside> 🪁 卡尔曼滤波： 卡尔曼滤波器是一种常用的动态估计方法，代表了一类用于运动分析的有力工具。 卡尔曼滤波器要求系统是线性的，同时要求系统观测是隐状态的线性方程。噪声，包括系统的和观测的，都假设是高斯白噪声。
</aside>
<aside> 🪁 粒子滤波： 卡尔曼滤波被广泛应用，但是有一些局限假设。粒子滤波器克服了大多数假设。 粒子滤波器基于每步时间上的统计采样方法；基于图像观测对采样进行调整。 粒子滤波器在视觉中常见的实现是CONDENSATION(condensation)。
</aside>
<h3> 7.8 跟踪学习检测——TLD</h3>
<aside> 🪁 绝大多数跟踪器受限于姿态变化、遮挡等。 很多跟踪器都包含了一个检测器来在图像中（重新）发现物体的表现。 TLD同时进行跟踪和检测，并且也动态地更新它学习的物体模型。 它在一定尺度范围上操作，并且常常基于Lucas-Kanade跟踪器。 在跟踪过程中，模型吸收新的表现，同时删减掉哪些被认为不可能的表现。
</aside>
<h1> 八、图像压缩</h1>
<h2> 8.1 图像数据压缩</h2>
<aside> 🪁 图像压缩的主要目的是在没有明显信息损失的情况下，尽量减少图像数据量。 数据压缩算法试图在保证能够重建图像的前提下去除数据冗余（数据相关性）；这叫做信息保留压缩。 典型的图像压缩/解压缩 过程包括了：去除数据冗余、编码、传输、解码和重建。 数据压缩的方法可以分为两类： 信息保留——lossless（无损）压缩保证无措的数据重建。 信息有损的压缩方法不能保证信息的完整性。
</aside>
<h2> 8.2 图像数据性质</h2>
<aside> 🪁 图像的平均信息量是一个很重要的性质，可以用熵来衡量。 已知图像熵后，就可以确定信息冗余。
</aside>
<h2> 8.3 图像数据压缩中的离散图像变换</h2>
<aside> 🪁 图像数据可以用离散图像变换的系数来表示。变换系数按照它们的重要程度，例如对于图像信息量的贡献进行排序，贡献小的系数被忽略。 Kahunen-Loeve 变换对于去除相关的（冗余）图像数据最有效。 余弦、傅里叶、哈达玛、沃尔什以及二值变换都可以用于图像数据的压缩。 离散余弦变换DCT-Ⅱ的性能比其它变换更接近于Kahunen-Loeve变换。DCT通常用于小的图像块（典型的是8x8像素），当压缩率高时，会产生降低质量的块效应。
</aside>
<h2> 8.4 预测压缩方法</h2>
<aside> 🪁 对于一个图像元素，预测压缩使用图像的信息冗余，通过它周围的灰度值对它本身做出估计。 我们可以期望估计值和真实值之间的差别的绝对值相对比较小，而将这个差别和预测模型参数一起编码和传送。
</aside>
<h2> 8.5 矢量量化</h2>
<aside> 🪁 矢量量化压缩技术基于将图像分成小块，并用矢量表示它们。 输入数据矢量使用码字字典中的唯一码字进行编码；存储和传输的是矢量编码，而不是矢量。 码字字典（码本）和编码后的数据一同传输。
</aside>
<h2> 8.6 分层和渐进的压缩方法</h2>
<aside> 🪁 通过金字塔表示一个数据源，就可以减少比特量。当图像具有大范围相同灰度区域时，使用四叉树可以获得显著的数据量缩减。 分层压缩使得渐进和只能图像传输变得更容易。 渐进图像传输基于这样一个事实：某些情况下不必传输所有的图像数据。 只能图像传输基于人类视觉感知器的感觉性质——在人眼不注意的图像区域没有必要以全分辨率来显示。
</aside>
<h2> 8.7 压缩方法比较</h2>
<aside> 🪁 以变换为基础的方法可以更好的保持主观图像质量，对于图像内和图像间的统计性质变化的敏感性较低。 预测方法可以以小得多的代价取得高压缩率，而且速度比基于变换和矢量量化的压缩机制要快得多。 矢量量化方法需要一个复杂的编码器，它们的参数罪域图像数据十分敏感，并且会模糊图像边缘，它的有点在于解码机制较简单，可以只包含一个查找表。
</aside>
<h2> 8.8 其它技术</h2>
<aside> 🪁 分形图像压缩技术提供特别高的压缩率和高质量的图像重建。它的主要原理是将图像分为碎片（分形），并认为自相似的哪些是一样的。分形图像可以无限放大，因此分形压缩与分辨率无关，一个压缩图像可以用于以任意分辨率的显示。
</aside>
<h2> 8.9 编码</h2>
<aside> 🪁 哈夫曼编码可以提供理想的压缩和无措的解压缩。哈夫曼编码的主要思想是用变长编码表示数据，较短的码字表示出现频率较高的数据。
</aside>
<h2> 8.10 JPEG和MPEG 图像压缩</h2>
<aside> 🪁 JPEG和JPEG-2000代表了图像压缩数据的国际标准。 JPEG图像压缩为通用的彩色静态图像压缩制定了一个标准。这个标准在很多应用领域得到了广泛的应用。JPEG压缩模式有四种： 基于DCT的顺序预测、基于DCT的渐进压缩、无损顺序预测压缩、有损和无损分层压缩。
</aside>
<aside> 🪁 JPEG-2000 是为了克服JPEG的局限性而设计的，尽管名称相似，但是不同于JPEG。 JPEG-2000 基于小波变换并提供了大量灵活的关于质量、分辨率和空间伸缩性的新功能。 JPEG-2000 通常情况下对于需要高质量图片重建和低比特流压缩的应用胜过JPEG压缩。 MPEG标准为全运动的视频图像序列而制定的。 现在常提及的三种标准： 用于压缩低分辨率全运动视频的MPEG-1。 用于高分辨率标注标准的MPEG-2。 用于压缩刷新要求不高的小帧全运动的MPEG-4。
</aside>]]></content:encoded>
    </item>
    <item>
      <title>计算机视觉岗位要求</title>
      <link>https://tobeprozy.github.io/03Computer_Vision/%E5%B2%97%E4%BD%8D%E8%A6%81%E6%B1%82.html</link>
      <guid>https://tobeprozy.github.io/03Computer_Vision/%E5%B2%97%E4%BD%8D%E8%A6%81%E6%B1%82.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">计算机视觉岗位要求</source>
      <description>提示 求职 💡 1、 基础的图像去噪、纹理增强、对比度控制、颜色管理； 2、负责双摄和多摄的成像算法设计，包括相机立体标定、深度计算、背景虚化、图像融合等； 3、负责图像处理和计算机视觉方向的算法研发，包括但不限于相机标定、图像拼接、图像复原、图像增强、生物识别、图像或视频的分类、检测、跟踪、分割等技术领域； 4、具有相机成像原理、数字图像处理、计算机视觉、图形学、机器学习等基础知识</description>
      <category>计算机视觉</category>
      <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container tip">
<p class="hint-container-title">提示</p>
<p>求职</p>
</div>
<p>💡
1、 基础的图像去噪、纹理增强、对比度控制、颜色管理；</p>
<p>2、负责双摄和多摄的成像算法设计，包括相机立体标定、深度计算、背景虚化、图像融合等；</p>
<p>3、负责图像处理和计算机视觉方向的算法研发，包括但不限于相机标定、图像拼接、图像复原、图像增强、生物识别、图像或视频的分类、检测、跟踪、分割等技术领域；</p>
<p>4、具有相机成像原理、数字图像处理、计算机视觉、图形学、机器学习等基础知识</p>
<p>5、图像增强、多目测量、投影几何、匹配和拼接、去模糊、去雾和超分等项目经验；</p>
]]></content:encoded>
    </item>
    <item>
      <title>优化方法概述</title>
      <link>https://tobeprozy.github.io/04Optimization_Algorithm/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0.html</link>
      <guid>https://tobeprozy.github.io/04Optimization_Algorithm/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">优化方法概述</source>
      <description>提示 优化方法概述 **作用：**最优化就是用来解决复杂函数的极值问题 主要方法 1、一阶方法：梯度下降法、最速下降法 2、二阶方法：牛顿法、拟牛顿法（构造一个近似的黑赛尔矩阵用于牛顿迭代）、高斯牛顿法（高斯—牛顿法不需要求解Hessian二阶信息）</description>
      <category>计算机视觉</category>
      <pubDate>Fri, 14 Apr 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container tip">
<p class="hint-container-title">提示</p>
<p>优化方法概述</p>
</div>
<p>**作用：**最优化就是用来解决复杂函数的极值问题</p>
<h3> 主要方法</h3>
<p>1、一阶方法：梯度下降法、最速下降法</p>
<p>2、二阶方法：牛顿法、拟牛顿法（构造一个近似的黑赛尔矩阵用于牛顿迭代）、高斯牛顿法（高斯—牛顿法不需要求解Hessian二阶信息）</p>
]]></content:encoded>
    </item>
    <item>
      <title>SLAM概述</title>
      <link>https://tobeprozy.github.io/06SLAM/SLAM%E6%A6%82%E8%BF%B0.html</link>
      <guid>https://tobeprozy.github.io/06SLAM/SLAM%E6%A6%82%E8%BF%B0.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">SLAM概述</source>
      <description>提示 SLAM概述 经典SLAM：视觉里程计、后端优化、回环检测、建图 1、三维刚体运动：主要内容就是坐标 理解左乘是基于自身坐标系，右乘是基于基坐标系 3、相机和图像：针孔相机模型、3D视觉</description>
      <category>SLAM</category>
      <pubDate>Fri, 14 Apr 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container tip">
<p class="hint-container-title">提示</p>
<p>SLAM概述</p>
</div>
<h3> 经典SLAM：<strong>视觉里程计、后端优化、回环检测、建图</strong></h3>
<h3> 1、三维刚体运动：主要内容就是坐标</h3>
<p>理解左乘是基于自身坐标系，右乘是基于基坐标系</p>
<h3> 3、相机和图像：针孔相机模型、3D视觉</h3>
<figure><figcaption></figcaption></figure>
<figure><figcaption></figcaption></figure>
<h3> 2、李群代数没有具体了解过</h3>
<h3> 4、非线性优化</h3>
<p>状态估计问题：</p>
<figure><figcaption></figcaption></figure>
<p>曲线拟合：使用Ceres、g2o曲线拟合</p>
<p>非线性最小二乘：一阶和二阶梯度算法、高斯牛顿法、列文伯格法</p>
<figure><figcaption></figcaption></figure>
<figure><figcaption></figcaption></figure>
<h3> 5、视觉里程计</h3>
<h3> 6、多视图几何</h3>
<p>记住几个基本概念：</p>
<figure><figcaption></figcaption></figure>
<p><strong>空间一点P</strong>在左右两个相机成像平面的对应点分别为$P_L和P_R$</p>
<p>************<strong>极平面：空间点P与左右相机光心$O_L和O_R$构成的平面</strong></p>
<p><strong>极线：<strong>极平面</strong>与左右相平面构成的交线</strong>叫做<strong>左右极线</strong></p>
<p>极点：左右光心<strong>与相平面的交点</strong>叫做<strong>左右极点</strong></p>
<p>极线约束：空间点P在左相机的投影点为$P_L$,将可能在右相机上存在无数个点与它对应，但是这些点一定都在所对应的<strong>右极线上</strong></p>
<p><strong>对极约束的几何含义是$P、O_L、O_R$三点共面</strong></p>
<figure><figcaption></figcaption></figure>
<p>存在如下所示的<strong>对极约束</strong>，其中<strong>x1,x2</strong>是<strong>归一化平面上p1,p2</strong>对应的坐标点：</p>
<p>$x_2<sup>TEx_1=p_2</sup>TFp_1=0$
</p>
<p><strong>本质矩阵：</strong>$E$,是反映【**空间一点P的像点】**在【**不同视角摄像机】**下【<strong>相机坐标系】中</strong>的位置关系。</p>
<figure><figcaption></figcaption></figure>
<p><strong>基础矩阵：</strong>$F$，反映【**空间一点P的像素点】**在【<strong>不同视角摄像机】<strong>下【</strong><a href="https://www.zhihu.com/search?q=%E5%9B%BE%E5%83%8F%E5%9D%90%E6%A0%87%E7%B3%BB&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A105606049%7D" target="_blank" rel="noopener noreferrer">图像坐标系</a></strong>】中的位置关系。</p>
<figure><figcaption></figcaption></figure>
<p>**单应性矩阵：**平面的单应性被定义为一个平面到另外一个平面的投影映射。单应性矩阵不只是描述同一平面的像素点之间的关系，而是同一个平面在任意坐标系之间都可以建立单应性变换关系，比如影像坐标系与影像坐标系之间，世界坐标系和影像坐标系之间，如下图所示。
</p>
]]></content:encoded>
    </item>
    <item>
      <title>深度学习常用框架</title>
      <link>https://tobeprozy.github.io/08AI_Machine_Learning_Deep_Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6.html</link>
      <guid>https://tobeprozy.github.io/08AI_Machine_Learning_Deep_Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">深度学习常用框架</source>
      <description>提示 深度学习常用框架 深度学习常用框架 深度学习框架的出现降低了入门的门槛，你不需要从复杂的神经网络开始编代码，你可以根据需要选择已有的模型，通过训练得到模型参数，你也可以在已有模型的基础上增加自己的layer，或者是在顶端选择自己需要的分类器和优化算法（比如常用的梯度下降法）。 总的来说深度学习框架提供了一些列的深度学习的组件（对于通用的算法，里面会有实现），当需要使用新的算法的时候就需要用户自己去定义，然后调用深度学习框架的函数接口使用用户自定义的新算法。</description>
      <category>人工智能</category>
      <pubDate>Fri, 14 Apr 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container tip">
<p class="hint-container-title">提示</p>
<p>深度学习常用框架</p>
</div>
<h1> 深度学习常用框架</h1>
<p>深度学习框架的出现降低了入门的门槛，你不需要从复杂的神经网络开始编代码，你可以根据需要选择已有的模型，通过训练得到模型参数，你也可以在已有模型的基础上增加自己的layer，或者是在顶端选择自己需要的分类器和优化算法（比如常用的梯度下降法）。</p>
<blockquote>
<p>总的来说深度学习框架提供了一些列的深度学习的组件（对于通用的算法，里面会有实现），当需要使用新的算法的时候就需要用户自己去定义，然后调用深度学习框架的函数接口使用用户自定义的新算法。</p>
</blockquote>
<table>
<thead>
<tr>
<th>名称</th>
<th>来源</th>
<th>概述</th>
<th>链接</th>
</tr>
</thead>
<tbody>
<tr>
<td>TensorFlow</td>
<td>Google开源的深度学习框架</td>
<td>是一款使用C++语言开发的开源数学计算软件，使用数据流图(Data Flow Graph)的形式进行计算。图中的节点代表数学运算，而图中的线条表示多维数据数组(tensor)之间的交互。</td>
<td><a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer">https://www.tensorflow.org/</a></td>
</tr>
<tr>
<td>PyTorch</td>
<td>Facebook开源的深度学习框架</td>
<td>是一个基于Python的科学计算包，主要用于两个领域：一个是替代NumPy来使用GPU，另一个是深度学习领域的自动微分机制。</td>
<td><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">https://pytorch.org/</a></td>
</tr>
<tr>
<td>Keras</td>
<td>由Google开发的深度学习框架</td>
<td>是一个高层神经网络API，由Python编写，能够以TensorFlow、CNTK或Theano为后端运行。</td>
<td><a href="https://keras.io/" target="_blank" rel="noopener noreferrer">https://keras.io/</a></td>
</tr>
<tr>
<td>Caffe</td>
<td>由Berkeley AI Research开发的深度学习框架</td>
<td>是一个深度学习框架，由C++编写，支持多种深度学习框架，如Caffe、TensorFlow、CNTK、Torch、MXNet等。</td>
<td><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener noreferrer">http://caffe.berkeleyvision.org/</a></td>
</tr>
<tr>
<td>MXNet</td>
<td>由Amazon开发的深度学习框架</td>
<td>是一个深度学习框架，由C++编写，支持多种深度学习框架，如Caffe、TensorFlow、CNTK、Torch、MXNet等。</td>
<td><a href="https://mxnet.apache.org/" target="_blank" rel="noopener noreferrer">https://mxnet.apache.org/</a></td>
</tr>
<tr>
<td>Theano</td>
<td>由MILA开发的深度学习框架</td>
<td>是一个Python库，用于数值计算，特别是用于深度学习。它可以在CPU和GPU上运行，支持动态计算图。</td>
<td><a href="http://deeplearning.net/software/theano/" target="_blank" rel="noopener noreferrer">http://deeplearning.net/software/theano/</a></td>
</tr>
<tr>
<td>CNTK</td>
<td>由微软开发的深度学习框架</td>
<td>是一个深度学习框架，由C++编写，支持多种深度学习框架，如Caffe、TensorFlow、CNTK、Torch、MXNet等。</td>
<td><a href="https://www.microsoft.com/en-us/cognitive-toolkit/" target="_blank" rel="noopener noreferrer">https://www.microsoft.com/en-us/cognitive-toolkit/</a></td>
</tr>
<tr>
<td>MXNet</td>
<td>由Amazon开发的深度学习框架</td>
<td>是一个深度学习框架，由C++编写，支持多种深度学习框架，如Caffe、TensorFlow、CNTK、Torch、MXNet等。</td>
<td><a href="https://mxnet.apache.org/" target="_blank" rel="noopener noreferrer">https://mxnet.apache.org/</a></td>
</tr>
<tr>
<td>Chainer</td>
<td>由Preferred Networks开发的深度学习框架</td>
<td>是一个深度学习框架，由Python编写，支持多种深度学习框架，如Caffe、TensorFlow、CNTK、Torch、MXNet等。</td>
<td><a href="https://chainer.org/" target="_blank" rel="noopener noreferrer">https://chainer.org/</a></td>
</tr>
<tr>
<td>PaddlePaddle</td>
<td>百度开发的深度学习框架</td>
<td>是一个深度学习框架，由Python编写，支持多种深度学习框架，如Caffe、TensorFlow、CNTK、Torch、MXNet等。</td>
<td><a href="https://www.paddlepaddle.org.cn/" target="_blank" rel="noopener noreferrer">https://www.paddlepaddle.org.cn/</a></td>
</tr>
<tr>
<td>Torch</td>
<td>由Facebook开发的深度学习框架</td>
<td>是一个深度学习框架，由C++编写，支持多种深度学习框架，如Caffe、TensorFlow、CNTK、Torch、MXNet等。</td>
<td><a href="http://torch.ch/" target="_blank" rel="noopener noreferrer">http://torch.ch/</a></td>
</tr>
</tbody>
</table>
]]></content:encoded>
    </item>
    <item>
      <title>自动驾驶概述</title>
      <link>https://tobeprozy.github.io/11Automatic_Driving/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%A1%86%E6%9E%B6.html</link>
      <guid>https://tobeprozy.github.io/11Automatic_Driving/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%A1%86%E6%9E%B6.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">自动驾驶概述</source>
      <description>自动驾驶概述 全自动驾驶资源大合集</description>
      <pubDate>Wed, 26 Apr 2023 10:25:41 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 自动驾驶概述</h1>
<figure><figcaption></figcaption></figure>
<h3> <a href="https://blog.csdn.net/weixin_47196664/article/details/106866502?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164941622316781685393295%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164941622316781685393295&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-8-106866502.142%5Ev7%5Epc_search_result_cache,157%5Ev4%5Econtrol&amp;utm_term=%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%AE%97%E6%B3%95&amp;spm=1018.2226.3001.4187" target="_blank" rel="noopener noreferrer">全自动驾驶资源大合集</a></h3>
<h1> 经典论文</h1>
<p>1.Self-Driving Cars: A Survey</p>
<p>链接地址：<a href="https://arxiv.org/abs/1901.04407v1" target="_blank" rel="noopener noreferrer"></a><a href="https://arxiv.org/abs/1901.04407v1" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1901.04407v1</a></p>
<p>2.Towards Fully Autonomous Driving: Systems and Algorithms</p>
<p>链接地址：<a href="https://www.ri.cmu.edu/wp-content/uploads/2017/12/levinson-iv2011.pdf" target="_blank" rel="noopener noreferrer"></a><a href="https://www.ri.cmu.edu/wp-content/uploads/2017/12/levinson-iv2011.pdf" target="_blank" rel="noopener noreferrer">https://www.ri.cmu.edu/wp-content/uploads/2017/12/levinson-iv2011.pdf</a></p>
<p>3.A Survey of Autonomous Driving: Common Practices and Emerging Technologies</p>
<p>链接地址：<a href="https://arxiv.org/abs/1906.05113" target="_blank" rel="noopener noreferrer"></a><a href="https://arxiv.org/abs/1906.05113" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1906.05113</a></p>
<p>4.A Survey of Deep Learning Techniques for Autonomous Driving</p>
<p>链接地址：<a href="https://arxiv.org/abs/1910.07738" target="_blank" rel="noopener noreferrer"></a><a href="https://arxiv.org/abs/1910.07738" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1910.07738</a></p>
<p>5.Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art</p>
<p>链接地址：<a href="https://arxiv.org/abs/1704.05519" target="_blank" rel="noopener noreferrer"></a><a href="https://arxiv.org/abs/1704.05519" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1704.05519</a></p>
<p>6.Simultaneous localization and mapping: A survey of current trends in autonomous driving</p>
<p>链接地址：<a href="https://ieeexplore.ieee.org/document/8025618" target="_blank" rel="noopener noreferrer"></a><a href="https://ieeexplore.ieee.org/document/8025618" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/document/8025618</a></p>
<p>7.Robust and Precise Vehicle Localization based on Multi-sensor Fusion in Diverse City Scenes</p>
<p>链接地址：<a href="https://arxiv.org/abs/1711.05805" target="_blank" rel="noopener noreferrer"></a><a href="https://arxiv.org/abs/1711.05805" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1711.05805</a></p>
<p>8.A Review of Tracking, Prediction and Decision Making Methods for Autonomous Driving</p>
<p>链接地址：<a href="https://arxiv.org/pdf/1909.07707.pdf" target="_blank" rel="noopener noreferrer"></a><a href="https://arxiv.org/pdf/1909.07707.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1909.07707.pdf</a></p>
<p>9.自主车辆的计算机视觉：问题，数据集和最新技术</p>
<p>论文链接：<a href="https://arxiv.org/pdf/1704.05519.pdf" target="_blank" rel="noopener noreferrer"></a><a href="https://arxiv.org/pdf/1704.05519.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1704.05519.pdf</a></p>
<p>10.用于自动驾驶汽车的机器学习算法</p>
<h1> 开源平台</h1>
<ol>
<li>Apollo: <a href="https://github.com/ApolloAuto/apollo" target="_blank" rel="noopener noreferrer"></a><a href="https://github.com/ApolloAuto/apollo" target="_blank" rel="noopener noreferrer">https://github.com/ApolloAuto/apollo</a></li>
<li>Autoware 2.0：名古屋大学的自动驾驶项目</li>
</ol>
<p>链接地址：<a href="https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto" target="_blank" rel="noopener noreferrer"></a><a href="https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto" target="_blank" rel="noopener noreferrer">https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto</a></p>
<ol>
<li>Carla: <a href="http://carla.org/" target="_blank" rel="noopener noreferrer"></a><a href="http://carla.org/" target="_blank" rel="noopener noreferrer">http://carla.org/</a></li>
<li>Baidu: <a href="http://apollo.auto/platform/simulation.html" target="_blank" rel="noopener noreferrer"></a><a href="http://apollo.auto/platform/simulation.html" target="_blank" rel="noopener noreferrer">http://apollo.auto/platform/simulation.html</a></li>
<li>Udacity- 优达学城的自动驾驶仿真平台</li>
</ol>
<p>链接地址：<a href="https://github.com/udacity/self-driving-car-sim" target="_blank" rel="noopener noreferrer"></a><a href="https://github.com/udacity/self-driving-car-sim" target="_blank" rel="noopener noreferrer">https://github.com/udacity/self-driving-car-sim</a></p>
<ol>
<li>AirSim- 微软的仿真平台，还可以用于无人机</li>
</ol>
<p>链接地址：<a href="https://github.com/Microsoft/AirSim" target="_blank" rel="noopener noreferrer"></a><a href="https://github.com/Microsoft/AirSim" target="_blank" rel="noopener noreferrer">https://github.com/Microsoft/AirSim</a></p>
<ol>
<li>lgsvl- LG的自动驾驶仿真平台：<a href="https://www.lgsvlsimulator.com/" target="_blank" rel="noopener noreferrer"></a><a href="https://www.lgsvlsimulator.com/" target="_blank" rel="noopener noreferrer">https://www.lgsvlsimulator.com/</a> </li>
<li>KITTI 是目前最知名的自动驾驶数据集之一，一些创业公司都会拿里面的数据进行排名比赛。</li>
</ol>
<p>链接地址：<a href="http://www.cvlibs.net/datasets/kitti/" target="_blank" rel="noopener noreferrer"></a><a href="http://www.cvlibs.net/datasets/kitti/" target="_blank" rel="noopener noreferrer">http://www.cvlibs.net/datasets/kitti/</a></p>
<ol>
<li>Cityscapes 目标是理解街景的语义，主要是针对城市街景做语义解析。</li>
</ol>
<p>链接地址：<a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer"></a><a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener noreferrer">https://www.cityscapes-dataset.com/</a></p>
<ol>
<li>DeepDrive是 Berkeley的大规模自动驾驶视频数据集</li>
</ol>
<p>链接地址：<a href="https://bdd-data.berkeley.edu/" target="_blank" rel="noopener noreferrer"></a><a href="https://bdd-data.berkeley.edu" target="_blank" rel="noopener noreferrer">https://bdd-data.berkeley.edu</a></p>
<ol>
<li>Mapillary 是一个由位于瑞典马尔默的Mapillary AB开发，用来分享含有地理标记照片的服务。其创建者想要利用众包的方式来把整个世界（不仅是街道）以照片的形式存储。</li>
</ol>
<p>链接地址：<a href="https://www.mapillary.com/" target="_blank" rel="noopener noreferrer"></a><a href="https://www.mapillary.com/" target="_blank" rel="noopener noreferrer">https://www.mapillary.com/</a></p>
<ol>
<li><a href="http://comma.ai/" target="_blank" rel="noopener noreferrer">comma.ai</a>'s Driving Dataset 目的是低成本的自动驾驶方案，目前是通过手机改装来做自动驾驶，开源的数据主要是行车记录仪的数据。</li>
</ol>
<p>链接地址：<a href="https://github.com/commaai/research" target="_blank" rel="noopener noreferrer"></a><a href="https://github.com/commaai/research" target="_blank" rel="noopener noreferrer">https://github.com/commaai/research</a></p>
<ol>
<li>Udacity's Driving Dataset 优达学城的自动驾驶数据集</li>
</ol>
<p>链接地址：<a href="https://github.com/udacity/self-driving-car/tree/master/datasets" target="_blank" rel="noopener noreferrer"></a><a href="https://github.com/udacity/self-driving-car/tree/master/datasets" target="_blank" rel="noopener noreferrer">https://github.com/udacity/self-driving-car/tree/master/datasets</a></p>
<ol>
<li>Washington DC's Lidar Data</li>
</ol>
<p>链接地址：<a href="https://aws.amazon.com/cn/blogs/publicsector/lidar-data-for-washington-dc-is-available-as-an-aws-public-dataset/" target="_blank" rel="noopener noreferrer"></a><a href="https://aws.amazon.com/cn/blogs/publicsector/lidar-data-for-washington-dc-is-available-as-an-aws-public-dataset/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/cn/blogs/publicsector/lidar-data-for-washington-dc-is-available-as-an-aws-public-dataset/</a></p>
<ol>
<li>Apolloscape 百度的自动驾驶数据集</li>
</ol>
<p>链接地址：<a href="http://apolloscape.auto/scene.html" target="_blank" rel="noopener noreferrer"></a><a href="http://apolloscape.auto/scene.html" target="_blank" rel="noopener noreferrer">http://apolloscape.auto/scene.html</a></p>
<ol>
<li>Oxford RobotCar 对牛津的一部分连续的道路进行了上百次数据采集，收集到了多种天气、行人和交通情况下的数据，也有建筑和道路施工时的数据。1000小时以上。</li>
</ol>
<p>链接地址：<a href="https://robotcar-dataset.robots.ox.ac.uk/datasets/" target="_blank" rel="noopener noreferrer"></a><a href="https://robotcar-dataset.robots.ox.ac.uk/datasets/" target="_blank" rel="noopener noreferrer">https://robotcar-dataset.robots.ox.ac.uk/datasets/</a></p>
<ol>
<li>nuscenes aptiv提供的数据集，带标注，宣称是目前最大的数据集之一，资源在Amazon S3</li>
</ol>
<p>链接地址：<a href="https://www.nuscenes.org/" target="_blank" rel="noopener noreferrer"></a><a href="https://www.nuscenes.org/" target="_blank" rel="noopener noreferrer">https://www.nuscenes.org/</a></p>
<h1> 交通标志数据集</h1>
<ol>
<li>KUL Belgium Traffic Sign Dataset 比利时的一个交通标志数据集。</li>
</ol>
<p>链接地址：<a href="http://www.vision.ee.ethz.ch/~timofter/traffic_signs/" target="_blank" rel="noopener noreferrer"></a><a href="http://www.vision.ee.ethz.ch/~timofter/traffic_signs/" target="_blank" rel="noopener noreferrer">http://www.vision.ee.ethz.ch/~timofter/traffic_signs/</a></p>
<ol>
<li>German Traffic Sign 德国交通标注数据集 。</li>
</ol>
<p>链接地址：<a href="http://benchmark.ini.rub.de/?p=gtsrb&amp;subp=dataset" target="_blank" rel="noopener noreferrer"></a><a href="http://benchmark.ini.rub.de/?p=gtsrb&amp;subp=dataset" target="_blank" rel="noopener noreferrer">http://benchmark.ini.rub.de/?p=gtsrb&amp;subp=dataset</a></p>
<ol>
<li>STSD 超过20 000张带有20％标签的图像，包含3488个交通标志。</li>
</ol>
<p>链接地址：<a href="https://www.cvl.isy.liu.se/research/datasets/traffic-signs-dataset/" target="_blank" rel="noopener noreferrer"></a><a href="https://www.cvl.isy.liu.se/research/datasets/traffic-signs-dataset/" target="_blank" rel="noopener noreferrer">https://www.cvl.isy.liu.se/research/datasets/traffic-signs-dataset/</a></p>
<ol>
<li>LISA 超过6610帧上的7855条标注。</li>
</ol>
<p>链接地址：<a href="http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html" target="_blank" rel="noopener noreferrer"></a><a href="http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html" target="_blank" rel="noopener noreferrer">http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html</a></p>
<ol>
<li>Tsinghua-Tencent 100K 腾讯和清华合作的数据集，100000张图片，包含30000个交通标志实例。</li>
</ol>
<p>链接地址：<a href="https://cg.cs.tsinghua.edu.cn/traffic-sign/" target="_blank" rel="noopener noreferrer"></a><a href="https://cg.cs.tsinghua.edu.cn/traffic-sign/" target="_blank" rel="noopener noreferrer">https://cg.cs.tsinghua.edu.cn/traffic-sign/</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>自动驾驶概述</title>
      <link>https://tobeprozy.github.io/11Automatic_Driving/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%A6%82%E8%BF%B0.html</link>
      <guid>https://tobeprozy.github.io/11Automatic_Driving/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%A6%82%E8%BF%B0.html</guid>
      <source url="https://tobeprozy.github.io/rss.xml">自动驾驶概述</source>
      <description>提示 自动驾驶概述 1、自动驾驶主要分为感知、定位、决策、控制 2、自动驾驶常见的传感器类型 1）激光雷达的测距精度、测距范围及对温度和光照的适应性都很厉害，缺点就是太贵； 2）相机对环境细节信息的提取能力吊炸天，但是光照影响太大，晚上就瞎了； 3）毫米波最大优点探测角度比较大，抗干扰性强，性能比较稳定，另外也不贵，缺点就是分辨率和精度跟不上。 4）超声波雷达最牛逼的地方是便宜，倒车防撞提醒用的一般是这个，一台车上装十个成本才一百块钱，精度也是最差的，不过低速倒车3m左右足够了。 5）红外优点不是很明显，主要就是晚上比较好用，智驾方案里不算标配，很多传感器方案都没有这个，这里就不过多提了。</description>
      <category>自动驾驶</category>
      <pubDate>Fri, 14 Apr 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<div class="hint-container tip">
<p class="hint-container-title">提示</p>
<p>自动驾驶概述</p>
</div>
<h1> 1、自动驾驶主要分为感知、定位、决策、控制</h1>
<h1> 2、自动驾驶常见的传感器类型</h1>
<figure><figcaption></figcaption></figure>
<p>1）激光雷达的测距精度、测距范围及对温度和光照的适应性都很厉害，缺点就是太贵；</p>
<p>2）相机对环境细节信息的提取能力吊炸天，但是光照影响太大，晚上就瞎了；</p>
<p>3）<a href="https://www.zhihu.com/search?q=%E6%AF%AB%E7%B1%B3%E6%B3%A2&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1851149672%7D" target="_blank" rel="noopener noreferrer">毫米波</a>最大优点探测角度比较大，抗干扰性强，性能比较稳定，另外也不贵，缺点就是分辨率和精度跟不上。
4）<a href="https://www.zhihu.com/search?q=%E8%B6%85%E5%A3%B0%E6%B3%A2%E9%9B%B7%E8%BE%BE&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1851149672%7D" target="_blank" rel="noopener noreferrer">超声波雷达</a>最牛逼的地方是便宜，倒车防撞提醒用的一般是这个，一台车上装十个成本才一百块钱，精度也是最差的，不过低速倒车3m左右足够了。
5）红外优点不是很明显，主要就是晚上比较好用，智驾方案里不算标配，很多传感器方案都没有这个，这里就不过多提了。</p>
<p>车载相机：<strong>信息提取最全</strong>，无论哪个方案，都要用这个；</p>
<p>激光雷达：<strong>精度最高，价格最贵</strong>；</p>
<p>毫米波：性能没激光雷达好，但是<strong>抗干扰能力最强</strong>，价格上实现整车量产没问题；</p>
<p>超声波：<strong>最便宜，感知范围最窄</strong>，但是5m以内测距问题不大，倒车防撞最经典应用。
加速度计和陀螺仪</p>
<ul>
<li>加速度计用来测量运动物体的加速度大小和方向，经过对时间一次积分得到速度，速度再经过一次积分可以得到位移；</li>
<li>陀螺仪用于测量运动体绕各个轴向的旋转角速率值，通过四元数角度结算形成导航坐标系，使加速度计的测量值投影到该坐标系中，并可给出航向和姿态角。
-磁力计用来测量磁场强度和方向，定位运动的方向，通过地磁向量的误差表征量，可以反馈到陀螺仪的姿态解算输出中，校准陀螺仪的漂移。</li>
</ul>
<h1> 3、自动驾驶常见的传感器</h1>
<h2> 3.1相机：<strong>TOF、RGB双目、结构光</strong></h2>
<p>

</p>
<h2> 3.2单目相机（Monocular）、双目相机（Stereo）、深度相机（RGB-D）</h2>
<p>🪁 以一定速率拍摄周围的环境，形成一个连续的视频流。普通相机能以<strong>每秒钟30秒</strong>张图片的速度采集图像，高速相机则更快一些。深度相机（RGB-D）原理较复杂，除了能够采集彩色图像之外，还能读出每个像素离相机的距离。</p>
<h3> 3.2.1单目相机</h3>
<p>**单目<a href="https://so.csdn.net/so/search?q=SLAM&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">SLAM</a>**估计的轨迹和地图，将与真实的轨迹’地图，相差一个因子，也就是所谓的尺度。由于单目SLAM无法仅凭图像确定这个真实尺寸，所以又称为尺度不确定性。&nbsp;本质原因是通过单张图像无法确定深度。</p>
<figure><figcaption></figcaption></figure>
<p>图像坐标系上一点坐标到像素坐标系下一坐标，需要坐标在u轴上缩放α倍，v轴上缩放β倍。</p>
<p>总体的变换过程如下：</p>
<p>$Z_c\begin{bmatrix} u \ v \ 1 \ \end{bmatrix} =\begin{bmatrix} \alpha&amp;0&amp;c_x \ 0&amp;\beta&amp;c_y \0&amp;0&amp;1 \ \end{bmatrix} \begin{bmatrix} f&amp;0&amp;0&amp;0 \ 0&amp;f&amp;0&amp;0 \0&amp;0&amp;1&amp;0 \ \end{bmatrix} \begin{bmatrix} R&amp;t\O&amp;1\end{bmatrix}\begin{bmatrix} X_w\Y_w\Z_w\1\end{bmatrix}$</p>
<p>$Z_c\begin{bmatrix} u \ v \ 1 \ \end{bmatrix} =\begin{bmatrix} f_x&amp;0&amp;c_x&amp;0 \ 0&amp;f_y&amp;c_y&amp;0 \0&amp;0&amp;1&amp;0 \ \end{bmatrix} \begin{bmatrix} R&amp;t\O&amp;1\end{bmatrix}\begin{bmatrix} X_w\Y_w\Z_w\1\end{bmatrix}$</p>
<p>其中，记$\alpha{f}=f_x,\beta{f}=f_y$，其含义为<strong>焦距f</strong>在<strong>图像坐标系</strong>两轴方向上的<strong>像素个数</strong>，单位像素。</p>
<p>右侧第一个矩阵为相机<strong>内参矩阵K</strong>（Camera Intrinsic Matrix），描述相机坐标与<strong>像素坐标</strong>之间的对应关系；</p>
<p>右侧第二个矩阵为相机外参矩阵T（Camera Extrinsic Matrix），描述<strong>世界坐标系</strong>与<strong>相机的位置</strong>关系。</p>
<p>左侧系数<strong>Zc</strong>称为<strong>比例因子</strong>，可记为s <strong><strong>归一化坐标：</strong></strong></p>
<p>投影过程可认为，将世界坐标系下点坐标：$^w{p}=[X_w \quad Y_w \quad Z_w ]<sup>T$转换到相机坐标系下点坐标系$</sup>c{p}=[X_c \quad Y_c \quad Z_c ]^T$，随后除去一维的数值（图像的深度信息），得到归一化坐标：$R\begin{bmatrix} X_w\Y_w\Z_w\end{bmatrix}+t=\begin{bmatrix} X_c\Y_c\Z_c\end{bmatrix} \to \begin{bmatrix} \frac{X_c}{Z_c}\\frac{Y_c}{Z_c}\1\end{bmatrix}$</p>
<p>归一化坐标可视作为，相机平面前方z=1处平面上的点，称为<strong>归一化平面</strong>，<strong>归一化坐标左乘内参矩阵可得像素坐标</strong>。</p>
<p>通常认为<strong>归一化平面同图像平面相同，也即图像平面在相机平面前方z=1处</strong>。可知，在针孔模型中图像的深度信息被丢弃，无法得到。</p>
<ul>
<li>由<strong>透镜形状</strong>对传播产生的畸变，称为<strong>径向畸变</strong>（Radial Distortion），这种畸变越靠近图像边缘，越明显。此类畸变由于透镜形状的中心对称而通常径向对称。</li>
<li>由机械安装对传播产生的畸变，称为<strong>切向畸变</strong>（Tangential Distortion），切向畸变产生于相机透镜组装的不精确，由于透镜制造上的缺陷使其本身同像平面不平行而产生</li>
</ul>
<figure><figcaption></figcaption></figure>
<p>畸变模型：</p>
<p>理想点（Ideal Point）为通过坐标变换得到的坐标，真实点（Real Point）为理想点$(x,y)$,经过径向畸变d r drdr和切向畸变d t dtdt后真实得到图像的上的点：</p>
<p>$\begin{cases} x_{dis}=x(1+k_1r<sup>2+k_2r</sup>4+k_3r<sup>6)+2p_1xy+p_2(r</sup>2+2x^2) \ y_{dis}=y(1+k_1r<sup>2+k_2r</sup>4+k_3r<sup>6)+p_1(r</sup>2+2y^2)+2p_2xy \ \end{cases}$</p>
<p>其中，径向畸变由参数:$[k_1,k_2,k_3]$确定；切向畸变由参数$[p_1,p2]$决定；参数r满足：$r<sup>2=x</sup>2+y^2$。</p>
<p>由畸变后的点通过内参矩阵可得到像素平面上的实际坐标：</p>
<p>$\begin{cases} u=f_xx_{dis}+c_x\ v=f_yy_{dis}+c_y\ \end{cases}$ 而畸变较真就是反推出真实的x，y在哪里。</p>
<figure><figcaption></figcaption></figure>
<figure><figcaption></figcaption></figure>
<h3> <strong>双目相机</strong></h3>
<p><strong>双目相机</strong>由两个单目相机组成，但这两个相机之间的距离（称为基线）是已知的。我们通过这个基线来估计每个像素的空间位置，基线距离越大，能够测量到的就越远，双目与多目的缺点是配置与标定均较为复杂，其深度量程和精度受双目的基线与分辨率的限制，而且视觉计算非常消耗计算资源，需要使用GPU和FPGA设备加速后，才能实时输出整张图像的距离信息。因此在现有的条件下，计算量是双目的主要问题之一。</p>
<figure><figcaption></figcaption></figure>
<p>则可由相似三角形得到：$\frac{z-f}{z}=\frac{v-u_L+u_R}{b}$</p>
<p>从而得到深度信息：$z=f\frac{b}{d}$</p>
<p>其中，参数d称为<strong>视差</strong>（disparity），用于描述物体在两相机上形成像素的横坐标之差：$d\triangleq=u_L-u_R$</p>
<p>由视差可估计得到物体的深度信息，视差越大，距离越近。由于视差最小为一个像素，则双目测量具有最大值：</p>
<p>$\forall{d=1}\qquad{s.t.}\qquad{z_{max}=fb}$</p>
<p>也即双目相机的基线越大，测量范围就越大。</p>
<h3> 深度相机</h3>
<p><strong>深度相机</strong>又称RGB-D相机，它最大的特点是可以通过红外结构光或Time-of-Flight(ToF)原理，像激光传感器那样，通过主动像物体发射光并接收返回的光，测出物体离相机的距离。目前常用的RGB-D相机还存在测量范围窄、噪声大、视野小、易受日光干扰、无法测量透射材质等诸多问题。</p>
<p>RGB-D相机按原理可分为两大类：</p>
<ul>
<li>红外结构光类（Structured Light）</li>
<li>飞行时间类（Time-of-Flight,ToF）</li>
</ul>
<p>RGB-D相机通过向<strong>探测目标发射光束</strong>（通常为红外光）进行测距。其中<strong>结构光相机返回结构光图案</strong>以计算距离，而飞行时间则计算<strong>发射与接收时间差值</strong>计算距离。</p>
<figure><figcaption></figcaption></figure>
<p>相机成像后，生成图像。图像在计算机中以矩阵形式存储（二维数组）。</p>
<figure><figcaption></figcaption></figure>
<h1> 4、自动驾驶传感器</h1>
<h2> 4.2 传感器分类</h2>
<figure><figcaption></figcaption></figure>
<h2> 4.2毫米波雷达</h2>
<figure><figcaption></figcaption></figure>
<h2> 4.3激光类雷达</h2>
<p>


**扫描式激光雷达：**被称为自动驾驶领域中必不可少的传感器。它可对车辆自身位置和目标物体之间的距离以及目标物体的形状进行分析，也可对包括行车道白线在内的道路形状等进行识别。</p>
<figure><figcaption></figcaption></figure>
<p>MEMS（Micro Electromechanical System）即微机电系统，是指尺寸在几毫米乃至更小的高科技装置，其内部结构一般在微米甚至纳米量级，是一个独立的智能系统。 MEMS微光反射镜是指采用光学MEMS技术制造的，把微光反射镜与MEMS驱动器继集成在一起的光学MEMS器件。MEMS微光反射镜的运动方式包括平动和扭转两种机械运动。 通过可旋转MEMS微光反射镜改变发射光束的方向，对特定范围进行扫描。目标物体会反射扫描光束，接收部件会识别反射光。通过发射激光和接收到反射光的时间，可以测定与目标物体间的距离以及目标物体的大小。</p>
<h2> 4.4超声波雷达</h2>
<p><strong>超声波传感器的构成</strong></p>
<figure><figcaption></figcaption></figure>
<p><strong>工作原理：</strong></p>
<p>超声波发射器发出超声波，超声波遇到障碍物会返回，超声波传感器正是根据发射波和回波之间的时间差来测定发射点到障碍物的实际距离。</p>
<p>
强大如超声波传感器可是很怕脏污的，应始终保持表面干净。因为当其被异物附着时，超声波喇叭的震动(残响时间*1)会发生异常。例如超声波喇叭上附着霜（冰）、雪、泥等异物时，会影响超声波喇叭的正常功能。挡板即护圈（保护零件），相当于超声波传感器嵌入车辆的缓冲器。</p>
<figure><figcaption></figcaption></figure>
<h2> 4.5相机</h2>
<p><strong>摄像头传感器的分类和构成</strong></p>
<p>摄像头传感器分为单镜头摄像头和多镜头立体摄像头两种。单镜头摄像头识别的是平面影像，而多镜头立体摄像头内置2个摄像头，除了可以识别立体物体，还可以测算到目标物体的距离。</p>
<figure><figcaption></figcaption></figure>
<p>投影点坐标的位置不精确会影响检测精度。请确保镜头已得到充分的校正、调整。
</p>
<p><strong>工作原理：</strong></p>
<p>像头传感器通过获取摄像头拍摄的车辆周边的实景画面，从实景画面中抽取场景特征信息、调整显像浓度，对画面进行预处理。根据预处理结果，更容易辨别对象的特征及形状、颜色等信息，从而提高检测速度。</p>
<figure><figcaption></figcaption></figure>
<figure><figcaption></figcaption></figure>
<p><strong>摄像头传感器</strong></p>
<h2> 4.5视觉检测应用</h2>
<p><strong>目标物体处理流程：</strong></p>
<p>图像传感器通过图像处理识别对象物体，根据驾驶辅助ECU检测到的信息进行内容识别、判断、控制车辆。</p>
<figure><figcaption></figcaption></figure>
<p><strong>检测车道：</strong></p>
<p>从经过处理的图像上抽取边缘画面（亮度变化大的区域），从边缘画面中找出行车线标记（车道两侧的实线及虚线，直道显示为直线），通过行车线标记测定车道。</p>
<p>基于行车线信息获取车道中央位置、车辆行进方向及测算距离，从而识别、判断、控制车辆。</p>
<p>Hough (霍夫变换)用于检测图像中的各类曲线（如直线、圆、抛物线、椭圆等），并以一定的函数关系进行描述，应用于影像分析、模式识别等很多领域。</p>
<figure><figcaption></figcaption></figure>
<p><strong>检测道路标识：</strong></p>
<p>从经过处理的图像上抽取对应的候补点，寻找由各点分布构成的直线、曲线、平面等任意图形，按照特定的模板推定标识。通过标识信息进行判断并控制车辆。

<strong>检测行人：</strong></p>
<p>物图像由于体型、姿势、衣着等因素影响较难识别。因此，从图像中区分出静止的背景和运动的人物，需要根据模型化部位（手脚等较大部位的图形）以及统计性特征（全身图像等）进行识别，符合特征的则被判定为行人。根据车辆与行人间的位置关系及测算的距离，识别、判断、控制车辆。
</p>
<p><strong>多镜头立体摄像头：</strong></p>
<p>单镜头摄像头拍摄到的某一个图像，在转化成二次元画面时，由于缺少目标物体纵深数据导致无法进行立体识别。而多镜头立体摄像头融合了2个摄像头拍摄的图像从而获得视觉差，并利用视觉差使用三件测量的方式计算出纵深数据。因此，立体地识别目标物体的大小及形状

确保摄像头视野，图像传感器和镜片密封玻璃（前置摄像头）间配备镜头加热器。通过监控车外温度，镜头加热器加热除雾。当摄像头前方视野模糊时，图像传感器将停止工作。</p>
<p>另外，车辆在酷热等环境下停放后，图像传感器的温度会变得很高，可能会影响识别功能甚至过热停机。（温度降低后将正常工作）</p>
<h2> 4.6IMU</h2>
<p>一些基本概念：</p>
<p>1、<strong>6轴 :</strong> 三轴(XYZ)加速度计 + 三轴(XYZ)陀螺仪(也叫角速度传感器) 2、<strong>9轴 :</strong> 6轴 + 三轴(XYZ)磁场传感器 6轴模块可以构成VRU(垂直参考单元)和IMU(惯性测量单元)，9轴模块可以构成AHRS(航姿参考系统)</p>
<p>3、**IMU惯性测量单元：**可以输出加速度和角速度。并不输出姿态角等其他信息。是测量物体三轴角速度和加速度的设备。一个IMU内可能会装有三轴陀螺仪和三轴加速度计，来测量物体在三维空间中的角速度和加速度。严格意义上的IMU只为用户提供三轴角速度以及三轴加速度数据。</p>
<p>4、<strong>VRU</strong>垂直参考单元**😗* IMU的基础上内置姿态解算算法，可以输出姿态信息。在IMU的基础上，以重力向量作为参考，用卡尔曼或者互补滤波等算法为用户提供有重力向量参考的俯仰角、横滚角以及无参考标准的航向角。通常所说的6轴姿态模块就属于这类系统。航向角没有参考，不管模块朝向哪里，启动后航向角都为0°(或一个设定的常数)。随着模块工作时间增加，航向角会缓慢累计误差。俯仰角，横滚角由于有重力向量参考，低机动运动情况下，长时间不会有累积误差。</p>
<p>4、<strong>AHRS航姿参考系统:</strong> VRU的基础上修改算法，可以解算被测物体的全姿态，包括绝对的航向角(与地磁北极的夹角)，因为要用到<strong>地磁传感器，所以必须是9轴模块</strong>。另外室内由于地磁场畸变非常严重，AHRS 在室内也很难获得准确的绝对航向角。</p>
<p>在VRU的基础上增加了磁力计或光流传感器，用卡尔曼或者互补滤波等算法为用户提供拥有绝对参考的俯仰角、横滚角以及航向角的设备，这类系统用来为飞行器提供准确可靠的姿态与航行信息。我们通常所说的9轴姿态传感器就属于这类系统，因为航向角有地磁场的参考，所以不会漂移。但地磁场很微弱，经常受到周围带磁物体的干扰，所以如何在高机动情况下抵抗各种磁干扰成为AHRS研究的热门。</p>
<ul>
<li><strong><strong>LLA、ENU、ECEF坐标系</strong></strong></li>
</ul>
<ol>
<li>LLA地理坐标系则通过经度(longitude)，纬度(latitude)和高度(altitude)来表示地球的位置，也叫经纬高坐标系(LLA坐标系)。</li>
<li>ENU站心坐标系也叫做站点坐标系、东-北-天坐标系ENU，英文名称是local Cartesian coordinates coordinate system，主要是用于需了解以观察者为中心的其他物体运动规律。</li>
</ol>
<p>(1)站心直角坐标系</p>
<p>定义：以站心（如GPS接收天线中心）为坐标系原点O，Z轴与椭球法线重合，向上为正（天向），y与椭球短半轴重合（北向），x轴与地球椭球的长半轴重合（东向）所构成的直角坐标系，称为当地东北天坐标系（ENU）。</p>
<p>(2)站心极坐标系</p>
<p>定义：以站心为坐标极点O，以水平面（即xoy平面）为基准面，以东向轴（即x轴）为极轴，ρ为卫星到站点的距离，az为星视方向角（azimuth angle），el为星视仰角（elevation）。</p>
<figure><figcaption></figcaption></figure>
<ol start="3">
<li>ECEF</li>
</ol>
<p>地心地固坐标系（Earth-Centered,Earth-Fixed，简称ECEF）简称地心坐标系，是一种以地心为原点的地固坐标系（也称地球坐标系），是一种笛卡儿坐标系。原点 O (0,0,0)为地球质心，z 轴与地轴平行指向北极点，x 轴指向本初子午线与赤道的交点，y 轴垂直于xOz平面(即东经90度与赤道的交点)构成右手坐标系。
</p>
<ul>
<li>GNSS/INS</li>
</ul>
<p>**GPS：**全球卫星定位系统</p>
<p><strong>GNSS:</strong> 全球卫星定位系统，GPS，北斗，格洛纳斯等系统的总称</p>
<p><strong>GNSS/INS:</strong> 卫星/惯导组合导航系统，是一种组合导航系统，顾名思义这种系统是利用全球卫星导航系统(Global Navigation Satellite System 简称GNSS，它是GPS，北斗，GLONASS、GALILEO等系统的统称) 与惯性导航(Inertial Navigation System)各自的优势进行算法融合，为用户提供更加精准的姿态及位置信息。</p>
<figure><figcaption></figcaption></figure>
<p>6轴和9轴输出的是本身传感器的测量数据，accel、gyro 都是基于轴上的线加速度和轴角的角速度变换，都是原始的数值。而9轴相对于6轴只是增加一个磁力计测绝对方向，9轴中的imu传感器本身的测量和6轴一样。</p>
<p>6轴使用dmp解算时，roll、pich是ENU系下的，是绝对值，但是yaw和地理航向就没有任何关系了，6轴dmp解算后的yaw是相对于上电校准成功后的角度。9轴是地磁航向，地磁航向和ENU航向之间有个夹角，是已知的，可以补偿。</p>
<p>“9轴的地磁航向” 和 “ENU的航向” 之间的夹角补偿，是指我们平时说的，在当地进行磁力计校准吗？磁力计校准后，9轴的航向就是ENU系下的是吗？不是，地磁的北极和地理的北极不一致，二者有夹角。“磁力计校准” 校准是计算传感器的测量误差。</p>
<p>现在自动驾驶的感知、规划方案，多数都是要基于先验高精地图。</p>
<p>建图时，初始航向可以取自9轴、6轴、INS吗？可以。很多slam建图都没有IMU呢，航向就是初始为0和ENU都没有任何关系，不也照样到导航、规划、感知。定位是基于地图的定位，只要相对于地图的位置和姿态是准的就行</p>
<h2> 4.7V2X技术</h2>
<p>在环境感知方面，除了利用车辆自身的智能，还可以借助外部环境实现信息的获取，这一类技术统称为V2X技术。</p>
<figure><figcaption></figcaption></figure>
<p><strong>V2X（Vehicle to Everything）技术又称为车用无线通信技术</strong>，本质上是一种物联网技术，V代表的是车辆，X代表的是道路、人、车、设备等一切可以连接的设备。</p>
<p>接下来详细介绍下V2X的各个场景包括：车与车之间（Vehicle to Vehicle,V2V）、车与路之间（Vehicle to Infrastructure,V2I）、车与人之间（Vehicle toPedestrian, V2P）、车与网络之间（Vehicle toNetwork, V2N）的交互。</p>
<h3> <strong>1、V2V</strong></h3>
<p>V2V是最经典的场景，指的是<strong>道路上车辆之间的通信</strong>。典型的就是前方车辆并道，后方车辆避让。</p>
<h3> <strong>2、V2I</strong></h3>
<p>V2I是指<strong>车载设备与路侧基础设施</strong>（如红绿灯、交通摄像头、路侧单元等）进行通信，路侧基础设施也可以获取附近区域车辆的信息并发布各种实时信息。V2I通信主要应用于实时信息服务、车辆监控管理、不停车收费等。</p>
<h3> <strong>3、V2P</strong></h3>
<p>V2P是指弱势交通群体（包括行人、骑行者等）使用用户设备（如手机、笔记本电脑等）与车载设备进行通信。V2P通信主要应用于避免或减少交通事故、信息服务等。</p>
<h3> <strong>4、V2N</strong></h3>
<p>V2N是指车载设备通过接入网/核心网与云平台连接，云平台与车辆之间进行数据交互，并对获取的数据进行存储和处理，提供车辆所需要的各类应用服务。V2N通信主要应用于车辆导航、车辆远程监控、紧急救援、信息娱乐服务等。</p>
<ul>
<li>机器人相关技术（多传感器融合、路径规划、控制）</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>