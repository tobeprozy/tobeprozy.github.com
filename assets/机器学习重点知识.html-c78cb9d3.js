import{_ as s,Y as l,Z as d,$ as t,a0 as e,a1 as o,a2 as a,D as r}from"./framework-d651fda7.js";const g="/assets/Untitled-d76fed7e.png",p="/assets/6-0e56b6a6.png",i="/assets/7-89f92bbe.png",c={},h=a('<h1 id="机器学习重点知识" tabindex="-1"><a class="header-anchor" href="#机器学习重点知识" aria-hidden="true">#</a> 机器学习重点知识</h1><h2 id="一、防止过拟合的办法" tabindex="-1"><a class="header-anchor" href="#一、防止过拟合的办法" aria-hidden="true">#</a> 一、防止过拟合的办法</h2><p>1、正则化：L1、L2、添加BN层</p><p>2、添加Dropout策略</p><p>3、降低模型复杂度</p><p>4、交叉验证</p><figure><img src="'+g+'" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><p>5、用更多的数据进行训练</p><p>6、早停 7、集成学习方法bagging(如随机森林）能有效防止过拟合</p><p>8、减少特征个数(不是太推荐)注意：降维不能解决过拟合。降维只是减小了特征的维度，并没有减小特征所有的信息。</p><h2 id="二、cnn参数和尺寸计算" tabindex="-1"><a class="header-anchor" href="#二、cnn参数和尺寸计算" aria-hidden="true">#</a> 二、CNN参数和尺寸计算</h2><p>AlexNet网络为例，以下是该网络的参数结构图。</p>',12),m=t("img",{src:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607123928671-1702012152.png",alt:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607123928671-1702012152.png",tabindex:"0",loading:"lazy"},null,-1),b={href:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607123928671-1702012152.png",target:"_blank",rel:"noopener noreferrer"},_=a('<h3 id="alexnet网络的层结构如下" tabindex="-1"><a class="header-anchor" href="#alexnet网络的层结构如下" aria-hidden="true">#</a> AlexNet网络的层结构如下：</h3><p><strong>1.Input:</strong>       图像的尺寸是227<em>227</em>3.</p><p><strong>2.Conv-1:</strong>    第1层卷积层的核大小11*11，96个核。步长(stride)为4，边缘填充（padding）为0。</p><p><strong>3.MaxPool-1:</strong>     池化层-1对Conv-1进行池化，尺寸为3*3，步长为2.</p><p><strong>4.Conv-2:</strong>    核尺寸：5*5，数量：256，步长：1，填充：2</p><p><strong>5.MaxPool-2:</strong>     尺寸：3*3，步长：2</p><p><strong>6.Conv-3:</strong> 核尺寸：3*3，数量：384，步长：1，填充：1</p><p><strong>7: Conv-4:</strong>   结构同Conv-3.</p><p><strong>8. Conv-5:</strong>   核尺寸：3*3，数量：256，步长：1，填充：1</p><p><strong>9. MaxPool-3</strong>: 尺寸：3*3，步长：2</p><p><strong>10.FC-1:</strong>       全连接层1共有4096个神经元。</p><p><strong>11.FC-1:</strong>       全连接层2共有4096个神经元。</p><p><strong>12.FC-3:</strong>       全连接层3共有1000个神经元。</p><h3 id="卷积层-conv-layer-的输出张量-图像-的大小" tabindex="-1"><a class="header-anchor" href="#卷积层-conv-layer-的输出张量-图像-的大小" aria-hidden="true">#</a> <strong>卷积层（Conv Layer）的输出张量（图像）的大小</strong></h3><p>定义如下：</p><p>O=输出图像的尺寸。</p><p>I=输入图像的尺寸。</p><p>K=卷积层的核尺寸</p><p>N=核数量</p><p>S=移动步长</p><p>P =填充数</p><p>输出图像尺寸的计算公式如下：</p><figure><img src="'+p+'" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><p>输出图像的通道数等于核数量N。</p><p>示例：AlexNet中输入图像的尺寸为227<em>227</em>3.第一个卷积层有96个尺寸为11<em>11</em>3的核。步长为4，填充为0.</p>',25),f=t("img",{src:"https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212148288-1891687249.png",alt:"https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212148288-1891687249.png",tabindex:"0",loading:"lazy"},null,-1),u={href:"https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212148288-1891687249.png",target:"_blank",rel:"noopener noreferrer"},x=a('<p>输出的图像为55<em>55</em>96（每个核对应1个通道）。</p><h3 id="池化层-maxpool-layer-的输出张量-图像-的大小" tabindex="-1"><a class="header-anchor" href="#池化层-maxpool-layer-的输出张量-图像-的大小" aria-hidden="true">#</a> <strong>池化层（MaxPool Layer）的输出张量（图像）的大小</strong></h3><p>定义如下：</p><p>O=输出图像的尺寸。I=输入图像的尺寸。S=移动步长PS=池化层尺寸</p><p>输出图像尺寸的计算公式如下：</p><figure><img src="'+i+'" alt="Untitled" tabindex="0" loading="lazy"><figcaption>Untitled</figcaption></figure><p>不同于卷积层，池化层的输出通道数不改变。</p><p>示例：每1层卷积层后的池化层的池化层尺寸为3<em>3，步长为2。根据前面卷积层的输出为55</em>55*96。池化层的输出图像尺寸如下：</p>',8),C=t("img",{src:"https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212243575-867401956.png",alt:"https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212243575-867401956.png",tabindex:"0",loading:"lazy"},null,-1),y={href:"https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212243575-867401956.png",target:"_blank",rel:"noopener noreferrer"},v=a('<p>输出尺寸为27<em>27</em>96。</p><h3 id="全连接层-fully-connected-layer-的输出张量-图像-的大小" tabindex="-1"><a class="header-anchor" href="#全连接层-fully-connected-layer-的输出张量-图像-的大小" aria-hidden="true">#</a> <strong>全连接层（Fully Connected Layer）的输出张量（图像）的大小</strong></h3><p>全连接层输出向量长度等于神经元的数量。</p><h3 id="通过alexnet改变张量-图像-的尺寸的结构如下" tabindex="-1"><a class="header-anchor" href="#通过alexnet改变张量-图像-的尺寸的结构如下" aria-hidden="true">#</a> <strong>通过AlexNet改变张量（图像）的尺寸的结构如下:</strong></h3><p>在AlexNet网络中，输出的图像尺寸为227<em>227</em>3.</p><p>Conv-1,尺寸变为55<em>55</em>96,池化层后变为27<em>27</em>96。</p><p>Conv-2,尺寸变为27<em>27</em>256,池化层后变为13<em>13</em>256.</p><p>Conv-3,尺寸变为13<em>13</em>384,经过Conv-4和Conv-5变回13<em>13</em>256.</p><p>最后,MaxPool-3尺寸缩小至6<em>6</em>256.</p><p>图像通过FC-1转换为向量4096<em>1.通过FC-2尺寸未改变.最终,通过FC-3输出1000</em>1的尺寸张量.</p><p>接下来,计算每层的参数数量.</p><h3 id="conv-layer参数数量" tabindex="-1"><a class="header-anchor" href="#conv-layer参数数量" aria-hidden="true">#</a> <strong>Conv Layer参数数量</strong></h3><p>在CNN中,每层有两种类型的参数:weights 和biases.总参数数量为所有weights和biases的总和.</p><p>定义如下:</p><p>WC=卷积层的weights数量</p><p>BC=卷积层的biases数量</p><p>PC=所有参数的数量</p><p>K=核尺寸</p><p>N=核数量</p><p>C =输入图像通道数</p><p>卷积层中,核的深度等于输入图像的通道数.于是每个核有K*K个参数.并且有N个核.由此得出以下的公式.</p>',21),N=t("img",{src:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124308478-1848716660.png",alt:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124308478-1848716660.png",tabindex:"0",loading:"lazy"},null,-1),F={href:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124308478-1848716660.png",target:"_blank",rel:"noopener noreferrer"},L=t("p",null,"示例:AlexNet网络中,第1个卷积层,输入图像的通道数(C)是3,核尺寸(K)是11*11,核数量是96. 该层的参数计算如下：",-1),P=t("img",{src:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124331325-1754637713.png",alt:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124331325-1754637713.png",tabindex:"0",loading:"lazy"},null,-1),k={href:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124331325-1754637713.png",target:"_blank",rel:"noopener noreferrer"},z=a('<p>计算出Conv-2, Conv-3, Conv-4, Conv-5 的参数分别为 614656 , 885120, 1327488 和884992.卷积层的总参数就达到3,747,200.</p><h3 id="maxpool-layer参数数量" tabindex="-1"><a class="header-anchor" href="#maxpool-layer参数数量" aria-hidden="true">#</a> <strong>MaxPool Layer参数数量</strong></h3><p>没有与MaxPool layer相关的参数量.尺寸,步长和填充数都是超参数.</p><h3 id="fully-connected-fc-layer参数数量" tabindex="-1"><a class="header-anchor" href="#fully-connected-fc-layer参数数量" aria-hidden="true">#</a> <strong>Fully Connected (FC) Layer参数数量</strong></h3><p>在CNN中有两种类型的全连接层.第1种是连接到最后1个卷积层,另外1种的FC层是连接到其他的FC层.两种情况我们分开讨论.</p><p>**类型1:**连接到Conv Layer</p><p>定义如下:</p><p>Wcf= weights的数量</p><p>Bcf= biases的数量O= 前卷积层的输出图像的尺寸</p><p>N = 前卷积层的核数量</p><p>F = 全连接层的神经元数量</p>',11),A=t("img",{src:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124425268-2023197792.png",alt:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124425268-2023197792.png",tabindex:"0",loading:"lazy"},null,-1),M={href:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124425268-2023197792.png",target:"_blank",rel:"noopener noreferrer"},B=t("p",null,[t("strong",null,"示例:"),e(" AlexNet网络中第1个FC层连接至Conv Layer.该层的O为6,N为256,F为4096.")],-1),I=t("img",{src:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124512882-1051362990.png",alt:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124512882-1051362990.png",tabindex:"0",loading:"lazy"},null,-1),U={href:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124512882-1051362990.png",target:"_blank",rel:"noopener noreferrer"},w=t("p",null,"参数数目远大于所有Conv Layer的参数和.",-1),K=t("p",null,"**类型2:**连接到FC Layer",-1),O=t("p",null,"定义如下:",-1),S=t("p",null,"Wff= weights的数量",-1),V=t("p",null,"Bff= biases的数量",-1),W=t("p",null,"Pff= 总参数的数量",-1),E=t("p",null,"F= 当前FC层的神经元数量",-1),T=t("p",null,"F-1 = 前FC层的神经元数量",-1),D=t("img",{src:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124536343-2096935768.png",alt:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124536343-2096935768.png",tabindex:"0",loading:"lazy"},null,-1),Y={href:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124536343-2096935768.png",target:"_blank",rel:"noopener noreferrer"},Z=t("p",null,"**示例:**AlexNet的最后1个全连接层,   F-1=4096,F=1000 .",-1),$=t("img",{src:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124557126-681880570.png",alt:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124557126-681880570.png",tabindex:"0",loading:"lazy"},null,-1),j={href:"https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124557126-681880570.png",target:"_blank",rel:"noopener noreferrer"},q=a('<h3 id="alexnet网络中张量-图像-尺寸和参数数量" tabindex="-1"><a class="header-anchor" href="#alexnet网络中张量-图像-尺寸和参数数量" aria-hidden="true">#</a> <strong>AlexNet网络中张量(图像)尺寸和参数数量</strong></h3><p>AlexNet网络中总共有5个卷积层和3个全连接层.总共有62,378,344个参数.以下是汇总表.</p><table><thead><tr><th>Layer Name</th><th>Tensor Size</th><th>Weights</th><th>Biases</th><th>Parameters</th></tr></thead><tbody><tr><td>Input Image</td><td>227x227x3</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Conv-1</td><td>55x55x96</td><td>34,848</td><td>96</td><td>34,944</td></tr><tr><td>MaxPool-1</td><td>27x27x96</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Conv-2</td><td>27x27x256</td><td>614,400</td><td>256</td><td>614,656</td></tr><tr><td>MaxPool-2</td><td>13x13x256</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Conv-3</td><td>13x13x384</td><td>884,736</td><td>384</td><td>885,120</td></tr><tr><td>Conv-4</td><td>13x13x384</td><td>1,327,104</td><td>384</td><td>1,327,488</td></tr><tr><td>Conv-5</td><td>13x13x256</td><td>884,736</td><td>256</td><td>884,992</td></tr><tr><td>MaxPool-3</td><td>6x6x256</td><td>0</td><td>0</td><td>0</td></tr><tr><td>FC-1</td><td>4096×1</td><td>37,748,736</td><td>4,096</td><td>37,752,832</td></tr><tr><td>FC-2</td><td>4096×1</td><td>16,777,216</td><td>4,096</td><td>16,781,312</td></tr><tr><td>FC-3</td><td>1000×1</td><td>4,096,000</td><td>1,000</td><td>4,097,000</td></tr><tr><td>Output</td><td>1000×1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Total</td><td></td><td></td><td></td><td>62,378,344</td></tr></tbody></table>',3);function G(H,J){const n=r("ExternalLinkIcon");return l(),d("div",null,[h,t("figure",null,[m,t("figcaption",null,[t("a",b,[e("https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607123928671-1702012152.png"),o(n)])])]),_,t("figure",null,[f,t("figcaption",null,[t("a",u,[e("https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212148288-1891687249.png"),o(n)])])]),x,t("figure",null,[C,t("figcaption",null,[t("a",y,[e("https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212243575-867401956.png"),o(n)])])]),v,t("figure",null,[N,t("figcaption",null,[t("a",F,[e("https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124308478-1848716660.png"),o(n)])])]),L,t("figure",null,[P,t("figcaption",null,[t("a",k,[e("https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124331325-1754637713.png"),o(n)])])]),z,t("figure",null,[A,t("figcaption",null,[t("a",M,[e("https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124425268-2023197792.png"),o(n)])])]),B,t("figure",null,[I,t("figcaption",null,[t("a",U,[e("https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124512882-1051362990.png"),o(n)])])]),w,K,O,S,V,W,E,T,t("figure",null,[D,t("figcaption",null,[t("a",Y,[e("https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124536343-2096935768.png"),o(n)])])]),Z,t("figure",null,[$,t("figcaption",null,[t("a",j,[e("https://images2018.cnblogs.com/blog/1093637/201806/1093637-20180607124557126-681880570.png"),o(n)])])]),q])}const R=s(c,[["render",G],["__file","机器学习重点知识.html.vue"]]);export{R as default};
