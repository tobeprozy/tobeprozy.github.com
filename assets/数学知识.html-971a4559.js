import{_ as i,Y as n,Z as o,$ as t,a0 as d,a1 as r,a2 as a,D as s}from"./framework-d651fda7.js";const c="/assets/1-12dc3da9.png",p="/assets/2-13598b7a.png",g="/assets/5-3a6d1c11.png",h="/assets/3-bfdbda61.png",l="/assets/4-768e3f13.png",_={},f=t("h1",{id:"一、概述",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#一、概述","aria-hidden":"true"},"#"),d(" 一、概述")],-1),b=t("p",null,"在机器学习中所用到的主要有微积分、线性代数、概率论、最优化方法、信息论、随机过程、图论这几门数学课的知识。它们之间的关系如下图所示。",-1),m=t("img",{src:"https://pic1.zhimg.com/v2-c06946f5e5025f1fc3dd14d8a56dbd58_b.jpg",alt:"https://pic1.zhimg.com/v2-c06946f5e5025f1fc3dd14d8a56dbd58_b.jpg",tabindex:"0",loading:"lazy"},null,-1),u={href:"https://pic1.zhimg.com/v2-c06946f5e5025f1fc3dd14d8a56dbd58_b.jpg",target:"_blank",rel:"noopener noreferrer"},z=a('<p>在这里，一元函数微积分，线性代数与矩阵论是最基础的知识，也是其他课程的先修课程。其中多元函数微积分是一元函数微积分向多元函数的推广，且使用了线性代数与矩阵论的知识。</p><p>最优化方法（连续优化问题，这里不考虑随机优化等特殊的算法）<strong>以多元函数微积分为基础</strong>，<strong>梯度下降法</strong>、<strong>牛顿法</strong>、<strong>拟牛顿法等数值优化算法</strong>的推导，以及<strong>拉拉格朗日乘数法</strong>等<strong>解析优化算法</strong>的推导与证明，均使用了<strong>多元函数微积分</strong>的知识。</p><p><strong>概率论</strong>以微积分和线性代数为基础，导数、积分在这里被大量地使用。<strong>信息论与随机过程都是概率论的延伸</strong>，要学好它们，必须先掌握概率论。</p><p>图论中使用了<strong>线性代数的知识，比如邻接矩阵</strong>，普图理论中的拉普拉斯矩阵等。在机器学习中，它还与概率论结合，诞生了概率图模型这种模型。</p><table><thead><tr><th>算法</th><th>所用的数学知识</th></tr></thead><tbody><tr><td>分类与回归</td><td>贝叶斯分类器: 随机变量，条件概率，贝叶斯公式，正态分布，最大似然估计<br>KNN算法: 距离函数</td></tr><tr><td>决策树</td><td>熵，信息增益，Gini系数</td></tr><tr><td>KNN算法</td><td>距离函数</td></tr><tr><td>线性判别分析</td><td>散布矩阵，逆矩阵，广义瑞利商，拉格朗日乘数法，特征值与特征向量，标准正交基，投影</td></tr><tr><td>人工神经网络</td><td>向量与矩阵运算，链式法则，交叉熵，欧氏距离，梯度下降法，有向图，自动微分</td></tr><tr><td>支持向量机</td><td>点到超平面的距离，凸优化，拉格朗日对偶，强对偶，Slater条件，KKT条件，Mercer条件</td></tr><tr><td>logistic回归与softmax回归</td><td>条件概率，伯努利分布，多项分布，最大似然估计，凸优化，梯度下降法，牛顿法</td></tr><tr><td>随机森林</td><td>抽样，方差</td></tr><tr><td>Boosting算法</td><td>数学期望，条件概率，泰勒公式，牛顿法</td></tr><tr><td>线性回归，岭回归，LASSO回归</td><td>均方误差，最小二乘法，向量范数，梯度下降法，凸优化</td></tr><tr><td>数据降维</td><td>主成分分析</td></tr><tr><td>核主成分分析</td><td>Mercer条件</td></tr><tr><td>流形学习</td><td>线性组合，均方误差，无向图，拉普拉斯矩阵，归一化拉普拉斯矩阵，拉格朗日乘数法，特征值与特征向量，KL散度，t分布，测地距离</td></tr><tr><td>距离度量学习</td><td>NCA</td></tr><tr><td>ITML</td><td>KL散度，带约束的优化问题</td></tr><tr><td>LMNN</td><td>线性变换，梯度下降法，高斯混合模型与EM算法，高斯过程回归</td></tr><tr><td>概率图模型</td><td>HMM</td></tr><tr><td>CRF</td><td>无向图，条件概率，最大似然估计，拟牛顿法</td></tr><tr><td>贝叶斯网络</td><td>有向图，条件概率，贝叶斯公式，最大似然估计</td></tr><tr><td>聚类</td><td>谱聚类</td></tr><tr><td>Mean Shift算法</td><td>核密度估计，梯度下降法</td></tr><tr><td>深度生成模型</td><td>GAN</td></tr><tr><td>VAE</td><td>概率分布变换，KL散度，变分推断，梯度下降法,变分推断,MCMC采样</td></tr><tr><td>强化学习</td><td>马尔可夫决策过程</td></tr><tr><td>DQN</td><td>数学期望，梯度下降法</td></tr><tr><td>策略梯度</td><td>马尔可夫过程，数学期望，极限分布，梯度下降法，KL散度</td></tr><tr><td>Actor-Critic算法</td><td>梯度下降法，KL散度</td></tr></tbody></table><h1 id="二、微分学" tabindex="-1"><a class="header-anchor" href="#二、微分学" aria-hidden="true">#</a> 二、微分学</h1><p>微积分由一元函数微积分、多元函数微积分两部分构成，它是整个高等数学的基石。通常情况下，机器学习需要得到一个函数（模型，或者说假设），既然是函数，那自然就离不开微积分了。微积分为我们研究函数的性质，包括单调性、凹凸性、以及极值提供了理论依据。同时它也是学习概率论、信息论、最优化方法等后续课程的基础。</p><p>在机器学习中，最应该被记住的微积分知识点是下面的两张图。第一张图是微分学：</p>',8),x=t("img",{src:"https://pic2.zhimg.com/80/v2-165ef42895a62abbaf2f1e01668c6881_720w.jpg",alt:"https://pic2.zhimg.com/80/v2-165ef42895a62abbaf2f1e01668c6881_720w.jpg",tabindex:"0",loading:"lazy"},null,-1),v={href:"https://pic2.zhimg.com/80/v2-165ef42895a62abbaf2f1e01668c6881_720w.jpg",target:"_blank",rel:"noopener noreferrer"},j=t("p",null,"微分学中最应该被记住的是链式法则和泰勒公式。后者是理解在机器学习中使用最多的梯度下降法、牛顿法、拟牛顿法等数值优化算法推导的基础，前者为计算各种目标函数的导数提供了依据。借助于雅克比矩阵，多元函数的链式法则有简介而优雅的表达，多元函数反函数求导公式可以与一元函数反函数求导公式达成形式上的统一。借助于梯度、Hessian矩阵以及向量内积、二次型，多元函数的泰勒公式与一元函数的泰勒公式可以达成形式上的统一。",-1),w=t("p",null,"第二张图是积分学：",-1),L=t("img",{src:"https://pic1.zhimg.com/80/v2-e4e0397d0659e72de034d1e4bbcea100_720w.jpg",alt:"https://pic1.zhimg.com/80/v2-e4e0397d0659e72de034d1e4bbcea100_720w.jpg",tabindex:"0",loading:"lazy"},null,-1),M={href:"https://pic1.zhimg.com/80/v2-e4e0397d0659e72de034d1e4bbcea100_720w.jpg",target:"_blank",rel:"noopener noreferrer"},K=t("p",null,"积分学中最关键的是积分换元公式，借助于雅克比行列式，可以与一元函数定积分的换元公式达成形式上的统一。积分换元公式在后面的概率论（如概率分布变换，逆变换采样算法），信息论（如多维正态分布的联合熵）等课程中有广泛的应用，务必要掌握。",-1),N=t("h1",{id:"三、线性代数",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#三、线性代数","aria-hidden":"true"},"#"),d(" 三、线性代数")],-1),y=t("p",null,"接下来看线性代数。线性代数对于机器学习是至关重要的。机器学习算法的输入、输出、中间结果通常为向量、矩阵。使用线性代数可以简化问题的表达，用一个矩阵乘法，比写成多重求和要简洁明了得多。线性代数是学习后续数学课程的基础。它可以与微积分结合，研究多元函数的性质。线性代数在概率论中也被使用，比如随机向量，协方差矩阵。线性代数在图论中亦有应用-如图的邻接矩阵，拉普拉斯矩阵。在随机过程中同样有应用-如状态转移矩阵。下面的图列出了线性代数的核心知识结构：",-1),C=t("img",{src:"https://pic3.zhimg.com/80/v2-f6cf84796a71462e59b6dbc451c9178e_720w.jpg",alt:"https://pic3.zhimg.com/80/v2-f6cf84796a71462e59b6dbc451c9178e_720w.jpg",tabindex:"0",loading:"lazy"},null,-1),k={href:"https://pic3.zhimg.com/80/v2-f6cf84796a71462e59b6dbc451c9178e_720w.jpg",target:"_blank",rel:"noopener noreferrer"},S=a('<p>向量与矩阵是线性代数中的基本计算对象，这门课基本上围绕着它们而展开。特征值与特征向量是机器学习中使用频率仅次于向量和矩阵的知识点，它连接其了众多的知识点，决定了矩阵的若干重要性质。</p><h1 id="四、概率论" tabindex="-1"><a class="header-anchor" href="#四、概率论" aria-hidden="true">#</a> 四、概率论</h1><h2 id="概率论" tabindex="-1"><a class="header-anchor" href="#概率论" aria-hidden="true">#</a> <strong>概率论</strong></h2><p>概率论对于机器学习来说也是至关重要的，它是一种重要的工具。如果将机器学习算法的输入、输出看作随机变量/向量，则可以用概率论的观点对问题进行建模。使用概率论的一个好处是可以对不确定性进行建模，这对于某些问题是非常有必要的。另外，它还可以挖掘变量之间的概率依赖关系，实现因果推理。概率论为某些随机算法-如蒙特卡洛算法、遗传算法，以及随机数生成算法-包括基本随机数生成413231637(、以及采样算法提供了理论依据和指导。最后，概率论也是信息论，随机过程的先导课程。下面这张图清晰地列出了概率论的核心知识：</p><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>下面这张图是对机器学习中概率模型的总结：</p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>从这张图可以清晰的看出频繁使用的概率论知识点，最重要的莫过于条件概率，贝叶斯公式，正态分布，最大似然估计。</p><h1 id="五、最优化方法" tabindex="-1"><a class="header-anchor" href="#五、最优化方法" aria-hidden="true">#</a> 五、<strong><strong>最优化方法</strong></strong></h1><p>最优化方法在机器学习中处于中心地位。几乎所有机器学习算法最后都归结于求解最优化问题，从而确定模型参数，或直接获得预测结果。前者的典型代表是有监督学习，通过最小化损失函数或优化其他类型的目标函数确定模型的参数；后者的典型代表是数据降维算法，通过优化某种目标函数确定降维后的结果，如主成分分析。下面这张图列出了最优化方法的核心知识：</p><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h1 id="六、信息论" tabindex="-1"><a class="header-anchor" href="#六、信息论" aria-hidden="true">#</a> 六、<strong><strong>信息论</strong></strong></h1><p>信息论是概率论的延伸，在机器学习与深度学习中通常用于构造目标函数，以及对算法进行理论分析与证明。在机器学习尤其是深度学习中，信息论的知识随处可见，比如：</p><ol><li><p>决策树的训练过程中需要使用熵作为指标</p></li><li><p>在深度学习中经常会使用交叉熵、KL散度、JS散度、互信息等概念</p></li><li><p>变分推断的推导需要以KL散度为基础</p></li><li><p>距离度量学习、流形降维等算法也需要信息论的知识</p></li></ol><p>总体来说，在机器学习中用得最多的是熵，交叉熵，KL散度，JS散度，互信息，条件熵等。下面这张图列出了信息论的核心知识：</p><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>熵是最基本的概念，推广到多个概率分布，可以得到交叉熵，KL散度，以及JS散度。推广到多个随机变量，可以得到互信息，条件熵。</p><h1 id="七、随机过程" tabindex="-1"><a class="header-anchor" href="#七、随机过程" aria-hidden="true">#</a> 七、随机过程</h1><p>随机过程同样是概率论的延伸。在机器学习中，随机过程被用于概率图模型、强化学习、以及贝叶斯优化等方法。不理解马尔可夫过程，你将对MCMC采样算法一筹莫展。下面这张图列出了机器学习中随机过程的核心知识：</p>',19),A=t("img",{src:"https://pic1.zhimg.com/80/v2-06979e6d6680a3565ee93d64ad5b0458_1440w.jpg",alt:"https://pic1.zhimg.com/80/v2-06979e6d6680a3565ee93d64ad5b0458_1440w.jpg",tabindex:"0",loading:"lazy"},null,-1),E={href:"https://pic1.zhimg.com/80/v2-06979e6d6680a3565ee93d64ad5b0458_1440w.jpg",target:"_blank",rel:"noopener noreferrer"},V=t("p",null,"在机器学习中所用的主要是马尔可夫过程和高斯过程。隐马尔可夫过程，马尔可夫决策过程都是它的延伸。平稳分布、细致平衡条件也是理解MCMC采样的核心基础。",-1),B=t("h1",{id:"八、图论",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#八、图论","aria-hidden":"true"},"#"),d(" 八、"),t("strong",null,[t("strong",null,"图论")])],-1),I=t("p",null,[d("在机器学习中，概率图模型是典型的图结构。流形降维算法与谱聚类算法均使用了谱图理论。计算图是图的典型代表，图神经网络作为一种新的深度学习模型，与图论也有密切的关系。下面这张图列出了图论的整体知识结构： "),t("img",{src:l,alt:"",loading:"lazy"})],-1),J=t("p",null,"这里相等难以理解的是谱图理论。谱图理论的核心是拉普拉斯矩阵，归一化拉普拉斯矩阵，理解它们需要扎实的线性代数基础。",-1);function T(D,G){const e=s("ExternalLinkIcon");return n(),o("div",null,[f,b,t("figure",null,[m,t("figcaption",null,[t("a",u,[d("https://pic1.zhimg.com/v2-c06946f5e5025f1fc3dd14d8a56dbd58_b.jpg"),r(e)])])]),z,t("figure",null,[x,t("figcaption",null,[t("a",v,[d("https://pic2.zhimg.com/80/v2-165ef42895a62abbaf2f1e01668c6881_720w.jpg"),r(e)])])]),j,w,t("figure",null,[L,t("figcaption",null,[t("a",M,[d("https://pic1.zhimg.com/80/v2-e4e0397d0659e72de034d1e4bbcea100_720w.jpg"),r(e)])])]),K,N,y,t("figure",null,[C,t("figcaption",null,[t("a",k,[d("https://pic3.zhimg.com/80/v2-f6cf84796a71462e59b6dbc451c9178e_720w.jpg"),r(e)])])]),S,t("figure",null,[A,t("figcaption",null,[t("a",E,[d("https://pic1.zhimg.com/80/v2-06979e6d6680a3565ee93d64ad5b0458_1440w.jpg"),r(e)])])]),V,B,I,J])}const F=i(_,[["render",T],["__file","数学知识.html.vue"]]);export{F as default};
