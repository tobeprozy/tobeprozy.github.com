import{_ as r,Y as n,Z as o,$ as e,a1 as a,a0 as t,a2 as i,D as l}from"./framework-39b9cf04.js";const d={},h=e("p",null,"分类、回归、聚类、概率模型",-1),c={href:"https://elitedatascience.com/machine-learning-algorithms#regression",target:"_blank",rel:"noopener noreferrer"},g={href:"https://elitedatascience.com/machine-learning-algorithms#regression",target:"_blank",rel:"noopener noreferrer"},m=i('<h1 id="一、回归算法" tabindex="-1"><a class="header-anchor" href="#一、回归算法" aria-hidden="true">#</a> 一、回归算法</h1><h2 id="_1-1-概念" tabindex="-1"><a class="header-anchor" href="#_1-1-概念" aria-hidden="true">#</a> 1.1 概念</h2><aside> 🪁 回归方法是一种对**数值型连续随机变量**进行预测和建模的**监督学习算法**。使用案例一般包括**房价预测**、**股票走势**或**测试成绩**等**连续变化**的案例。 <p>回归任务的<strong>特点</strong>是<strong>标注的数据集具有数值型的目标变量</strong>。也就是说，每一个观察样本都有<strong>一个数值型的标注真值</strong>以监督算法。</p></aside><h2 id="_1-2-线性回归" tabindex="-1"><a class="header-anchor" href="#_1-2-线性回归" aria-hidden="true">#</a> 1.2 线性回归</h2><aside> 🪁 **python实现：[](https://scikit-learn.org/stable/modules/linear_model.html)[https://scikit-learn.org/stable/modules/linear_model.html](https://scikit-learn.org/stable/modules/linear_model.html)** </aside><aside> 🪁 **线性回归是处理回归任务最常用的算法之一**。该算法的形式十分简单，它期望**使用一个超平面拟合数据集**（只有两个变量的时候就是一条直线）。如果数据集中的变量存在线性关系，那么其就能拟合地非常好。 </aside><aside> 🪁 在实践中，简单的线性回归通常被**使用正则化的回归方法**（LASSO、Ridge 和 Elastic-Net）所代替。 <p><strong>正则化</strong>其实就是<strong>一种对过多回归系数采取惩罚以减少过拟合风险的技术</strong>。当然，我们还得确定惩罚强度以让模型在欠拟合和过拟合之间达到平衡。</p></aside><aside> 🪁 **优点：**线性回归的理解与解释都十分直观，并且还能通过正则化来降低过拟合的风险。另外，线性模型很容易使用随机梯度下降和新数据更新模型权重。 **缺点：**线性回归在变量是**非线性关系**的时候表现很差。并且其也不够灵活以捕捉更复杂的模式，添加正确的交互项或使用多项式很困难并需要大量时间。 </aside><h2 id="_1-3-回归树-集成方法" tabindex="-1"><a class="header-anchor" href="#_1-3-回归树-集成方法" aria-hidden="true">#</a> 1.3 <strong><strong>回归树（集成方法）</strong></strong></h2><aside> 🪁 随机森林Python实现：[](http://scikit-learn.org/stable/modules/ensemble.html#random-forests)[http://scikit-learn.org/stable/modules/ensemble.html#random-forests](http://scikit-learn.org/stable/modules/ensemble.html#random-forests) 梯度提升树Python实现：[](http://scikit-learn.org/stable/modules/ensemble.html#classification)[http://scikit-learn.org/stable/modules/ensemble.html#classification](http://scikit-learn.org/stable/modules/ensemble.html#classification) </aside><aside> 🪁 回归树（决策树的一种）通过**将数据集重复分割为不同的分支**而实现**分层学习**，分割的标准是**最大化每一次分离的信息增益**。这种分支结构让回归树很自然地学习到非线性关系。 </aside><aside> 🪁 **集成方法**，如**随机森林（RF）**或梯度提升树（GBM）则组合了许多独立训练的树。这种算法的主要思想就是组合**多个弱学习算法**而成为一种强学习算法，不过这里并不会具体地展开。**在实践中 RF 通常很容易有出色的表现**，而 GBM 则更难调参，不过通常梯度提升树具有更高的性能上限。 </aside><aside> 🪁 **优点：**决策树能学习**非线性关系**，对异常值也具有**很强的鲁棒性**。集成学习在实践中表现非常好，其经常赢得许多经典的（非深度学习）机器学习竞赛。 **缺点：无约束的**，单棵树很容易过拟合，因为单棵树可以保留分支（不剪枝），并直到其记住了训练数据。集成方法可以削弱这一缺点的影响。 </aside><h2 id="_1-4-最近邻算法" tabindex="-1"><a class="header-anchor" href="#_1-4-最近邻算法" aria-hidden="true">#</a> 1.4 <strong><strong>最近邻算法</strong></strong></h2><aside> 🪁 最近邻算法是「基于实例的」，这就意味着其需要保留每一个训练样本观察值。最近邻算法通过搜寻最相似的训练样本来预测新观察样本的值。 <p>而这种算法是<strong>内存密集型</strong>，对高维数据的处理效果并不是很好，并且还需要高效的距离函数来度量和计算相似度。在实践中，基本上使用<strong>正则化的回归或树型集成方法</strong>是最好的选择。</p></aside><h2 id="_1-5-深度学习" tabindex="-1"><a class="header-anchor" href="#_1-5-深度学习" aria-hidden="true">#</a> 1.5 深度学习</h2><aside> 🪁 资源：[](https://keras.io/)[https://keras.io/](https://keras.io/) </aside><aside> 🪁 深度学习是指能学习极其复杂模式的多层神经网络。该算法使用在输入层和输出层之间的隐藏层对数据的中间表征建模，这也是其他算法很难学到的部分。 <p>深度学习还有其他几个重要的机制，如<strong>卷积和 drop-out</strong> 等，这些机制令该算法能有效地学习到<strong>高维数据</strong>。然而<strong>深度学习</strong>相对于其他算法需要更多的数据，因为其有更大数量级的参数需要估计。</p></aside><aside> 🪁 优点：深度学习是目前某些领域最先进的技术，如**计算机视觉**和语音识别等。**深度神经网络**在图像、音频和文本等数据上表现优异，并且该算法也很容易对新数据使用反向传播算法更新模型参数。它们的架构（即层级的数量和结构）能够适应于多种问题，并且隐藏层也减少了算法对特征工程的依赖。 缺点：**深度学习算法**通常**不适合作为通用目的**的算法，因为其需要大量的数据。实际上，**深度学习通常在经典机器学习问题上并没有集成方法表现得好**。另外，其在训练上是计算密集型的，所以这就需要更富经验的人进行调参（即设置架构和超参数）以减少训练时间。 </aside><h1 id="二、分类算法" tabindex="-1"><a class="header-anchor" href="#二、分类算法" aria-hidden="true">#</a> 二、分类算法</h1><h2 id="_2-1-概念" tabindex="-1"><a class="header-anchor" href="#_2-1-概念" aria-hidden="true">#</a> 2.1 概念</h2><aside> 🪁 1、分类算法是一种离散随机变量建模或预测的**监督学习**算法。 2、许多回归算法都有与其相对应的分类算法，分类算法通常适**用于预测一个类别**（或类别的概率）而**不是连续的数值**。 </aside><h2 id="_2-2-logistic-回归" tabindex="-1"><a class="header-anchor" href="#_2-2-logistic-回归" aria-hidden="true">#</a> 2.2 Logistic 回归</h2><aside> 🪁 **Python 实现：**[](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)[http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) </aside><aside> 🪁 Logistic 回归是与线性回归相对应的一种分类方法，且该算法的基本概念由线性回归推导而出。Logistic 回归通过 Logistic 函数（即 **Sigmoid 函数**）将**预测映射到 0 到 1 中间**，因此预测值就可以**看成某个类别的概率。** </aside><aside> 🪁 **优点：**输出有很好的概率解释，并且算法也能正则化而避免过拟合。Logistic 模型很**容易使用随机梯度下降和新数据更新模型权重**。 **缺点：**Logistic 回归在多条或非线性决策边界时性能比较差。 </aside><h2 id="_2-3-分类树-集成方法" tabindex="-1"><a class="header-anchor" href="#_2-3-分类树-集成方法" aria-hidden="true">#</a> 2.3 分类树（集成方法）</h2>',27),p={href:"http://scikit-learn.org/stable/modules/ensemble.html#classification",target:"_blank",rel:"noopener noreferrer"},_={href:"http://scikit-learn.org/stable/modules/ensemble.html#classification",target:"_blank",rel:"noopener noreferrer"},b=i('<aside> 🪁 与回归树相对应的分类算法是分类树。它们通常都是指**决策树**，或更严谨一点地称之为「分类回归树（CART）」，这也就是非常著名的 CART 的算法。 </aside><aside> 🪁 **优点：**同回归方法一样，分类树的集成方法在实践中同样表现十分优良。它们通常对异常数据具有相当的鲁棒性和可扩展性。因为它的层级结构，分类树的集成方法能很自然地对非线性决策边界建模。 **缺点：**不可约束，**单棵树趋向于过拟合**，使用集成方法可以削弱这一方面的影响。 </aside><h2 id="_2-4-支持向量机-svm" tabindex="-1"><a class="header-anchor" href="#_2-4-支持向量机-svm" aria-hidden="true">#</a> 2.4 <strong><strong>支持向量机(SVM)</strong></strong></h2><aside> 🪁 Python 实现：[](http://scikit-learn.org/stable/modules/svm.html#classification)[http://scikit-learn.org/stable/modules/svm.html#classification](http://scikit-learn.org/stable/modules/svm.html#classification) </aside><aside> 🪁 支持向量机（SVM）可以使用一个称之为**核函数的技巧**扩展到**非线性分类**问题，而该算法本质上就是**计算**两个称之为支持向量的观测数据之间的**距离**。 SVM 算法寻找的**决策边界**即最大化其与样本间隔的边界，因此支持向量机又称为**最大间距分类器**。 </aside><aside> 🪁 支持向量机中的**核函数采用非线性变换**，**将非线性问题变换为线性问题。** 例如，SVM 使用线性核函数就能得到类似于 logistic 回归的结果，只不过支持向量机因为最大化了间隔而更具鲁棒性。因此，在实践中，SVM 最大的优点就是可以使用非线性核函数对非线性决策边界建模。 </aside><aside> 🪁 **优点：**SVM 能对**非线性决策边界建模**，并且有许多可选的**核函数**形式。SVM 同样面对**过拟合有相当大的鲁棒性**，这一点在高维空间中尤其突出。 **缺点：**然而，SVM 是内存密集型算法，由于选择正确的核函数是很重要的，所以其**很难调参**，也不能扩展到较大的数据集中。目前在工业界中，**随机森林通常优于支持向量机算法**。 </aside><h2 id="_2-5-深度学习" tabindex="-1"><a class="header-anchor" href="#_2-5-深度学习" aria-hidden="true">#</a> 2.5 深度学习</h2><aside> 🪁 资源：[](https://keras.io/)[https://keras.io/](https://keras.io/) </aside><aside> 🪁 深度学习同样很容易适应于分类问题。实际上，深度学习应用地更多的是分类任务，如图像分类等。 </aside><aside> 🪁 **优点：**深度学习非常适用于分类音频、文本和图像数据。 **缺点：**和回归问题一样，深度神经网络需要大量的数据进行训练，所以其也不是一个通用目的的算法。 </aside><h2 id="_2-6-朴素贝叶斯" tabindex="-1"><a class="header-anchor" href="#_2-6-朴素贝叶斯" aria-hidden="true">#</a> 2.6 <strong><strong>朴素贝叶斯</strong></strong></h2><aside> 🪁 Python 实现：[](http://scikit-learn.org/stable/modules/naive_bayes.html)[http://scikit-learn.org/stable/modules/naive_bayes.html](http://scikit-learn.org/stable/modules/naive_bayes.html) </aside><aside> 🪁 朴素贝叶斯（NB）是一种**基于贝叶斯定理**和**特征条件独立假设**的**分类方法**。本质上朴素贝叶斯模型就是一个**概率表**，其通过训练数据更新这张表中的概率。为了预测一个新的观察值，朴素贝叶斯算法就是根据样本的**特征值**在概率表中**寻找最大概率**的那个类别。 <p>之所以称之为「朴素」，是因为该算法的<strong>核心就是特征条件独立性假设（每一个特征之间相互独立）</strong>，而这一假设在现实世界中基本是不现实的。</p></aside><aside> 🪁 **优点：**即使条件独立性假设很难成立，但朴素贝叶斯算法在实践中表现出乎意料地好。该算法**很容易实现并能随数据集的更新而扩展**。 **缺点：**因为朴素贝叶斯算法太简单了，所以其也经常被以上列出的分类算法所替代。 </aside><h1 id="三、聚类" tabindex="-1"><a class="header-anchor" href="#三、聚类" aria-hidden="true">#</a> 三、聚类</h1><h2 id="_3-1-概念" tabindex="-1"><a class="header-anchor" href="#_3-1-概念" aria-hidden="true">#</a> 3.1 概念</h2><aside> 🪁 **聚类**是一种**无监督学习**任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。使用案例包括**细分客户**、**新闻聚类**、**文章推荐**等。 <p>因为聚类是一种无监督学习（即数据没有标注），并且通常<strong>使用数据可视化评价结果</strong>。如果存在「正确的回答」（即在训练集中存在预标注的集群），那么分类算法可能更加合适。</p></aside><h2 id="_3-2-k均值聚类" tabindex="-1"><a class="header-anchor" href="#_3-2-k均值聚类" aria-hidden="true">#</a> 3.2 K均值聚类</h2><aside> 🪁 Python 实现：[](http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation)[http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation](http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation) </aside><aside> 🪁 **K 均值聚类是一种通用目的的算法**，聚类的度量**基于样本点之间的几何距离**（即在坐标平面中的距离）。 <p><strong>集群</strong>是围绕在<strong>聚类中心的族群</strong>，而集群呈现出类球状并具有<strong>相似的大小</strong>。该算法不仅十分简单，而且还足够灵活以面对大多数问题都能给出合理的结果。</p></aside><aside> 🪁 **优点**：K 均值聚类是最流行的聚类算法，因为该算法**足够快速、简单**，并且如果你的预处理数据和特征工程十分有效，那么该聚类算法将拥有令人惊叹的灵活性。 <p>缺点：该算法需要<strong>指定集群的数量</strong>，而 K 值的选择通常都不是那么容易确定的。另外，如果训练数据中的真实集群并不是类球状的，那么 K 均值聚类<strong>会得出一些比较差</strong>的集群。</p></aside><h2 id="_3-3-ap聚类" tabindex="-1"><a class="header-anchor" href="#_3-3-ap聚类" aria-hidden="true">#</a> 3.3 AP聚类</h2><aside> 🪁 Python 实现：[](http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)[http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering](http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) </aside><aside> 🪁 AP 聚类算法是一种相对较新的聚类算法，该聚类算法**基于两个样本点之间的图形距离**（graph distances）确定集群。采用该聚类方法的集群拥有**更小**和不相等的大小。 </aside><aside> 🪁 **优点：**该算法**不需要指出明确的集群数量**（但是需要指定「sample preference」和「damping」等超参数）。 **缺点：**AP 聚类算法主要的缺点就是**训练速度比较慢**，并需要大量内存，因此也就很难扩展到大数据集中。另外，该算法同样假定潜在的集群是类球状的。 </aside><h2 id="_3-4-dbscan" tabindex="-1"><a class="header-anchor" href="#_3-4-dbscan" aria-hidden="true">#</a> 3.4 DBSCAN</h2><aside> 🪁 Python 实现：[](http://scikit-learn.org/stable/modules/clustering.html#dbscan)[http://scikit-learn.org/stable/modules/clustering.html#dbscan](http://scikit-learn.org/stable/modules/clustering.html#dbscan) </aside><aside> 🪁 DBSCAN 是一个**基于密度**的算法，它**将样本点的密集区域组成一个集群。**最近还有一项被称为 HDBSCAN 的新进展，它允许改变密度集群。 </aside><aside> 🪁 **优点：**DBSCAN 不需要假设集群为球状，并且它的性能是可扩展的。此外，它不需要每个点都被分配到一个集群中，这降低了集群的异常数据。 **缺点：**用户必须要调整「epsilon」和「min_sample」这两个定义了集群密度的超参数。DBSCAN 对这些超参数非常敏感。 </aside>',30);function u(f,k){const s=l("ExternalLinkIcon");return n(),o("div",null,[h,e("p",null,[e("a",c,[a(s)]),e("a",g,[t("https://elitedatascience.com/machine-learning-algorithms#regression"),a(s)])]),m,e("aside",null,[t(" 🪁 随机森林 Python 实现：[](http://scikit-learn.org/stable/modules/ensemble.html#regression)[http://scikit-learn.org/stable/modules/ensemble.html#regression](http://scikit-learn.org/stable/modules/ensemble.html#regression) "),e("p",null,[t("梯度提升树 Python 实现："),e("a",p,[a(s)]),e("a",_,[t("http://scikit-learn.org/stable/modules/ensemble.html#classification"),a(s)])])]),b])}const y=r(d,[["render",u],["__file","基本算法.html.vue"]]);export{y as default};
