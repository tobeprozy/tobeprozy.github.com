import{_ as t,Y as i,Z as r,$ as n,a0 as s,a1 as l,a2 as a,D as p}from"./framework-39b9cf04.js";const o="/assets/image-20230427171334081-bb670331.png",c={},d=a('<div class="hint-container tip"><p class="hint-container-title">提示</p><p>nntc</p></div><h2 id="基本概念" tabindex="-1"><a class="header-anchor" href="#基本概念" aria-hidden="true">#</a> 基本概念</h2><p>nntc是一个神经网络编译器，可以将神经网络模型转换为特定硬件平台上可执行的代码。</p><p>你需要熟悉nntc支持的模型格式（如ONNX、TensorFlow、PyTorch等），以及如何将模型转换为特定硬件的指令集。</p><h2 id="应用移植的流程" tabindex="-1"><a class="header-anchor" href="#应用移植的流程" aria-hidden="true">#</a> 应用移植的流程</h2><ul><li><p>模型编译量化-nntc工具</p></li><li><p>应用开发部署-libsophone</p></li></ul><figure><img src="'+o+'" alt="image-20230427171334081" tabindex="0" loading="lazy"><figcaption>image-20230427171334081</figcaption></figure><p><strong>基本流程：</strong></p><p>其它模型----&gt;生成BMODEL-----&gt;加载BMODEL----&gt;前处理-----&gt;推理------后处理------&gt;编译应用-----&gt;打包</p><h3 id="模型的编译量化" tabindex="-1"><a class="header-anchor" href="#模型的编译量化" aria-hidden="true">#</a> 模型的编译量化</h3><p><strong>一、FP32 BMDEL生成</strong>，也就是将原始模型直接转化为与原始模型精度一致的可以运行在TPU上的模型</p><p><strong>二、量化BMODEL生成</strong>，根据需求修改模型：</p><p>如果 FP32 不能满足性能需求，需要对模型进行量化, 量化后模型精度会有一定损失，但能充 分发挥芯片算力。</p><p><strong>第一步：</strong> 准备量化用的输入数据集，比如包含输入图片的文件夹</p><p><strong>第二步：</strong> 量化：</p><p>​ <strong>（1）自动量化：</strong> 利用原始模型和数据集直接生成量化BMODEL，即（1）（3）----&gt;（4）</p><p>​ <strong>优点：</strong> 步骤简单、参数自动搜索</p><p>​ <strong>缺点：</strong> 时间比较长、性能不一定是最优的。</p><p>​ <strong>（2）分步量化：</strong> 对应于步骤 (5)(6)(7)(8)。其中重点是步骤 (7) 量化调参，需要一定的技巧和经验。</p><p><strong>注意两个MODEL：</strong></p><p>（1）BMODEL 是用于设备加载运行的。所有类型的模型转换成 BMODEL 才能在设备上运行。</p><p>（2）UMODEL 是用于量化的中间模型。可由原始模型导出 FP32 的 UMODEL，经过量化后会生成 INT8 的 UMODEL，最终也会转换成 BMODEL 到设备上运行。</p><h2 id="例子resnet18" tabindex="-1"><a class="header-anchor" href="#例子resnet18" aria-hidden="true">#</a> 例子resnet18</h2><h3 id="第一步-准备原始模型" tabindex="-1"><a class="header-anchor" href="#第一步-准备原始模型" aria-hidden="true">#</a> 第一步：准备原始模型</h3>',24),u={href:"http://219.142.246.77:65000/sharing/NVUS3acJ7",target:"_blank",rel:"noopener noreferrer"},m=a(`<ul><li><strong>下载到本地 resnet18_classify.tar.gz，并解压：</strong><code>tar zxvf resnet18_classify.tar.gz</code></li></ul><p>resnet18_classify 目录中包含以下文件： · model/resnet18.onnx resnet18 原始模型 · images/ 测试图片集 · ILSVRC2012/ 量化用数据集 · scripts/ 本示例中脚本文件 · src/ 应用源码目录 · CMakeLists.txt 构建脚本</p><h3 id="第二步-初始化tpn-nntc环境" tabindex="-1"><a class="header-anchor" href="#第二步-初始化tpn-nntc环境" aria-hidden="true">#</a> 第二步：初始化tpn-nntc环境</h3><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 将当前用户加入 docker 组</span>
<span class="token function">sudo</span> <span class="token function">usermod</span> <span class="token parameter variable">-aG</span> <span class="token function">docker</span> <span class="token environment constant">$USER</span>
newgrp <span class="token function">docker</span>

<span class="token function">mkdir</span> tpu-nntc
<span class="token comment"># 将压缩包解压到tpu-nntc</span>
<span class="token function">tar</span> zxvf tpu-nntc_vx.y.z-<span class="token operator">&lt;</span>hash<span class="token operator">&gt;</span>-<span class="token operator">&lt;</span>date<span class="token operator">&gt;</span>.tar.gz --strip-components<span class="token operator">=</span><span class="token number">1</span> <span class="token parameter variable">-C</span> tpu-nntc

<span class="token comment"># 进入docker，如果当前系统没有对应镜像，会自动从docker hub上下载</span>
<span class="token builtin class-name">cd</span> tpu-nntc
<span class="token function">docker</span> run <span class="token parameter variable">-v</span> <span class="token environment constant">$PWD</span>/<span class="token punctuation">..</span>:/workspace <span class="token parameter variable">-p</span> <span class="token number">8001</span>:8001 <span class="token parameter variable">-it</span> sophgo/tpuc_dev:latest
<span class="token comment"># 此时已经进入docker，并在/workspace目录下</span>
<span class="token comment"># 下面初始化软件环境</span>
<span class="token builtin class-name">cd</span> /workspace/tpu-nntc
<span class="token builtin class-name">source</span> scripts/envsetup.sh

<span class="token comment"># 环境初始化完成后，进入 resnet18_classify 目录</span>
<span class="token builtin class-name">cd</span> /workspace/resnet18_classify
<span class="token comment"># 为了方便清理，建议创建一个空的工作目录</span>
<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> workdir <span class="token operator">&amp;&amp;</span> <span class="token builtin class-name">cd</span> workdir
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="第三步-编译fp32bmodel" tabindex="-1"><a class="header-anchor" href="#第三步-编译fp32bmodel" aria-hidden="true">#</a> 第三步：编译FP32BMODEL</h3><p>转换的是 onnx 模型，所以需要 bmneto 前端(意思就说onnex 模型转换到bm模型)</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code># 此时在resnet18_classify/workdir目录中
python3 -m bmneto --model ../model/resnet18.onnx \\
--input_names &quot;input&quot; \\
--shapes &quot;[[1,3,224,224]]&quot; \\
--target BM1684X \\
--outdir bmodel/fp32
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可以输入查看下 fp32 bmodel 的信息：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>tpu_model --info bmodel/fp32/compilation.bmodel
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="自动量化生成-int8-bmodel" tabindex="-1"><a class="header-anchor" href="#自动量化生成-int8-bmodel" aria-hidden="true">#</a> 自动量化生成 INT8 BMODEL</h3><p><strong>自动量化工具</strong>会<strong>自动</strong>处理<strong>图片数据集</strong> 将其转为 <strong>lmdb 数据集</strong>，并使用不同量化策略量化多次，自动生成 bmodel。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 量化采用 ILSVRC2012 的部分图片</span>
<span class="token comment"># 注意cali_image_preprocess参数，要和模型原始应用的预处理一致</span>
<span class="token comment"># 否则会出现模型量化精度高，但在应用上精度低的情况</span>
python3 <span class="token parameter variable">-m</span> ufw.cali.cali_model <span class="token punctuation">\\</span>
<span class="token parameter variable">--net_name</span> <span class="token string">&quot;resnet18&quot;</span> <span class="token punctuation">\\</span>
<span class="token parameter variable">--model</span> <span class="token punctuation">..</span>/model/resnet18.onnx <span class="token punctuation">\\</span>
<span class="token parameter variable">--cali_image_path</span> <span class="token punctuation">..</span>/ILSVRC2012/ <span class="token punctuation">\\</span>
<span class="token parameter variable">--cali_image_preprocess</span> <span class="token string">&#39;
resize_h=224,resize_w=224;
mean_value=123.675:116.28:103.53;
scale=0.0171:0.0175:0.0174;
bgr2rgb=True&#39;</span> <span class="token punctuation">\\</span>
<span class="token parameter variable">--input_names</span> <span class="token string">&#39;input&#39;</span> <span class="token punctuation">\\</span>
<span class="token parameter variable">--input_shapes</span> <span class="token string">&#39;[1,3,224,224]&#39;</span> <span class="token punctuation">\\</span>
<span class="token parameter variable">--target</span> BM1684X <span class="token punctuation">\\</span>
<span class="token parameter variable">--outdir</span> auto_cali_out
<span class="token function">cp</span> <span class="token parameter variable">-r</span> auto_cali_out/resnet18_batch1 bmodel/auto-int8
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>查看下 bmodel 的信息：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>tpu_model <span class="token parameter variable">--info</span> bmodel/auto-int8/compilation.bmodel
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>查看量化精度情况(可选)</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment">#在本机的 8001 端口 (启动 docker 时显示的端口) 可以打开网页，如果是本机直接打开http://localhost:8001 ，如果是远程服务器请将 localhost 替换成服务器 IP</span>
python3 <span class="token parameter variable">-m</span> ufw.tools.app <span class="token parameter variable">--port</span> <span class="token number">8001</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>注意：</strong> 这里可视化工具是在量化数据集中随机找一张图进行一次推理，并把数据显示出来。每 次显示时会不一样，但比较接近。</p><p>如果最终精度或在业务上验证精度不满足要求，可以考虑增加迭代次数或其他参数。</p><p>回到开发环境，Ctrl+C 结束精度显示服务。</p>`,19);function v(b,h){const e=p("ExternalLinkIcon");return i(),r("div",null,[d,n("p",null,[s("链接："),n("a",u,[s("SOPHGO"),l(e)])]),m])}const k=t(c,[["render",v],["__file","nntc.html.vue"]]);export{k as default};
